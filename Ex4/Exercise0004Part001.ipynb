{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7SYfVx2ZUyR"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# UnSupervised Learning Methods\n",
        "\n",
        "## Exercise 004 - Part I\n",
        "\n",
        "> Notebook by:\n",
        "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "| 1.0.000 | 08/09/2023 | Royi Avital | First version                                                      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIo6mUFyZUyU"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_08/Exercise0004Part001.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55od1SzsZUyU"
      },
      "source": [
        "## Notations\n",
        "\n",
        "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
        "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
        "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
        "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyx_iSmkZUyU"
      },
      "source": [
        "## Guidelines\n",
        "\n",
        " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
        " - Answer all questions / tasks within the Jupyter Notebook.\n",
        " - Use MarkDown + MathJaX + Code to answer.\n",
        " - Verify the rendering on VS Code.\n",
        " - Submission in groups (Single submission per group).\n",
        " - The submission files should have the format: `<fileName>_GRP_<#>`.  \n",
        "   For instance, `Exercise001Part002_GRP_A.ipynb` or `AuxFun_GRP_A.py`.\n",
        " - You may and _should_ use the forums for questions.\n",
        " - Good Luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxPmhYP-ZUyV"
      },
      "source": [
        "## Team Members\n",
        "\n",
        " - `Ori_Blanka_208994764`.\n",
        " - `Or_Benson_308577345`.\n",
        " - `Alon_Hertz_315682773`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRoV9fosZUyV"
      },
      "source": [
        "## 1. Classic Multi Dimensional Scaling (MDS)\n",
        "\n",
        " * Given a function $ \\phi \\left( \\cdot \\right) : \\mathbb{R}^{D} \\to \\mathbb{R}^{M} $.\n",
        " * Consider the following inner product: ${\\left \\langle \\boldsymbol{x}, \\boldsymbol{y} \\right \\rangle}_{\\phi} = \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle$.\n",
        " * Yields the induced norm: $ {\\left\\| \\boldsymbol{x} \\right\\|}_{\\phi} = \\sqrt{ \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{x} \\right) \\right \\rangle } $.\n",
        " * Yields the induced metric: ${d}_{\\phi} \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left\\| \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{\\phi}$.\n",
        "\n",
        "### 1.1. Question\n",
        "\n",
        "Consider the data (Training set) $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ and let $\\boldsymbol{D}_{\\phi} \\left[ i, j \\right] = {d}_{\\phi}^{2} \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$.\n",
        "\n",
        "Show that $- \\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = J \\boldsymbol{K}_{\\phi} \\boldsymbol{J}$ where:\n",
        "\n",
        " * $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$ - The centering matrix.\n",
        " * $\\boldsymbol{K}_{\\phi} = \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}$ where: $\\boldsymbol{\\Phi} = \\begin{bmatrix} \\mid & \\mid &  & \\mid \\\\ \\phi \\left( \\boldsymbol{x}_{1} \\right) & \\phi \\left( \\boldsymbol{x}_{2} \\right) & \\dots & \\phi \\left( \\boldsymbol{x}_{N} \\right) \\\\ \\mid & \\mid & & \\mid \\end{bmatrix} \\in \\mathbb{R}^{M \\times N} = \\boldsymbol{\\Phi} = \\begin{bmatrix} \\mid & \\mid &  & \\mid \\\\ \\boldsymbol{\\phi}_{1} & \\boldsymbol{\\phi}_{2} & \\dots & \\boldsymbol{\\phi}_{N} \\\\ \\mid & \\mid & & \\mid \\end{bmatrix} \\in \\mathbb{R}^{M \\times N}$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> Hints:\n",
        "    * Show that the transformation $\\phi \\left( \\cdot \\right)$ must be linear: $\\phi \\left( \\alpha \\boldsymbol{x}, \\beta \\boldsymbol{y} \\right) = \\alpha \\phi \\left( \\boldsymbol{x} \\right) + \\beta \\phi \\left( \\boldsymbol{y} \\right)$.  \n",
        "      You may use $\\left \\langle \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y}, \\boldsymbol{z} \\right \\rangle$ as a starting point.\n",
        "    * Show that ${d}_{\\phi}^{2} \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) - \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2} = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) \\right\\|}_{2}^{2} - 2 \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle + {\\left\\| \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2}$\n",
        "    * Use the lecture notes to conclude $- \\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = J \\boldsymbol{K}_{\\phi} \\boldsymbol{J}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbWd_LKvZUyW"
      },
      "source": [
        "### 1.1. Solution\n",
        "\n",
        "First, we will show that $\\phi \\left( \\cdot \\right)$ must be linear assuming that  $\\left \\langle \\boldsymbol{⋅}, \\boldsymbol{⋅} \\right \\rangle _{\\phi} $ is an inner product. And indeed, from the linearity of $\\left \\langle \\boldsymbol{⋅}, \\boldsymbol{⋅} \\right \\rangle _{\\phi} $ we can conclude that for any $ \\boldsymbol{z} \\in \\mathbb{R}^{D} $ it holds that:\n",
        "\n",
        "$$ <\\alpha \\boldsymbol{x}+\\beta \\boldsymbol{y}, \\boldsymbol{z}>_\\phi = \\alpha < \\boldsymbol{x}, \\boldsymbol{z}>_\\phi + \\beta < \\boldsymbol{y}, \\boldsymbol{z}>_\\phi ⇒ $$\n",
        "\n",
        "$$ <\\phi \\left( \\alpha \\boldsymbol{x}+\\beta \\boldsymbol{y} \\right), \\phi \\left( \\boldsymbol{z} \\right) > = \\alpha < \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{z} \\right)> + \\beta < \\phi \\left( \\boldsymbol{y} \\right), \\phi \\left( \\boldsymbol{z} \\right) > ⇒ $$\n",
        "\n",
        "$$ {\\phi \\left( \\alpha \\boldsymbol{x}+\\beta \\boldsymbol{y} \\right)}^T ⋅ \\phi \\left( \\boldsymbol{z} \\right) = \\alpha \\phi \\left( \\boldsymbol{x} \\right)^T ⋅ \\phi \\left( \\boldsymbol{z} \\right) + \\beta \\phi \\left( \\boldsymbol{y} \\right)^T ⋅ \\phi \\left( \\boldsymbol{z} \\right) ⇒ $$\n",
        "\n",
        "$$ {\\phi \\left( \\alpha \\boldsymbol{x}+\\beta \\boldsymbol{y} \\right)}^T ⋅ \\phi \\left( \\boldsymbol{z} \\right) - \\left( \\alpha \\phi \\left( \\boldsymbol{x} \\right)^T ⋅ \\phi \\left( \\boldsymbol{z} \\right) + \\beta \\phi \\left( \\boldsymbol{y} \\right)^T ⋅ \\phi \\left( \\boldsymbol{z} \\right) \\right) = 0 ⇒_* $$\n",
        "\n",
        "$$ \\left( {\\phi \\left( \\alpha \\boldsymbol{x}+\\beta \\boldsymbol{y} \\right)}^T -  \\alpha \\phi \\left( \\boldsymbol{x} \\right)^T  - \\beta \\phi \\left( \\boldsymbol{y} \\right)^T \\right) ⋅ \\phi \\left( \\boldsymbol{z} \\right) = 0 ⇒ $$\n",
        "\n",
        "$$ \\left \\langle {\\phi \\left( \\alpha \\boldsymbol{x}+\\beta \\boldsymbol{y} \\right)}-  \\alpha \\phi \\left( \\boldsymbol{x} \\right)  - \\beta \\phi \\left( \\boldsymbol{y} \\right) , \\phi \\left( \\boldsymbol{z} \\right) \\right \\rangle = 0  $$\n",
        "\n",
        "When we used the linearity of matrix multiplication in transition *. Now, from the Positive-definiteness of $\\left \\langle \\boldsymbol{⋅}, \\boldsymbol{⋅} \\right \\rangle $ we conclude that either\n",
        "\n",
        "$$ {\\phi \\left( \\alpha \\boldsymbol{x}+\\beta \\boldsymbol{y} \\right)}-  \\alpha \\phi \\left( \\boldsymbol{x} \\right)  - \\beta \\phi \\left( \\boldsymbol{y} \\right) = 0 $$\n",
        "or\n",
        "$$ \\phi \\left( \\boldsymbol{z} \\right) = 0 $$\n",
        "\n",
        "If $ \\phi $ is the zero function then its linear. Assuming its not, we have a $ \\boldsymbol{z} \\in \\mathbb{R}^{D} $ such that $ \\phi \\left( \\boldsymbol{z} \\right) \\neq 0 $. Since $ \\left \\langle {\\phi \\left( \\alpha \\boldsymbol{x}+\\beta \\boldsymbol{y} \\right)}-  \\alpha \\phi \\left( \\boldsymbol{x} \\right)  - \\beta \\phi \\left( \\boldsymbol{y} \\right) , \\phi \\left( \\boldsymbol{z} \\right) \\right \\rangle = 0  $ for all $ \\boldsymbol{z} \\in \\mathbb{R}^{D} $ we conclude that $  {\\phi \\left( \\alpha \\boldsymbol{x}+\\beta \\boldsymbol{y} \\right)}-  \\alpha \\phi \\left( \\boldsymbol{x} \\right)  - \\beta \\phi \\left( \\boldsymbol{y} \\right) = 0 $ which gives us:\n",
        "\n",
        "$$  {\\phi \\left( \\alpha \\boldsymbol{x}+\\beta \\boldsymbol{y} \\right)} =  \\alpha \\phi \\left( \\boldsymbol{x} \\right)  + \\beta \\phi \\left( \\boldsymbol{y} \\right)  $$\n",
        "\n",
        "and we conclude that $ \\phi $ is linear.\n",
        "\n",
        "Secondly, we have:\n",
        "\n",
        "$$ {d}_{\\phi}^{2} \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left\\|  \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{\\phi}^{2} = \\left \\langle \\phi \\left( \\boldsymbol{x} - \\boldsymbol{y} \\right) , \\phi \\left( \\boldsymbol{x} - \\boldsymbol{y} \\right) \\right \\rangle = \\left \\langle \\phi \\left( \\boldsymbol{x} \\right) - \\phi \\left( \\boldsymbol{y} \\right) , \\phi \\left( \\boldsymbol{x} \\right) - \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle  = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) - \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2} = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) \\right\\|}_{2}^{2} - 2 \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle + {\\left\\| \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2} $$\n",
        "\n",
        "Finally, we will show that  $- \\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = J \\boldsymbol{K}_{\\phi} \\boldsymbol{J}$ .\n",
        "\n",
        "We have:\n",
        "\n",
        "$$  \\boldsymbol{D}_{\\phi}[i,j] = {d}_{\\phi}^{2} \\left( \\boldsymbol{x}_i, \\boldsymbol{x}_j \\right) = {\\left\\| \\phi \\left( \\boldsymbol{x}_i \\right) \\right\\|}_{2}^{2} + {\\left\\| \\phi \\left( \\boldsymbol{x}_j \\right) \\right\\|}_{2}^{2} - 2 \\left \\langle \\phi \\left( \\boldsymbol{x}_i \\right), \\phi \\left( \\boldsymbol{x}_j \\right) \\right \\rangle $$\n",
        "\n",
        "We define:\n",
        "\n",
        "$$ \\boldsymbol{P} = \\begin{bmatrix} {\\left\\| \\phi \\left( \\boldsymbol{x}_1 \\right) \\right\\|}_{2}^{2} \\\\ {\\left\\| \\phi \\left( \\boldsymbol{x}_2 \\right) \\right\\|}_{2}^{2} \\\\ \\vdots \\\\ {\\left\\| \\phi \\left( \\boldsymbol{x}_N \\right) \\right\\|}_{2}^{2} \\end{bmatrix} \\cdot \\boldsymbol{1}_N^T $$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$$ \\boldsymbol{D}_{\\phi} = \\boldsymbol{P} - 2 \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi} + \\boldsymbol{P}^T $$\n",
        "\n",
        "Now, We saw in class that $ \\boldsymbol{P}\\boldsymbol{J} = \\boldsymbol{J}\\boldsymbol{P}^T = 0 $ therefore we conclude that:\n",
        "\n",
        "$$ - \\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} =  \\boldsymbol{J} \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi} \\boldsymbol{J} = \\boldsymbol{J} \\boldsymbol{K}_{\\phi} \\boldsymbol{J} $$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGKVGAEIZUyW"
      },
      "source": [
        "Consider the data (Training set) $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ and let $\\boldsymbol{D} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2}$.\n",
        "\n",
        "### 1.2. Question\n",
        "\n",
        "Show that $\\boldsymbol{v}^{T} \\boldsymbol{D} \\boldsymbol{v} \\leq 0$ for any $\\boldsymbol{v}$ such that $\\left \\langle \\boldsymbol{v}, \\boldsymbol{1} \\right \\rangle = 0$.  \n",
        "What does it imply on _distance matrices_?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3v1SlZvZUyW"
      },
      "source": [
        "### 1.2. Solution\n",
        "\n",
        "Denote $P'=\\begin{bmatrix} {\\left\\| \\boldsymbol{x}_{1} \\right\\|}_{2}^{2} \\\\ {\\left\\| \\boldsymbol{x}_{2} \\right\\|}_{2}^{2}\\\\ \\vdots \\\\ {\\left\\| \\boldsymbol{x}_{N} \\right\\|}_{2}^{2} \\end{bmatrix}$. We've seen that the distance matrix $D$ can be represented as $P-2X^{T}X+P^{T}$ where $P= P'1_N^T$.\n",
        "\n",
        "Therefore for any $v$ that satisfies the condition it hold that $<v,1_{N}>\\ =\\ <1_{N},v>\\ =1_{N}^{T}v=0$ and then we have:\n",
        "$$v^{T}\\left(P-2X^{T}X+P^{T}\\right)=<v,P'1_{N}^{T}v>-2<Xv,Xv>+<P'1_{N}^{T}v,v>\\ =\\ -2\\left|\\left|Xv\\right|\\right|_{2}^{2}\\le0$$\n",
        "    \n",
        "    \n",
        "1. As vector $\\boldsymbol{v}$ is orthogonal to the all-ones vector $\\boldsymbol{1}$, then $\\boldsymbol{v}^{T} \\boldsymbol{D} \\boldsymbol{v} \\leq 0$. This means that in the space spanned by the original data, any direction orthogonal to the mean (which the all-ones vector represents) does not increase the squared Euclidean distances between the data points\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyFutXp6ZUyX"
      },
      "source": [
        "## 2. Majorization Minimization / Maximization\n",
        "\n",
        "Consider the matrix:\n",
        "\n",
        "$$ \\boldsymbol{Y} \\left[ i, j \\right] = \\begin{cases} \\boldsymbol{X} \\left[ i, j \\right] & \\text{ if } \\boldsymbol{M} \\left[ i, j \\right] = 1 \\\\ 0 & \\text{ if } \\boldsymbol{M} \\left[ i, j \\right] = 0 \\end{cases} $$\n",
        "\n",
        "Namely $\\boldsymbol{Y} = \\boldsymbol{M} \\circ \\boldsymbol{X}$ where $\\circ$ is the Hadamard Product and $\\boldsymbol{M} \\in {\\left\\{ 0, 1 \\right\\}}^{M \\times N}$ is a _binary mask matrix_.\n",
        "\n",
        "Given $\\boldsymbol{Y} \\in \\mathbb{R}^{M \\times N}$, the low rank matrix completion objective is given by:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\arg \\min_{\\boldsymbol{X} \\in \\mathbb{R}^{M \\times N}} \\quad & {\\left\\| \\boldsymbol{M} \\circ \\left( \\boldsymbol{X} - \\boldsymbol{Y} \\right) \\right\\|}_{F}^{2} \\\\\n",
        "\\text{subject to} \\quad & \\begin{aligned}\n",
        "\\operatorname{Rank} \\left( \\boldsymbol{X} \\right) & \\leq d \\\\\n",
        "\\end{aligned}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "### 2.1. Question (Bonus 2 Points)\n",
        "\n",
        "Consider the following function:\n",
        "\n",
        "$$ g \\left( \\boldsymbol{X}, \\boldsymbol{Z} \\right) = {\\left\\| \\boldsymbol{X} - \\boldsymbol{Z} + \\boldsymbol{M} \\circ \\left( \\boldsymbol{Z} - \\boldsymbol{Y} \\right) \\right\\|}_{F}^{2} $$\n",
        "\n",
        "Show that $g \\left( \\cdot, \\cdot \\right)$ surrogates the objective function $f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{M} \\circ \\left( \\boldsymbol{X} - \\boldsymbol{Y} \\right) \\right\\|}_{F}^{2}$.\n",
        "\n",
        "\n",
        "* <font color='brown'>(**#**)</font> You may show $g \\left( \\boldsymbol{X}, \\boldsymbol{Z} \\right) = {\\left\\| \\boldsymbol{M} \\circ \\left( \\boldsymbol{X} - \\boldsymbol{Y} \\right) + \\tilde{\\boldsymbol{M}} \\circ \\left( \\boldsymbol{X} - \\boldsymbol{Z} \\right) \\right\\|}_{F}^{2}$ where $\\tilde{\\boldsymbol{M}} = \\boldsymbol{1} \\boldsymbol{1}^{T} - \\boldsymbol{M}$ is the complement of $\\boldsymbol{M}$, as an intermediate step.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cKXpHTYZUyX"
      },
      "source": [
        "Let's start with the objective function:\n",
        "\n",
        "$f(\\boldsymbol{X}) = \\|\\boldsymbol{M} \\circ (\\boldsymbol{X} - \\boldsymbol{Y})\\|_F^2$\n",
        "\n",
        "And the surrogate function:\n",
        "\n",
        "$g(\\boldsymbol{X}, \\boldsymbol{Z}) = \\|\\boldsymbol{X} - \\boldsymbol{Z} + \\boldsymbol{M}\\circ(\\boldsymbol{Z}-\\boldsymbol{Y})\\|_F^2$\n",
        "\n",
        "To show $g$ surrogates $f$, we need to verify two conditions:\n",
        "\n",
        "$g(\\boldsymbol{X}, \\boldsymbol{X}) = f(\\boldsymbol{X})$\n",
        "Substituting $\\boldsymbol{Z} = \\boldsymbol{X}$ in $g$, we get:\n",
        "$g(\\boldsymbol{X}, \\boldsymbol{X}) = \\|\\boldsymbol{X} - \\boldsymbol{X} + \\boldsymbol{M}\\circ(\\boldsymbol{X}-\\boldsymbol{Y})\\|_F^2$\n",
        "$= \\|\\boldsymbol{M}\\circ(\\boldsymbol{X}-\\boldsymbol{Y})\\|_F^2$\n",
        "$= f(\\boldsymbol{X})$\n",
        "$\\nabla_\\boldsymbol{X} g(\\boldsymbol{X}, \\boldsymbol{Z}) = \\nabla f(\\boldsymbol{X})$ for any $\\boldsymbol{X}, \\boldsymbol{Z}$\n",
        "Taking the gradient of $g$ w.r.t. $\\boldsymbol{X}$, we get:\n",
        "$\\nabla_\\boldsymbol{X} g(\\boldsymbol{X}, \\boldsymbol{Z}) = 2(\\boldsymbol{X} - \\boldsymbol{Z} + \\boldsymbol{M}\\circ(\\boldsymbol{Z}-\\boldsymbol{Y}))$\n",
        "$= 2\\boldsymbol{M}\\circ(\\boldsymbol{X}-\\boldsymbol{Y})$\n",
        "$= \\nabla f(\\boldsymbol{X})$\n",
        "Since both conditions are satisfied, $g(\\boldsymbol{X},\\boldsymbol{Z})$ surrogates $f(\\boldsymbol{X})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DUdCrh7ZUyX"
      },
      "source": [
        "## 3. Metric Multi Dimensional Scaling (MDS)\n",
        "\n",
        "The metric MDS objective is given by:\n",
        "\n",
        "$$ \\arg \\min_{\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}} {\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} $$\n",
        "\n",
        "Where:\n",
        "\n",
        " * $\\boldsymbol{\\Delta}_{x} \\left[ i, j \\right] = d \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ - The given distance matrix.\n",
        " * $\\boldsymbol{D}_{z} = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}$.\n",
        "\n",
        "Consider the surrogate function:\n",
        "\n",
        "$$ g \\left( \\boldsymbol{Z}, \\tilde{\\boldsymbol{Z}} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} + 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right) - 4 \\left \\langle \\boldsymbol{Z}^{T} \\tilde{\\boldsymbol{Z}}, \\boldsymbol{B} \\right \\rangle $$\n",
        "\n",
        "Where:\n",
        "\n",
        " * $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$ - The centering matrix.\n",
        " * $\\tilde{\\boldsymbol{D}}_{\\tilde{z}} \\left[ i, j \\right] = {\\left\\| \\tilde{\\boldsymbol{z}}_{i} - \\tilde{\\boldsymbol{z}}_{j} \\right\\|}_{2}$.\n",
        " * $\\boldsymbol{C} \\left[ i, j \\right] = \\begin{cases} 0 & \\text{ if } i = j \\\\ - \\frac{ \\boldsymbol{\\Delta}_{x} \\left[ i, j \\right] }{ \\tilde{\\boldsymbol{D}}_{z} \\left[ i, j \\right] } & \\text{ if } i \\neq j \\end{cases}$.\n",
        " * $\\boldsymbol{B} = \\boldsymbol{C} - \\operatorname{Diag} \\left( \\boldsymbol{C} \\boldsymbol{1} \\right)$.\n",
        "\n",
        "### 3.1. Question\n",
        "\n",
        "Prove that $ \\boldsymbol{B} \\boldsymbol{J} = \\boldsymbol{B} $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GJLf0nSZUyY"
      },
      "source": [
        "### 3.1. Solution\n",
        "\n",
        "$$BJ=B\\left(I-\\frac{1}{N}11^{T}\\right)=B-\\frac{1}{N}B\\ 11^{T}$$\n",
        "\n",
        "We will show that $B\\ 11^{T}=0$.\n",
        "    \n",
        "1.Note that the multiplication of $C1$ creates a vector $v$ such that $v_i=\\sum_{j}^{N}C\\left[i,j\\right]$, and as we are taking the Diag of this vector, we get a diagonal matrix $M$ such that $M\\left[i,i\\right]=\\sum_{j}^{N}C\\left[i,j\\right]$ and $0$ everywhere else.\n",
        "    \n",
        "2.Note that multiplication of matrix $C$ by $11^T$ creates a new matrix $A$ in which every enery along row $i$ of the matrix is the sum of row $i$ of $C$, that is $A\\left[i,j\\right]=\\sum_{k}^{N}C\\left[i,k\\right]$ for every $j$.\n",
        "    \n",
        "3.$B\\ 11^{T}=\\left(C-Diag\\left[C\\ 1\\right]\\right)11^{T}=C\\ 11^{T}-Diag\\left[C\\ 1\\right]11^{T}=T$\n",
        "\n",
        "Combining 1,2,3 together we can examine entery $i,l$ of $T$:\n",
        "    $$T\\left[i,l\\right]=\\sum_{k}^{N}C\\left[i,k\\right]-\\sum_{j}^{N}C\\left[i,j\\right]=0$$\n",
        "    \n",
        "Therefore $BJ=B$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIlBHeGMZUyY"
      },
      "source": [
        "### 3.2. Question\n",
        "\n",
        "Show that $g \\left( \\boldsymbol{Z}, \\boldsymbol{Z} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$.\n",
        "\n",
        "\n",
        " * <font color='brown'>(**#**)</font> Hints (See _lecture notes_):\n",
        "     * ${\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} - 2 \\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle + {\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$.\n",
        "     * ${\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} = 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right)$.\n",
        "     * $\\boldsymbol{D}^{\\circ 2}_{z} \\left[ i, j \\right] = \\boldsymbol{p} \\boldsymbol{1}^{T} - 2 \\boldsymbol{Z}^{T} \\boldsymbol{Z} + 1 \\boldsymbol{p}^{T}, \\; \\boldsymbol{p} = \\begin{bmatrix} {\\left\\| \\boldsymbol{z}_{1} \\right\\|}_{2}^{2} \\\\ {\\left\\| \\boldsymbol{z}_{2} \\right\\|}_{2}^{2} \\\\ \\vdots {\\left\\| \\boldsymbol{z}_{N} \\right\\|}_{2}^{2} \\end{bmatrix}$.\n",
        "     * For $\\tilde{\\boldsymbol{Z}} = \\boldsymbol{Z}$ we have $\\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle = - \\left \\langle \\boldsymbol{C}, \\boldsymbol{D}^{\\circ 2}_{z} \\right \\rangle$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiiu_dgzZUyY"
      },
      "source": [
        "### 3.2. Solution\n",
        "\n",
        "$$g\\left(Z,Z\\right)=\\left|\\left|\\Delta_{X}\\right|\\right|_{F}^{2}+2NTr\\left\\{ZJZ^{T}\\right\\}-4<Z^{T}Z,B>\\ =\\left|\\left|\\Delta_{X}\\right|\\right|_{F}^{2}+\\left|\\left|D_{z}\\right|\\right|_{F}^{2}-4<Z^{T}Z,B>$$\n",
        "    \n",
        "We will show $-<\\Delta_{X},D_{z}>\\ =-2<Z^{T}Z,B>$:\n",
        "    \n",
        "$$-2<Z^{T}Z,B>\\ =-2<B,Z^{T}Z>\\ =\\ -2<C-Diag\\left\\{C\\ 1\\right\\},\\ Z^{T}Z>\\ $$\n",
        "$$=-2\\left(<C,\\ Z^{T}Z>\\ -<Diag\\left\\{C\\ 1\\right\\},\\ Z^{T}Z>\\right)\\ =-2\\left(<C,\\ Z^{T}Z>\\ -<C\\ 1,\\ diag\\left\\{Z^{T}Z\\right\\}>\\right)$$\n",
        "$$=-2\\left(<C,\\ Z^{T}Z>\\ -<C\\ 1,\\ p>\\right)\\ =-2\\left(<C,\\ Z^{T}Z>\\ -<C\\ ,\\ p\\ 1^{T}>\\ \\right)=-2<C,\\ Z^{T}Z>\\ +2<C\\ ,\\ P>\\ $$\n",
        "$$=-2<C,\\ Z^{T}Z>\\ +<C\\ ,\\ P>\\ +<C,P>\\ =-2<C,\\ Z^{T}Z>\\ +<P^{T},\\ C^{T}>\\ +<C,P>$$\n",
        "$$=-2<C,\\ Z^{T}Z>\\ +<P^{T},\\ C>\\ +<C,P>\\ =-2<C,\\ Z^{T}Z>\\ +<C,\\ P^{T}>\\ +<C,P>$$\n",
        "$$=<C,P-2Z^{T}Z+P^{T}>\\ =<C,D_{z}^{2}>\\ =\\ -<\\Delta_{x},\\ D_{z}>$$\n",
        "    \n",
        "Therefore we have:\n",
        "$$g\\left(Z,Z\\right)=\\left|\\left|\\Delta_{X}\\right|\\right|_{F}^{2}+\\left|\\left|D_{z}\\right|\\right|_{F}^{2}-2<\\Delta_{x},\\ D_{z}>={\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFM91_t8ZUyY"
      },
      "source": [
        "## 4. IsoMap\n",
        "\n",
        "Let $G = \\left( V, E, W \\right) $ be a simple, undirected and weighted graph with no negative weights / edges.  \n",
        "Let $\\boldsymbol{D} \\in \\mathbb{R}^{N \\times N}$ be the shortest path distance matrix where $ \\left| V \\right| = N$.\n",
        "\n",
        "\n",
        "### 4.1. Question\n",
        "\n",
        "Prove or disprove: There is an embedding ${\\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{d} \\right\\}}_{i = 1}^{N}$ for some $d \\in \\mathbb{N}$ such that:\n",
        "\n",
        "$$ \\forall i, j \\; \\boldsymbol{D} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQO-9GRiZUyY"
      },
      "source": [
        "### 4.1. Solution\n",
        "\n",
        "**False:** The graph described above may be disconnected. In that case the value $ \\boldsymbol{D}[i,j] $ for $v_i$ and $v_j$ that has no path between them will be $∞$, which is not in the image of the Euclidean norm.\n",
        "\n",
        "Will give a counter example for a connected graph, just to make sure we cover the answer fully:\n",
        "\n",
        "Let's look at the graph that has four vertices: $A, B, C, D$. Vertex $A$ is \"in the middle\" of the graph and is connected to the three other vertices with edge length $1$. So we have:\n",
        "\n",
        "Edge $AB = AC = AD = 1$.\n",
        "\n",
        "The outer vertices aren't connected between them.\n",
        "\n",
        "We will get the following shortest path distance matrix:\n",
        "\n",
        "$$ \\boldsymbol{D} =\n",
        "\\begin{matrix}\n",
        "  && A && B && C && D \\\\\n",
        "A && 0 && 1 && 1 && 1 \\\\\n",
        "B && 1 && 0 && 2 && 2 \\\\\n",
        "C && 1 && 2 && 0 && 2 \\\\\n",
        "D && 1 && 2 && 2 && 0 \\\\\n",
        "\\end{matrix}\n",
        "$$\n",
        "\n",
        "Now, we assume by contradiction that there exists an embedding ${\\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{d} \\right\\}}_{i = 1}^{N}$ for some $d \\in \\mathbb{N}$ that fulfills the demand above. First, $d\\neq 1$ since there aren't three different real numbers that the distance between each two of them is $2$. So we assume $d\\geq 2$. The points $\\boldsymbol{z}_{B}, \\boldsymbol{z}_{C}, \\boldsymbol{z}_{D}$ creates an equilateral triangle placed on a surface that is contained in $\\mathbb{R}^{d}$ . The point  $\\boldsymbol{z}_{A}$ has to be on the line that is created from all the points that are in equal distance from $\\boldsymbol{z}_{B}, \\boldsymbol{z}_{C}, \\boldsymbol{z}_{D}$. The point on this line clostest to $\\boldsymbol{z}_{B}, \\boldsymbol{z}_{C}, \\boldsymbol{z}_{D}$ is on the same surface, but its distance from these points is equal to $ \\frac{2}{3} \\cdot \\sqrt{3}$ which is greater than $1$. Hance, there is no point that could be at distance of $1$ from all $\\boldsymbol{z}_{B}, \\boldsymbol{z}_{C}, \\boldsymbol{z}_{D}$ at the same time.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8g9Hg5TZUyZ"
      },
      "source": [
        "### 4.2. Question\n",
        "\n",
        " * Let $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$ be the training set.\n",
        " * Let $\\mathcal{Z} = \\left\\{ \\boldsymbol{z}_{i} \\right\\}_{i = 1}^{N}$ be the representation obtained by IsoMap (Encoded data).\n",
        " * Consider a new point $\\boldsymbol{x}^{\\ast}$ where $\\boldsymbol{x}^{\\ast} = \\boldsymbol{x}_{k}, \\; k \\in \\left\\{ 1, 2, \\ldots, N \\right\\}$.\n",
        " * Consider a new point $\\boldsymbol{x}^{\\dagger}$ where $\\nexists l : \\, \\boldsymbol{x}^{\\dagger} = \\boldsymbol{x}_{l}, \\; l \\in \\left\\{ 1, 2, \\ldots, N \\right\\}$.\n",
        " * Let $\\boldsymbol{z}^{\\ast}$ be the out of sample encoding applied to $\\boldsymbol{x}^{\\ast}$.\n",
        " * Let $\\boldsymbol{z}^{\\dagger}$ be the out of sample encoding applied to $\\boldsymbol{x}^{\\dagger}$.\n",
        "\n",
        "1. Prove or disprove: $\\boldsymbol{z}^{\\ast} = \\boldsymbol{z}_{k}$.\n",
        "1. Prove or disprove: Must exists $l$ such that $\\boldsymbol{z}^{\\dagger} = \\boldsymbol{z}_{l}$.\n",
        "\n",
        " * <font color='brown'>(**#**)</font> The out of sample encoding is as shown in _lecture notes_.\n",
        " * <font color='brown'>(**#**)</font> The question is basically if the out of sample extension is equivalent to having the point in the training set for such case.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE47ZRQjZUyZ"
      },
      "source": [
        "### 4.2. Solution\n",
        "\n",
        "The first statement is true. We can prove it by constructing an embedding ${\\boldsymbol{z}i \\in \\mathbb{R}^d}{i=1}^N$ that satisfies the given condition.\n",
        "\n",
        "Consider the graph $G$ and its shortest path distance matrix $\\boldsymbol{D}$. Let $d = N-1$, and we will construct the embedding ${\\boldsymbol{z}i \\in \\mathbb{R}^d}{i=1}^N$ as follows:\n",
        "\n",
        "1. Initialize an $N \\times d$ matrix $\\boldsymbol{Z}$ with all entries set to 0.\n",
        "2. For each pair of vertices $i$ and $j$, if $\\boldsymbol{D}[i, j]$ is the shortest path distance between $i$ and $j$, set $\\boldsymbol{Z}[i, j] = \\boldsymbol{D}[i, j]$ and $\\boldsymbol{Z}[j, i] = -\\boldsymbol{D}[i, j]$.\n",
        "3. Normalize the rows of $\\boldsymbol{Z}$ to have zero mean, i.e., subtract the mean of each row from the corresponding row.\n",
        "Let's verify that this embedding satisfies the given condition:\n",
        "\n",
        "For any pair of vertices $i$ and $j$, we have:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "|\\boldsymbol{z}i - \\boldsymbol{z}j|2 = \\left| \\boldsymbol{Z}[i, :] - \\boldsymbol{Z}[j, :] \\right|2 \\\n",
        "= \\left| \\boldsymbol{D}[i, :] - \\boldsymbol{D}[j, :] \\right|2 \\\n",
        "= \\sqrt{\\sum{k=1}^{d} \\left(\\boldsymbol{D}[i, k] - \\boldsymbol{D}[j, k]\\right)^2} \\\n",
        "= \\sqrt{\\sum{k=1}^{d} \\boldsymbol{D}^2[i, k] - 2\\boldsymbol{D}[i, k]\\boldsymbol{D}[j, k] + \\boldsymbol{D}^2[j, k]} \\\n",
        "= \\sqrt{\\boldsymbol{D}^2[i, :] - 2\\boldsymbol{D}[i, :]\\boldsymbol{D}[j, :] + \\boldsymbol{D}^2[j, :]} \\\n",
        "= \\sqrt{\\sum{k=1}^{N} \\boldsymbol{D}^2[i, k] - 2\\boldsymbol{D}[i, :]\\boldsymbol{D}[j, :] + \\sum{k=1}^{N} \\boldsymbol{D}^2[j, k]} \\\n",
        "= \\sqrt{\\sum{k=1}^{N} \\left(\\boldsymbol{D}[i, k]\\right)^2 - 2\\boldsymbol{D}[i, :]\\boldsymbol{D}[j, :] + \\sum_{k=1}^{N} \\left(\\boldsymbol{D}[j, k]\\right)^2} \\\n",
        "= \\sqrt{\\sum_{k=1}^{N} \\left(\\boldsymbol{D}[i, k] - \\boldsymbol{D}[j, k]\\right)^2} \\\n",
        "= \\boldsymbol{D}[i, j]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Thus, we have shown that the constructed embedding satisfies the condition $\\boldsymbol{D}[i, j] = |\\boldsymbol{z}_i - \\boldsymbol{z}_j|_2$ for all $i$ and $j$. Therefore, the first statement is true.\n",
        "\n",
        "\n",
        "The second statement is false\n",
        "let's analyze this step-by-step:\n",
        "\n",
        "Let $\\mathcal{X} = {\\boldsymbol{x}i \\in \\mathbb{R}^D}{i=1}^N$ be the training set.\n",
        "Let $\\mathcal{Z} = {\\boldsymbol{z}i}{i=1}^N$ be the representation obtained by IsoMap on the training set $\\mathcal{X}$.\n",
        "Consider a point $\\boldsymbol{x}^* = \\boldsymbol{x}_k$ for some $k \\in {1,2,\\ldots,N}$, i.e. $\\boldsymbol{x}^*$ is one of the points in the original training set $\\mathcal{X}$.\n",
        "Consider another point $\\boldsymbol{x}^\\dagger$ such that $\\boldsymbol{x}^\\dagger \\notin \\mathcal{X}$, i.e. $\\boldsymbol{x}^\\dagger$ is a new point not in the original training set.\n",
        "Let $\\boldsymbol{z}^$ be the out-of-sample encoding of $\\boldsymbol{x}^$ using IsoMap.\n",
        "Let $\\boldsymbol{z}^\\dagger$ be the out-of-sample encoding of $\\boldsymbol{x}^\\dagger$ using IsoMap.\n",
        "We need to prove or disprove: $\\exists l$ such that $\\boldsymbol{z}^\\dagger = \\boldsymbol{z}_l$\n",
        "\n",
        "From the out-of-sample extension formula, we have:\n",
        "\n",
        "$\\boldsymbol{z}^* = \\sum_{j=1}^N w_j^* \\boldsymbol{z}_j$ where $w_j^* = \\exp(-|\\boldsymbol{x}^*-\\boldsymbol{x}_j|^2/\\epsilon)$\n",
        "\n",
        "$\\boldsymbol{z}^\\dagger = \\sum_{j=1}^N w_j^\\dagger \\boldsymbol{z}_j$ where $w_j^\\dagger = \\exp(-|\\boldsymbol{x}^\\dagger-\\boldsymbol{x}_j|^2/\\epsilon)$\n",
        "\n",
        "Since $\\boldsymbol{x}^* \\in \\mathcal{X}$, there exists $k$ such that $\\boldsymbol{x}^* = \\boldsymbol{x}_k$. This means $w_k^* = 1$ and $w_j^* = 0 ; \\forall j \\neq k$.\n",
        "\n",
        "Therefore, $\\boldsymbol{z}^* = \\boldsymbol{z}_k$.\n",
        "\n",
        "However, since $\\boldsymbol{x}^\\dagger \\notin \\mathcal{X}$, we cannot conclude that $\\exists l$ such that $\\boldsymbol{z}^\\dagger = \\boldsymbol{z}_l$. The weights $w_j^\\dagger$ will likely be non-zero for multiple $j$.\n",
        "\n",
        "Therefore, the second statement is false. There does not necessarily exist $l$ such that $\\boldsymbol{z}^\\dagger = \\boldsymbol{z}_l$.\n",
        "\n",
        "In summary:\n",
        "\n",
        "For a point in the original training set, its encoded representation using out-of-sample extension will be equal to its original encoded representation.\n",
        "\n",
        "But for a new point not in the original training set, its encoded representation using out-of-sample extension cannot be guaranteed to be equal to any of the original encoded representations.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcjeRYEsZUyZ"
      },
      "source": [
        "## 5. Laplacian Eigenmaps\n",
        "\n",
        " * Let $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$.\n",
        " * Let $G = \\left( V, E, W \\right)$ be a weighted graph with with $V = \\mathcal{X}$.\n",
        " * Define $\\boldsymbol{W} \\left[ i, j \\right] = \\begin{cases} \\exp \\left( - \\frac{ {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} }{2 {\\sigma}^{2}} \\right) & \\text{ if } \\boldsymbol{x}_{i} \\in \\mathcal{N}_{j} \\text{ or } \\boldsymbol{x}_{j} \\in \\mathcal{N}_{i} \\\\ 0 & \\text{ else } \\end{cases}$.\n",
        " * Then ${e}_{ij} \\in E$ if $\\boldsymbol{W} \\left[ i, j \\right] \\neq 0$.\n",
        " * The _Graph Laplacian_ $\\boldsymbol{L} = \\boldsymbol{D} - \\boldsymbol{W}$.\n",
        " * The _Degree Matrix_ $\\boldsymbol{D} = \\operatorname{Diag} \\left( \\boldsymbol{W} \\boldsymbol{1} \\right)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6tHRRekZUyZ"
      },
      "source": [
        "\n",
        "### 5.1. Question\n",
        "\n",
        " * Let $\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}$ be a set of data with $\\boldsymbol{z}_{i}$ being the $i$ -th column of $\\boldsymbol{Z}$.\n",
        " * Define $\\boldsymbol{D}_{z} \\in \\mathbb{R}^{N \\times N}$ where $\\boldsymbol{D}_{z} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}^{2}$.  \n",
        "\n",
        "Show that:\n",
        "\n",
        "$$ \\frac{1}{2} \\left \\langle \\boldsymbol{W}, \\boldsymbol{D}_{z} \\right \\rangle = \\operatorname{Tr} \\left(  \\boldsymbol{Z} \\boldsymbol{L} \\boldsymbol{Z}^{T} \\right) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YFgvgRoZUyZ"
      },
      "source": [
        "### 5.1. Solution\n",
        "\n",
        "We want to show:\n",
        "$\\frac{1}{2} \\langle \\boldsymbol{W}, \\boldsymbol{D}_z \\rangle = \\text{Tr}( \\boldsymbol{Z} \\boldsymbol{L} \\boldsymbol{Z}^T)$\n",
        "\n",
        "Starting from the left hand side:\n",
        "\n",
        "$\\frac{1}{2} \\langle \\boldsymbol{W}, \\boldsymbol{D}z \\rangle$\n",
        "$= \\frac{1}{2} \\sum{i=1}^N \\sum_{j=1}^N \\boldsymbol{W}[i,j] \\boldsymbol{D}z[i,j]$\n",
        "$= \\frac{1}{2} \\sum{i=1}^N \\sum_{j=1}^N \\boldsymbol{W}[i,j] |\\boldsymbol{z}_i - \\boldsymbol{z}_j|_2^2$\n",
        "\n",
        "The right hand side:\n",
        "\n",
        "$\\text{Tr}( \\boldsymbol{Z} \\boldsymbol{L} \\boldsymbol{Z}^T)$\n",
        "$= \\text{Tr}( \\boldsymbol{Z} (\\boldsymbol{D} - \\boldsymbol{W}) \\boldsymbol{Z}^T)$\n",
        "$= \\text{Tr}(\\boldsymbol{Z} \\boldsymbol{D} \\boldsymbol{Z}^T) - \\text{Tr}(\\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^T)$\n",
        "\n",
        "Expanding the first term:\n",
        "$\\text{Tr}(\\boldsymbol{Z} \\boldsymbol{D} \\boldsymbol{Z}^T) = \\sum_{i=1}^N \\boldsymbol{D}[i,i] |\\boldsymbol{z}_i|_2^2$\n",
        "\n",
        "Since $\\boldsymbol{D}[i,i] = \\sum_{j=1}^N \\boldsymbol{W}[i,j]$, this becomes:\n",
        "\n",
        "$\\text{Tr}(\\boldsymbol{Z} \\boldsymbol{D} \\boldsymbol{Z}^T) = \\sum_{i=1}^N \\sum_{j=1}^N \\boldsymbol{W}[i,j] |\\boldsymbol{z}_i|_2^2$\n",
        "\n",
        "Expanding the second term:\n",
        "$\\text{Tr}(\\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^T) = \\sum_{i=1}^N \\sum_{j=1}^N \\boldsymbol{W}[i,j] \\langle \\boldsymbol{z}_i, \\boldsymbol{z}_j \\rangle$\n",
        "\n",
        "$\\therefore \\text{Tr}( \\boldsymbol{Z} \\boldsymbol{L} \\boldsymbol{Z}^T) = \\sum_{i=1}^N \\sum_{j=1}^N \\boldsymbol{W}[i,j] (|\\boldsymbol{z}_i|_2^2 - \\langle \\boldsymbol{z}_i, \\boldsymbol{z}_j \\rangle)$\n",
        "\n",
        "Using $|\\boldsymbol{z}_i - \\boldsymbol{z}_j|_2^2 = |\\boldsymbol{z}_i|_2^2 - 2\\langle \\boldsymbol{z}_i, \\boldsymbol{z}_j\\rangle + |\\boldsymbol{z}_j|_2^2$, this reduces to:\n",
        "\n",
        "$\\text{Tr}( \\boldsymbol{Z} \\boldsymbol{L} \\boldsymbol{Z}^T) = \\sum_{i=1}^N \\sum_{j=1}^N \\boldsymbol{W}[i,j] |\\boldsymbol{z}_i - \\boldsymbol{z}_j|_2^2$\n",
        "\n",
        "Which is equal to the left hand side.\n",
        "\n",
        "Therefore, the equation holds.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhOWcjpuZUya"
      },
      "source": [
        "Let $G$ be an undirected graph with a non negative weights with the corresponding graph laplacian matrix $\\boldsymbol{L}$.  \n",
        "The matrix $\\boldsymbol{L}$ has the eigen value $0$ with multiplicity of $k$.\n",
        "\n",
        "### 5.3. Question\n",
        "\n",
        "Show that the number of connected components of the graph is $k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTebsJCAZUya"
      },
      "source": [
        "### 5.3. Solution\n",
        "\n",
        "Proof:\n",
        "\n",
        "By definition, the graph Laplacian $\\boldsymbol{L} = \\boldsymbol{D} - \\boldsymbol{W}$, where $\\boldsymbol{D}$ is the degree matrix and $\\boldsymbol{W}$ is the adjacency matrix.\n",
        "\n",
        "For an undirected graph, $\\boldsymbol{L}$ is symmetric and positive semidefinite.\n",
        "\n",
        "The multiplicity of the 0 eigenvalue of $\\boldsymbol{L}$ equals the number of connected components in the graph. This is because:\n",
        "\n",
        "The null space of $\\boldsymbol{L}$ has a basis vector $\\boldsymbol{v}_i$ for each connected component $i$.\n",
        "Each $\\boldsymbol{v}_i$ is an indicator vector for nodes in component $i$.\n",
        "The 0 eigenvalue's algebraic multiplicity equals the dimension of the null space.\n",
        "Therefore, since $\\boldsymbol{L}$ has eigenvalue 0 with multiplicity $k$, the number of connected components in G is $k$.\n",
        "\n",
        "In summary:\n",
        "\n",
        "The multiplicity of the 0 eigenvalue of the graph Laplacian equals the number of connected components in an undirected graph.\n",
        "Since $\\boldsymbol{L}$ has eigenvalue 0 with multiplicity $k$, G must have $k$ connected components.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzA4grk_ZUya"
      },
      "source": [
        "[link text](https://)## 6. SNE & t-SNE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbY4V4kRZUya"
      },
      "source": [
        "Let $ p $ be the perplexity hyper parameter of the _SNE_ algorithm.  \n",
        "\n",
        "### 6.2. Question\n",
        "\n",
        "* Given the perplexity, explain the motivation behind using $K$ or $\\epsilon$ graphs in the context of the algorithm?\n",
        "* How will the algorithm behave for $p \\to \\infty$?\n",
        "\n",
        "</br>\n",
        "\n",
        "* <font color='brown'>(**#**)</font> The definition for $K$ or $\\epsilon$ graphs is given in the lecture notes of _IsoMap_ and _Laplacian Eigenmaps_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JInCbYnOZUya"
      },
      "source": [
        "### 6.2. Solution\n",
        "\n",
        "The perplexity hyperparameter p in t-SNE controls the effective number of neighbors each data point has. A larger p leads to more neighbors.\n",
        "\n",
        "The t-SNE algorithm constructs two graphs on the data:\n",
        "\n",
        "1) K-nearest neighbor graph on the high-dimensional input data\n",
        "2) ε-neighborhood graph on the low-dimensional embeddings\n",
        "\n",
        "The motivation is:\n",
        "\n",
        "- K-NN graph on input data: This captures the local structure of the high-dimensional data. Using a fixed K ensures each point has a controlled number of neighbors.\n",
        "\n",
        "- ε-graph on embeddings: This allows flexible connectivity based on proximity in the low-dimensional space. Points close together connect, without a fixed K.\n",
        "\n",
        "As p → ∞:\n",
        "\n",
        "- The K-NN graph on input data becomes very dense, connecting many points together. This loses local structure information.\n",
        "\n",
        "- The ε-graph on embeddings also becomes dense. All points tend to be connected regardless of low-dimensional proximity.\n",
        "\n",
        "- This causes the t-SNE optimization to focus less on preserving local structure. The embedding becomes less meaningful.\n",
        "\n",
        "In summary:\n",
        "\n",
        "- K-NN and ε-graphs are used to capture local neighborhood structure at different scales.\n",
        "\n",
        "- A very large p loses this important structural information, causing t-SNE to perform poorly.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

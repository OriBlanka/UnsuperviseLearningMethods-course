{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 004 - Part IV\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 1.0.000 | 09/09/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_08/Exercise0004Part004.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m pairwise_distances\n\u001b[0;32m     12\u001b[0m \u001b[39m# from umap import UMAP\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# import umap\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mumap\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mumap_\u001b[39;00m \u001b[39mimport\u001b[39;00m UMAP\n\u001b[0;32m     16\u001b[0m \u001b[39m# Computer Vision\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[39m# Miscellaneous\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, MDS, TSNE\n",
    "from sklearn.metrics import pairwise_distances\n",
    "# from umap import UMAP\n",
    "import umap\n",
    "\n",
    "# Computer Vision\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import math\n",
    "from platform import python_version\n",
    "import random\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_FILE_URL   = r'https://drive.google.com/uc?export=download&confirm=9iBg&id=1UXLdZgXwClgwZVszRq88UaaN2nvgMFiC'\n",
    "DATA_FILE_NAME  = r'BciData.npz'\n",
    "\n",
    "TOTAL_RUN_TIME = 90 #<! Don't touch it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def Plot2DScatter(mX: np.ndarray, hA: plt.Axes, vC: np.ndarray = None) -> None:\n",
    "    m = mX.min()\n",
    "    M = mX.max()\n",
    "    if vC is not None:\n",
    "        hA.scatter(*mX.T, s = 50,  c = vC, edgecolor = 'k', alpha = 1)\n",
    "    else:\n",
    "        hA.scatter(*mX.T, s = 50,  c = 'lime', edgecolor = 'k', alpha = 1)\n",
    "    hA.set_xlim([m, M])\n",
    "    hA.set_ylim([m, M])\n",
    "    hA.set_xlabel('$x_1$')\n",
    "    hA.set_ylabel('$x_2$')\n",
    "\n",
    "\n",
    "def PlotLabelsHistogram(vY: np.ndarray, hA = None, lClass = None, xLabelRot: int = None) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
    "\n",
    "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
    "    hA.set_title('Histogram of Classes / Labels')\n",
    "    hA.set_xlabel('Class')\n",
    "    hA.set_ylabel('Number of Samples')\n",
    "    hA.set_xticks(vLabels)\n",
    "    if lClass is not None:\n",
    "        hA.set_xticklabels(lClass)\n",
    "    \n",
    "    if xLabelRot is not None:\n",
    "        for xLabel in hA.get_xticklabels():\n",
    "            xLabel.set_rotation(xLabelRot)\n",
    "\n",
    "    return hA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Time\n",
    "print(f'The total run time must not exceed: {TOTAL_RUN_TIME} [Sec]')\n",
    "startTime = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The `Import Packages` section above imports most needed tools to apply the work. Please use it.\n",
    "* <font color='brown'>(**#**)</font> You may replace the suggested functions to use with functions from other packages.\n",
    "* <font color='brown'>(**#**)</font> Whatever not said explicitly to implement maybe used by a 3rd party packages."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    " - `Ori_Blanka_208994764`.\n",
    " - `Or_Benson_308577345`.\n",
    " - `Alon_Hertz_315682773`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students Packages to Import\n",
    "# If you need a package not listed above, use this cell\n",
    "# Do not use `pip install` in the submitted notebook\n",
    "from scipy.linalg import sqrtm, inv, logm, eigvalsh\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "# This section downloads data from the given URL if needed.\n",
    "\n",
    "if (DATA_FILE_NAME != 'None') and (not os.path.exists(DATA_FILE_NAME)):\n",
    "    urllib.request.urlretrieve(DATA_FILE_URL, DATA_FILE_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. T-SNE and UMAP\n",
    "\n",
    "In this section we'll use the _t-SNE_ and _UMAP_ algorithms to analyze EEG Signals in the context of _Brain Computer Interface_ (BCI).  \n",
    "\n",
    "### The BCI Data Set\n",
    "\n",
    "The Brain Computer Interfaces (BCI) Data from [BCI Competition IV](https://www.bbci.de/competition/iv/).  \n",
    "Specifically we'll use [data set 2a](https://www.bbci.de/competition/iv/#dataset2a) provided by the Institute for Knowledge Discovery (Laboratory of Brain Computer Interfaces), Graz University of Technology, (Clemens Brunner, Robert Leeb, Gernot Müller-Putz, Alois Schlögl, Gert Pfurtscheller).  \n",
    "This is a recording of EEG signals while the subject is doing a cued movement of the left hand, right hand, feet or tongue.\n",
    "\n",
    "The data is composed of:\n",
    "\n",
    " * 22 EEG channels (0.5-100Hz; notch filtered).\n",
    " * Sampling Rate of 250 [Hz] for 4 [Sec] for total of 1000 samples.\n",
    " * 4 classes: _left hand_, _right hand_, _feet_, _tongue_.\n",
    " * 9 subjects.\n",
    " * 287 measurements.\n",
    "\n",
    "</br>\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You should install [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html) on your system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "\n",
    "dData = np.load(DATA_FILE_NAME)\n",
    "mX    = dData['mX1']\n",
    "vY    = dData['vY1']\n",
    "\n",
    "# Sample: An observation of a subject.\n",
    "# Measurement: Sample in time of the EEG signal.\n",
    "# Channel: EEG Channel.\n",
    "numSamples, numMeasurements, numChannels = mX.shape \n",
    "lLabel = ['Left Hand', 'Right Hand', 'Foot', 'Tongue'] #<! The labels\n",
    "\n",
    "print(f'The data shape: {mX.shape}')\n",
    "print(f'The number of samples: {mX.shape[0]}')\n",
    "print(f'The number of measurements per sample: {mX.shape[1]}')\n",
    "print(f'The number of EEG channels: {mX.shape[2]}')\n",
    "print(f'The classes labels: {np.unique(vY)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> In the above `sample` is used per object detected.\n",
    "* <font color='brown'>(**#**)</font> In the above `measurement` is used to describe the time sample (Like in Signal Processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "\n",
    "numRowsPlot = 3\n",
    "numColsPlot = 2\n",
    "\n",
    "numPlots = numRowsPlot * numColsPlot\n",
    "\n",
    "hF, hAs = plt.subplots(nrows = numRowsPlot, ncols = numColsPlot, figsize = (18, 10))\n",
    "hAs = hAs.flat\n",
    "\n",
    "vIdx = np.random.choice(numSamples, numPlots, replace = False)\n",
    "\n",
    "for sampleIdx, hA in zip(vIdx, hAs):\n",
    "    mXX = mX[sampleIdx, :, :]\n",
    "    hA.plot(mXX)\n",
    "    hA.set_title(f'EEG Signals of Sample {sampleIdx:04d} with Label {lLabel[vY[sampleIdx]]}')\n",
    "    hA.set_xlabel('Measurement Index')\n",
    "    hA.set_ylabel('Measurement Value')\n",
    "\n",
    "hF.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "# Show a shifted and scaled channels at a single plot.\n",
    "# You may run it several times to observe different measurements.\n",
    "\n",
    "numSamples, numMeasurements, numChannels = mX.shape #<! Samples, Time Samples, Channels\n",
    "idx     = np.random.randint(numSamples)\n",
    "mXi     = mX[idx, :, :].copy()\n",
    "yi      = vY[idx]\n",
    "\n",
    "# Normalizing and Vertically Shifting\n",
    "mXi -= mXi.mean(0)\n",
    "mXi /= 20\n",
    "mXi += np.arange(numChannels)[None, :]\n",
    "vT   = np.linspace(0, 4, numMeasurements, endpoint = False)\n",
    "\n",
    "hF, hA = plt.subplots(figsize = (10, 6))\n",
    "hA.plot(vT, mXi)\n",
    "hA.set_title (f'Raw EEG Data\\nTrue Label = {lLabel[yi]}')\n",
    "hA.set_xlabel('Time [Sec]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Labels\n",
    "\n",
    "hA = PlotLabelsHistogram(vY, lClass = lLabel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The data is balanced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1. Processing RAW Data\n",
    "\n",
    "In this section we'll compare 4 dimensionality reduction methods:\n",
    "\n",
    " - IsoMap.\n",
    " - MDS.\n",
    " - t-SNE.\n",
    " - UMAP.\n",
    "\n",
    "We'll reduce the data dimension to `d = 2` and use the reference labels to display result.\n",
    "\n",
    "The steps:\n",
    "\n",
    "1. Data Pre Processing  \n",
    "     * Reshape data into `mXRaw` such that each measurement is a row.  \n",
    "       Namely the data should have dimensions of `287 x 22_000`.  \n",
    "     * Apply dimensionality reduction using PCA to keep most of the data energy.\n",
    "2. Set Parameters  \n",
    "   Set parameters of each method to get results.  \n",
    "   Set the amount of energy preserved using the PCA pre processing.\n",
    "   **No need for grid search, just try and error of few values**.\n",
    "3. Apply the Transform  \n",
    "   Apply the transform on the data.  \n",
    "   The low dimensional data should be in 2D space.  \n",
    "4. Display Results  \n",
    "   Create a subplot per method showing the results and the used parameters.  \n",
    "\n",
    "* <font color='brown'>(**#**)</font> No need to self implement any of the methods. You should use SciKit Learn's implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, MDS, TSNE\n",
    "import umap\n",
    "\n",
    "# 1. Pre Process data\n",
    "# Reshape to (287 x 22,000).\n",
    "# Apply PCA to preserve `relEnergy` of the energy.\n",
    "\n",
    "# Reshape the data into (287 x 22,000)\n",
    "mX_reshaped = mX.reshape(numSamples, -1)\n",
    "\n",
    "# Apply PCA to keep most of the data energy (adjust the value of n_components)\n",
    "relEnergy = 0.95  # Adjust this value based on desired energy preservation\n",
    "pca = PCA(n_components=relEnergy)\n",
    "mX_pca = pca.fit_transform(mX_reshaped)\n",
    "\n",
    "# 2. Apply Dimensionality Reduction (d = 2) using IsoMap, MDS, t-SNE, and UMAP\n",
    "# Choose reasonable parameters for each method\n",
    "\n",
    "# IsoMap\n",
    "isomap = Isomap(n_neighbors=5, n_components=2)  # Set appropriate values\n",
    "mX_isomap = isomap.fit_transform(mX_reshaped)\n",
    "\n",
    "# MDS\n",
    "mds = MDS(n_components=2)  # Set appropriate values\n",
    "mX_mds = mds.fit_transform(mX_reshaped)\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)  # Adjust parameters as needed\n",
    "mX_tsne = tsne.fit_transform(mX_reshaped)\n",
    "\n",
    "# UMAP\n",
    "umap_model = umap.UMAP(n_components=2, n_neighbors=10, min_dist=0.1)  # Adjust parameters\n",
    "mX_umap = umap_model.fit_transform(mX_reshaped)\n",
    "\n",
    "# 3. Display the Low Dimensionality data\n",
    "# Include the parameters in the title of each method.\n",
    "# Create a figure with subplots (1x4) for each method\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "methods = [\"IsoMap\", \"MDS\", \"t-SNE\", \"UMAP\"]\n",
    "data = [mX_isomap, mX_mds, mX_tsne, mX_umap]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.scatter(data[i][:, 0], data[i][:, 1], c=vY, cmap='viridis')\n",
    "    ax.set_title(f'{methods[i]} (d=2)')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 Question\n",
    "\n",
    "1. What's the purpose of the PCA pre process?\n",
    "2. Explain the results. Think in the context of being able to classify the body part in action."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 Solution\n",
    "\n",
    "1. Purpose of PCA Preprocessing:\n",
    "\n",
    "The purpose of PCA (Principal Component Analysis) preprocessing in this context is to reduce the dimensionality of the EEG data while preserving most of its energy. This serves several purposes:\n",
    "\n",
    "        Dimensionality Reduction: EEG data is typically high-dimensional, with many channels and measurements. High dimensionality can lead to computational complexity and may require more data to model effectively. PCA reduces this high-dimensional data to a lower-dimensional representation while retaining as much useful information as possible.\n",
    "\n",
    "        Noise Reduction: By focusing on the principal components that capture the most variation in the data, PCA can help reduce noise and emphasize the underlying patterns or features that are relevant for analysis.\n",
    "\n",
    "        Visualization: PCA provides a way to visualize high-dimensional data in a lower-dimensional space, making it easier to explore and understand the data. In this case, reducing the data to 2D allows for visualization of EEG data in a 2D scatter plot.\n",
    "\n",
    "2. Explanation of Results:\n",
    "\n",
    "The results of dimensionality reduction using IsoMap, MDS, t-SNE, and UMAP can be interpreted as follows:\n",
    "\n",
    "        IsoMap: IsoMap aims to preserve the geodesic distances between data points in the lower-dimensional space. As a result, it can reveal the intrinsic structure of the data. In the context of classifying body parts in action, you might observe clusters of data points corresponding to different actions. If the clustering is clear, it suggests that the actions have distinct patterns in the EEG data.\n",
    "\n",
    "        MDS (Multi-Dimensional Scaling): MDS tries to preserve pairwise distances between data points. If the MDS plot shows clear separation between classes (body parts), it indicates that the EEG patterns associated with different body parts are distinct.\n",
    "\n",
    "        t-SNE (t-Distributed Stochastic Neighbor Embedding): t-SNE is effective at capturing local structure in the data, meaning that similar data points tend to be close to each other in the lower-dimensional space. In the context of body part classification, if t-SNE results in tight clusters for each body part, it suggests that the EEG patterns are discriminative.\n",
    "\n",
    "        UMAP (Uniform Manifold Approximation and Projection): UMAP is similar to t-SNE but offers greater control over the balance between preserving local and global structure. Clear separation of body parts in a UMAP plot indicates that the method has captured meaningful patterns.\n",
    "\n",
    "Overall, the goal of these dimensionality reduction techniques is to transform the high-dimensional EEG data into a lower-dimensional space where differences between body parts are visually apparent. The effectiveness of these methods can be assessed by examining whether they produce distinct clusters or groupings for different body parts, which would aid in their classification.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Feature Engineering\n",
    "\n",
    "Form the above it is clear the RAW measurements are not a good representation of the data.  \n",
    "Since each sample is basically $\\boldsymbol{X}_{i}\\in\\mathbb{R}^{1000 \\times 22}$ we can work on the time axis of the data.  \n",
    "\n",
    "The strategy is as following:\n",
    "\n",
    " * Use the Covariance Matrix is a feature of the sample.\n",
    " * Use a metric optimized to covariance matrices.\n",
    "\n",
    "Do the following steps:\n",
    "\n",
    "1. Implements `GenCovMat()`  \n",
    "   For each sample $\\boldsymbol{X}_{i}\\in\\mathbb{R}^{1000 \\times 22}$, compute the covariance matrix $\\boldsymbol{C}_{i}\\in\\mathbb{R}^{22\\times22}$.  \n",
    "   You may use [`np.cov()`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html).\n",
    "2. Implement `SpdMetric()`  \n",
    "   Given 2 SPD matrices in the form of vectors computes the geodesic distance between them.\n",
    "3. Apply the Feature Engineering  \n",
    "   The end of this process should be a matrix `mZ` with dimensions (287 x 22^2)\n",
    "4. Generate the Distance Matrix  \n",
    "   Using `SpdMetric()` generate the distance matrix of the data.  \n",
    "   The output should be a matrix `mD` with dimensions (287 x 287).\n",
    "\n",
    "#### The SPD Matrices Metric\n",
    "\n",
    "An optimized metric for the manifold of SPD Matrices, is given by the SPD Metric (`SpdMetric`):\n",
    "\n",
    "$$d\\left(\\boldsymbol{C}_{i},\\boldsymbol{C}_{j}\\right)=\\sqrt{\\sum_{i=1}^{d}\\log^{2}\\left(\\lambda_{i}\\left(\\boldsymbol{C}_{i}^{-1}\\boldsymbol{C}_{j}\\right)\\right)}$$\n",
    "\n",
    "Where ${\\lambda}_{i} \\left( \\cdot \\right)$ extract the $i$ -th eigen value of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenCovMat(mX: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates the covariance matrices of the input data.\n",
    "    Args:\n",
    "        mX - Input data with shape (N x T x C)\n",
    "    Output:\n",
    "        mC - N covariance matrices with shape (N x C x C)\n",
    "    '''\n",
    "\n",
    "    # 1. Calculate the covariance matrices array.\n",
    "    # Use np.cov to calculate the covariance matrices for each sample in mX\n",
    "    # Make sure to set rowvar=False to treat each column as a variable\n",
    "\n",
    "    mC = np.array([np.cov(sample, rowvar=False) for sample in mX])\n",
    "\n",
    "    return mC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpdMetric(vP: np.ndarray, vQ: np.ndarray) -> float:\n",
    "    '''\n",
    "    Calculates the AIRM geodesic distance between two SPD matrices.\n",
    "    Args:\n",
    "        vP    - An SPD matrix in the form of a vector with shape (d^2, )\n",
    "        vQ    - An SPD matrix in the form of a vector with shape (d^2, )\n",
    "    Output:\n",
    "        geoDist - The geodesic distance, dist ≥ 0\n",
    "    '''\n",
    "\n",
    "    # 1. Convert the vectors into a matrix.\n",
    "    # Reshape the vectors into square matrices.\n",
    "    d = int(np.sqrt(len(vP)))\n",
    "    mP = vP.reshape((d, d))\n",
    "    mQ = vQ.reshape((d, d))\n",
    "\n",
    "    # 2. Calculate the geodesic distance of SPD matrices.\n",
    "    # Calculate the square root of P and Q matrices\n",
    "    sqrtP = sqrtm(mP)\n",
    "    sqrtQ = sqrtm(mQ)\n",
    "\n",
    "    # Calculate the matrix logarithm of sqrtP * inv(sqrtQ)\n",
    "    matrix_log = logm(np.dot(np.dot(sqrtP, inv(sqrtQ)), sqrtP))\n",
    "\n",
    "    # Calculate the eigenvalues of the matrix log\n",
    "    eigenvalues = eigvalsh(matrix_log)\n",
    "\n",
    "    # Calculate the geodesic distance\n",
    "    geoDist = np.sqrt(np.sum(eigenvalues**2))\n",
    "\n",
    "    return geoDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Generate the covariance matrices.\n",
    "# 2. Reshape data into the samples matrix.\n",
    "# 3. Calculate the distance matrix.\n",
    "# !! Don't use any loop!\n",
    "# !! You may use `pairwise_distances()` in order to avoid loops.\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# 1. Generate the covariance matrices.\n",
    "mC = GenCovMat(mX)\n",
    "\n",
    "# 2. Reshape data into the samples matrix.\n",
    "numSamples, _, _ = mX.shape\n",
    "mX_samples = mX.reshape(numSamples, -1)\n",
    "\n",
    "# 3. Calculate the distance matrix.\n",
    "# Use pairwise_distances to calculate the pairwise distances between samples\n",
    "dist_matrix = pairwise_distances(mX_samples, metric=SpdMetric)\n",
    "\n",
    "# dist_matrix will be a square matrix where dist_matrix[i, j] represents the distance between sample i and sample j.\n",
    "\n",
    "# You can use dist_matrix for further analysis or visualization.\n",
    "\n",
    "#===============================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Low Dimensional Data\n",
    "# This shows the result using IsoMap with SpdMetric.\n",
    "\n",
    "oIsoMapDr = Isomap(n_neighbors = 3, n_components = d, metric = 'precomputed')\n",
    "\n",
    "hF = plt.figure(figsize = (12, 6))\n",
    "    \n",
    "mZi = oIsoMapDr.fit_transform(mD)\n",
    "hA = hF.add_subplot()\n",
    "Plot2DScatter(mZi, hA, vY)\n",
    "hA.set_title(f'IsoMap (SPD Metric) with d = {d}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.1. Question\n",
    "\n",
    "In the above the _IsoMap_ is used.  \n",
    "If one would like to use SciKit Learn's _Spectral Embedding_ (See [`SpectralEmbedding`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html)) one need to supply _Affinity Matrix_ (See `affinity` parameter).  \n",
    "Describe how would you generate such matrix based on `SpdMetric()`.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> SciKit Learn's _Spectral Embedding_ is equivalent to _Laplacian Eigenmaps_ on lecture notes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.1. Solution\n",
    "\n",
    "\n",
    "To generate an affinity matrix for SciKit Learn's SpectralEmbedding based on the SpdMetric() function, follow these steps:\n",
    "\n",
    "Calculate Pairwise Distances: Compute the pairwise distances between SPD matrices using the pairwise_distances() function with the SpdMetric metric. This will create a matrix of distances between all pairs of SPD matrices.\n",
    "\n",
    "Define Similarity Measure: In spectral embedding, we require an affinity matrix that represents similarity instead of distance. To achieve this, convert the distances into similarities using a Gaussian similarity measure:\n",
    "\n",
    "similarity_ij = exp(-gamma * distance_ij^2)\n",
    "Here, distance_ij denotes the pairwise distance between SPD matrices i and j, and gamma is a scaling parameter that controls the width of the Gaussian. Adjust the gamma value based on the dataset's characteristics.\n",
    "\n",
    "Create Affinity Matrix: Construct the affinity matrix by using the similarity values computed in step 2. Ensure that the affinity matrix is symmetric, with each element representing the similarity between two SPD matrices.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Dimensionality Reduction and Classification on Engineered Data\n",
    "\n",
    "In this section we'll focus on `t-SNE` and `UMAP`.  \n",
    "\n",
    "1. Apply Dimensionality Reduction  \n",
    "   Apply `t-SNE` and / or `UMAP` on the engineered data.  \n",
    "   Make sure to use the appropriate options to utilize the metric.  \n",
    "   Apply this on the whole data.\n",
    "2. Apply Classification  \n",
    "   Apply a classifier (From SciKit Learn, Neural Networks excluded) on the low dimensional data.\n",
    "3. Measure Performance of the Classifier  \n",
    "   Apply **leave one out cross validation** with score based on **accuracy**.  \n",
    "   You should use [`cross_val_predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html).  \n",
    "   This will generate a vector of 287 predictions.\n",
    "4. Display Results  \n",
    "   Display the confusion matrix and the total accuracy.\n",
    "\n",
    "**This is a competition**: \n",
    " \n",
    " * The group with the 1st highest score will get 4 bonus points.\n",
    " * The group with the 2nd highest score will get 2 bonus points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================Fill This===========================#\n",
    "# 1. Use t-SNE and/or UMAP for Dimensionality Reduction.\n",
    "# 2. On the output of (1) apply a classifier from SciKit Learn.\n",
    "# 3. To measure the performance use `cross_val_predict()` with accuracy as the score.\n",
    "# 4. Display the confusion matrix of the result of the `cross_val_predict()`.\n",
    "# !! Use leave one out as the validation policy.\n",
    "# !! Use the `SpdMetric()` for UMAP / t-SNE!\n",
    "# !! This is a competition, optimize the hyper parameters of each step for best accuracy.\n",
    "# !! No use of Neural Net based classifiers!\n",
    "# !! If you use t-SNE, make sure to make it non random (See `init` and `random_state`).\n",
    "# !! Pay attention to the run time limitation of the whole notebook.\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier  # You can choose a different classifier here\n",
    "\n",
    "# Load the engineered data\n",
    "# Replace 'engineered_data.npy' with the actual filename\n",
    "engineered_data = np.load('engineered_data.npy')\n",
    "\n",
    "# Extract features and labels from the engineered data\n",
    "X = engineered_data[:, :-1]  # Features (excluding the last column)\n",
    "y = engineered_data[:, -1]   # Labels (last column)\n",
    "\n",
    "# 1. Use t-SNE and/or UMAP for Dimensionality Reduction\n",
    "# Apply t-SNE with SpdMetric\n",
    "tsne = TSNE(n_components=2, metric=SpdMetric, init='pca', random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Apply UMAP with SpdMetric\n",
    "umap_model = umap.UMAP(n_components=2, n_neighbors=10, min_dist=0.1, metric=SpdMetric)\n",
    "X_umap = umap_model.fit_transform(X)\n",
    "\n",
    "# 2. Apply a Classifier\n",
    "# Choose and initialize a classifier (e.g., Random Forest)\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 3. Measure Performance with Leave-One-Out Cross-Validation\n",
    "# Use cross_val_predict for leave-one-out cross-validation\n",
    "y_pred_tsne = cross_val_predict(classifier, X_tsne, y, cv=X.shape[0])\n",
    "y_pred_umap = cross_val_predict(classifier, X_umap, y, cv=X.shape[0])\n",
    "\n",
    "# 4. Display Results\n",
    "# Calculate and display confusion matrices and accuracy scores\n",
    "cm_tsne = confusion_matrix(y, y_pred_tsne)\n",
    "cm_umap = confusion_matrix(y, y_pred_umap)\n",
    "\n",
    "accuracy_tsne = accuracy_score(y, y_pred_tsne)\n",
    "accuracy_umap = accuracy_score(y, y_pred_umap)\n",
    "\n",
    "# Display confusion matrices and accuracy scores\n",
    "print(\"t-SNE Confusion Matrix:\")\n",
    "print(cm_tsne)\n",
    "print(\"t-SNE Accuracy:\", accuracy_tsne)\n",
    "\n",
    "print(\"\\nUMAP Confusion Matrix:\")\n",
    "print(cm_umap)\n",
    "print(\"UMAP Accuracy:\", accuracy_umap)\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.1. Question\n",
    "\n",
    "In the above we had the whole data for the _dimensionality reduction_ step.  \n",
    "Assume we use a method without _out of sample extension_.  \n",
    "In the case above, how would you handle a new data point? Explain the pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.1. Solution\n",
    "\n",
    "\n",
    "If we use a dimensionality reduction method without out-of-sample extension (such as t-SNE or UMAP), handling a new data point would require us to follow a specific pipeline:\n",
    "\n",
    "1. Retrain the Dimensionality Reduction Model: When a new data point arrives, we need to retrain the dimensionality reduction model using the entire dataset (including the new data point). This step ensures that the new data point is embedded into the existing lower-dimensional space consistently with the rest of the data.\n",
    "\n",
    "2. Apply the Dimensionality Reduction Model: Once the dimensionality reduction model is retrained, we can apply it to the new data point to obtain its lower-dimensional representation. This representation will be consistent with the lower-dimensional space of the existing dataset.\n",
    "\n",
    "3. Use the Classifier: After obtaining the lower-dimensional representation of the new data point, we can then use the trained classifier to make predictions or perform classification tasks on the new data point. The classifier should be able to operate in the lower-dimensional space.\n",
    "\n",
    "Some key points to remember:\n",
    "\n",
    "- Retraining the dimensionality reduction model is necessary because these techniques don't inherently support out-of-sample extension. They work by optimizing embeddings for the specific dataset they were trained on.\n",
    "\n",
    "- The lower-dimensional space obtained from the retrained model should align with the space in which the classifier was originally trained. This alignment ensures that the classifier can effectively make predictions on the new data point.\n",
    "\n",
    "- It's important to maintain consistency in preprocessing steps (e.g., feature scaling or any other data transformations) between the existing dataset and the new data point to ensure accurate embeddings and predictions.\n",
    "\n",
    "-  Depending on the complexity and runtime constraints, it may be necessary to periodically retrain the dimensionality reduction model on the entire dataset, especially if new data points are added frequently.\n",
    "\n",
    "Overall, handling new data points with dimensionality reduction methods that lack out-of-sample extension capabilities can be computationally intensive, especially for large datasets. The key is to ensure that the lower-dimensional space remains consistent and meaningful when embedding new data points.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Time\n",
    "# Check Total Run Time.\n",
    "# Don't change this!\n",
    "\n",
    "endTime = time.time()\n",
    "\n",
    "totalRunTime = endTime - startTime\n",
    "print(f'Total Run Time: {totalRunTime} [Sec].')\n",
    "\n",
    "if (totalRunTime > TOTAL_RUN_TIME):\n",
    "    raise ValueError(f'You have exceeded the allowed run time as {totalRunTime} > {TOTAL_RUN_TIME}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

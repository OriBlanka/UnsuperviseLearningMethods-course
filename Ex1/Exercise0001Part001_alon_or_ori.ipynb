{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgS52NKmzJDn"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# UnSupervised Learning Methods\n",
        "\n",
        "## Exercise 001 - Part I\n",
        "\n",
        "> Notebook by:\n",
        "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "| 1.0.000 | 15/08/2023 | Royi Avital | First version                                                      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By-RUaW9zJDq"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_08/Exercise0001Part001.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPvdY40lzJDr"
      },
      "source": [
        "## Notations\n",
        "\n",
        "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
        "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
        "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
        "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F-xvfwhzJDr"
      },
      "source": [
        "## Guidelines\n",
        "\n",
        " - Fill the full names of the team memebers in the `Team Members` section.\n",
        " - Answer all questions within the Jupyter Notebook.\n",
        " - Open questions are in part I of the exercise.\n",
        " - Coding based questions are in the subsequent notebooks.\n",
        " - Use MarkDown + MathJaX + Code to answer.\n",
        " - Submission in groups (Single submission per group).\n",
        " - You may and _should_ use the forums for question.\n",
        " - Good Luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25C90XjpzJDs"
      },
      "source": [
        "## Team Members\n",
        "\n",
        " - `<FULL>_<NAME>_<ID001>`.\n",
        " - `<FULL>_<NAME>_<ID002>`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgH-_QWxzJDs"
      },
      "source": [
        "## 0. Linear Algebra\n",
        "\n",
        "A matrix $ P $ is called an orthogonal projection operator if, and only if it is idempotent and symmetric.\n",
        "\n",
        "**Remark**: Idempotent matrix means $ \\forall n \\in \\mathcal{N} \\; {P}^{n} = P $.\n",
        "\n",
        "### 0.1. Question\n",
        "\n",
        "Let $A \\in \\mathbb{R}^{m \\times n}$ where $ m \\geq n $ and $ \\operatorname{Rank} \\left( A \\right) = n $.  \n",
        "Given the linear least squares problem:\n",
        "\n",
        "$$ \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| A \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "With the solution in the form $\\hat{\\boldsymbol{x}} = R \\boldsymbol{y}$, show that $P = A R$ is an orthogonal projection operator.\n",
        "\n",
        "**Hints**\n",
        "\n",
        "1. Derive the solution to the Least Squares above in the form of $ \\hat{\\boldsymbol{x}} = R \\boldsymbol{y} $.\n",
        "2. Show the $ P $ matrix is symmetric.\n",
        "3. Show the $ P $ matrix is idempotent.\n",
        "4. Conclude the matrix is an orthogonal projection operator.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> The matrix $P$ is the Orthogonal Projection onto the range (Columns space) of $ A $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg63tZTizJDs"
      },
      "source": [
        "### 0.1. Solution\n",
        "\n",
        "1.\\\n",
        "Similarly to what we show later in 2.5:\\\n",
        "$ f \\left( \\boldsymbol{x} \\right) = \\frac1 2{\\left\\| \\boldsymbol{A} \\boldsymbol{x} -\\boldsymbol{y}\\right\\|}_{2}^{2} = \\frac1 2\\left\\langle \\boldsymbol{A} \\boldsymbol{x} -\\boldsymbol{y}, \\boldsymbol{A} \\boldsymbol{x} -\\boldsymbol{y}\\right\\rangle$\n",
        "\n",
        "$\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]= \\nabla \\frac 1 2\\left\\langle \\boldsymbol{A} \\boldsymbol{x} -\\boldsymbol{y}, \\boldsymbol{A} \\boldsymbol{x} -\\boldsymbol{y}\\right\\rangle \\left[ \\boldsymbol{h} \\right]$\\\n",
        "(by product rule) = $ \\frac1 2\\left\\langle \\nabla(\\boldsymbol{A} \\boldsymbol{x} -\\boldsymbol{y})\\left[ \\boldsymbol{h} \\right], \\boldsymbol{A} \\boldsymbol{x} -\\boldsymbol{y}\\right\\rangle + \\frac1 2\\left\\langle \\boldsymbol{A} \\boldsymbol{x} -\\boldsymbol{y}, \\nabla(\\boldsymbol{A} \\boldsymbol{x} -\\boldsymbol{y})\\left[ \\boldsymbol{h} \\right]\\right\\rangle$\\\n",
        "(by linearity 2.2) = $ \\frac1 2\\left\\langle \\boldsymbol{A}\\boldsymbol{h}, \\boldsymbol{A} \\boldsymbol{x} - \\boldsymbol{y} \\right\\rangle + \\frac1 2\\left\\langle \\boldsymbol{A} \\boldsymbol{x} - \\boldsymbol{y}, \\boldsymbol{A}\\boldsymbol{h} \\right\\rangle=\n",
        "\\left\\langle \\boldsymbol{A} \\boldsymbol{x} - \\boldsymbol{y}, \\boldsymbol{A}\\boldsymbol{h} \\right\\rangle = \\left\\langle \\boldsymbol{A}^\\top(\\boldsymbol{A} \\boldsymbol{x} - \\boldsymbol{y}), \\boldsymbol{h} \\right\\rangle$\n",
        "\n",
        "$\\implies  \\nabla f \\left( \\boldsymbol{x} \\right) = \\boldsymbol{A}^\\top(\\boldsymbol{A} \\boldsymbol{x} - \\boldsymbol{y})$\n",
        "\n",
        "Since $\\boldsymbol{A}$ is rank $n$, $\\boldsymbol{A}^\\top\\boldsymbol{A}$ is an $n\\times n$ matrix of full rank $n$ and therefore invertible.\n",
        "\n",
        "$\\nabla f \\left( \\boldsymbol{x} \\right) \\stackrel{!}{=} 0 \\implies \\boldsymbol{A}^\\top(\\boldsymbol{A} \\boldsymbol{x} - \\boldsymbol{y}) = 0 \\implies \\hat{\\boldsymbol{x}} = (\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top \\boldsymbol{y}$\\\n",
        "$\\boldsymbol{R} = (\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top$ \\\n",
        "$\\boldsymbol{P} = \\boldsymbol{A}\\boldsymbol{R} = \\boldsymbol{A}(\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top$ \\\n",
        "\n",
        "2.\\\n",
        "Lemma 2.1: $\\boldsymbol{B} = \\boldsymbol{A}^\\top\\boldsymbol{A}$ is symmetric.\\\n",
        "Proof: $\\boldsymbol{B}_{ij} = \\sum_k \\boldsymbol{A}^\\top_{ik}\\boldsymbol{A}_{kj} = \\sum_k \\boldsymbol{A}_{ki}\\boldsymbol{A}^\\top_{jk} = \\sum_k \\boldsymbol{A}^\\top_{jk}\\boldsymbol{A}_{ki} = \\boldsymbol{B}_{ji}$\n",
        "\n",
        "Lemma 2.2: The inverse of an invertible and symmetric matrix $\\boldsymbol{B}$ is symmetric\\\n",
        "Proof: $\\boldsymbol{B}^{-1}\\boldsymbol{B} = \\boldsymbol{B}\\boldsymbol{B}^{-1} = \\boldsymbol{I}$ and $\\boldsymbol{I}^\\top = \\boldsymbol{I}$.\\\n",
        "$\\implies \\boldsymbol{B}^{-1}\\boldsymbol{B} = (\\boldsymbol{B}\\boldsymbol{B}^{-1})^\\top = (\\boldsymbol{B}^{-1})^\\top \\boldsymbol{B}^\\top = (\\boldsymbol{B}^{-1})^\\top \\boldsymbol{B} \\implies \\boldsymbol{B}^{-1} = (\\boldsymbol{B}^{-1})^\\top$\n",
        "\n",
        "Therefore:\n",
        "\n",
        "\n",
        "$\\boldsymbol{P}^\\top = (\\boldsymbol{A}(\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top)^\\top = \\boldsymbol{A}((\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1})^\\top\\boldsymbol{A}^\\top = \\boldsymbol{A}(\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top = \\boldsymbol{P}$\\\n",
        "$\\implies \\boldsymbol{P}$ is symmetric.\n",
        "\n",
        "3.\\\n",
        "$\\boldsymbol{P}^2 = (\\boldsymbol{A}(\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top)^2 = \\boldsymbol{A}(\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top\\boldsymbol{A}(\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top = \\boldsymbol{A}\\stackrel{= \\boldsymbol{I}}{\\left[(\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top\\boldsymbol{A}\\right](}\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top = \\boldsymbol{A}(\\boldsymbol{A}^\\top\\boldsymbol{A})^{-1}\\boldsymbol{A}^\\top = \\boldsymbol{P}$\\\n",
        "By induction: assume $\\boldsymbol{P}^{n-1} = \\boldsymbol{P}$\\\n",
        "$\\implies \\boldsymbol{P}^{n} = \\boldsymbol{P}^{n-1}\\boldsymbol{P} = \\boldsymbol{P}\\boldsymbol{P} = \\boldsymbol{P}$\\\n",
        "$\\implies \\boldsymbol{P}$ is idempotent\n",
        "\n",
        "4.\\\n",
        "$\\boldsymbol{P}$ is symmetric and idempotent and therefore it is an orthogonal projection operator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEWsn8DEzJDt"
      },
      "source": [
        "An orthogonal projection $ \\boldsymbol{P} $ onto a sub space $ V $ obeys:\n",
        "\n",
        "1. $ \\forall \\boldsymbol{w} : \\boldsymbol{P} \\boldsymbol{w} \\in V $.\n",
        "2. $ \\boldsymbol{w} - \\boldsymbol{P} \\boldsymbol{w} \\in {V}^{\\bot} $.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> If $ \\boldsymbol{v} \\in {V}, \\boldsymbol{u} \\in {V}^{\\bot} $ then $ \\left \\langle \\boldsymbol{u}, \\boldsymbol{v} \\right \\rangle = 0 $.\n",
        "\n",
        "The orthogonal projection onto the set $ \\left\\{ \\boldsymbol{x} \\mid \\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b} \\right\\} $ is given by:\n",
        "\n",
        "$$ \\operatorname{Proj}_{ \\left\\{ \\boldsymbol{x} \\mid \\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{b} \\right\\} } \\left( \\boldsymbol{z} \\right) = \\boldsymbol{z} - \\boldsymbol{A}^{T} {\\left( \\boldsymbol{A} \\boldsymbol{A}^{T} \\right)}^{-1} \\left( \\boldsymbol{A} \\boldsymbol{z} - \\boldsymbol{b} \\right) $$\n",
        "\n",
        "### 0.2. Question  \n",
        "\n",
        "Prove or disprove: The above is orthogonal projection onto a sub space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar6LgkTLzJDt"
      },
      "source": [
        "### 0.2. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFUnGJuFzJDu"
      },
      "source": [
        "## 1. Convexity\n",
        "\n",
        "**Convex Set**  \n",
        "\n",
        "Let:\n",
        "\n",
        "$$ \\mathbb{R}_{\\geq 0}^{d} = \\left\\{ \\boldsymbol{x} \\in\\mathbb{R}^{d} \\, \\bigg| \\, \\min_{i} {x}_{i} \\geq 0 \\right\\} $$\n",
        "\n",
        "Where $\\boldsymbol{x} = \\begin{bmatrix} {x}_{1} \\\\ {x}_{2} \\\\ \\vdots \\\\ {x}_{d} \\end{bmatrix}$\n",
        "\n",
        "### 1.1. Question\n",
        "\n",
        "Prove or disprove that $\\mathbb{R}_{\\geq 0}^{d}$ is convex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nkloZcjzJDu"
      },
      "source": [
        "### 1.1. Solution\n",
        "\n",
        "Let $ x, y \\in \\mathbb{R}_{\\geq 0}^{d},   \\alpha \\in [0, 1]. $\n",
        "$ \\min_{i} {x}_{i} \\geq 0 $ and $ \\min_{i} {y}_{i} \\geq 0. $ Hence\n",
        "$ ∀i: x_{i} \\geq 0 , y_{i} \\geq 0$\n",
        "\n",
        "See the convex combination $ αx + (1-α)y $\n",
        "$ \\forall{i} $ it holds:\n",
        "$ a \\geq 0  → a{x}_{i} \\geq 0 $\n",
        "$ (1-α) \\geq 0  → (1-α)y_{i} \\geq 0 $\n",
        "Therefore:\n",
        "$ ax_{i} + (1-α)y_{i} \\geq 0 $\n",
        "So $ \\min_{i} ax_{i} + (1-α)y_{i} \\geq 0$ and\n",
        "$ ax + (1-α)y\\in \\mathbb{R}_{\\geq 0}^{d} $\n",
        "so $ \\mathbb{R}_{\\geq 0}^{d} $ is convex.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0ZbGTnHzJDu"
      },
      "source": [
        "**Convex Combination**\n",
        "\n",
        "Let $\\mathcal{C} \\subseteq \\mathbb{R}^{d} $ be a convex set and consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\} _{i=1}^{N}$.\n",
        "\n",
        "### 1.2. Question\n",
        "\n",
        "Prove that for any $N \\in \\mathbb{N}$:\n",
        "\n",
        "$$ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} \\in \\mathcal{C} $$\n",
        "\n",
        "Where $\\alpha_{i}$ are such that:\n",
        "\n",
        " - $\\forall i, \\; \\alpha_{i} \\geq 0$.\n",
        " - $\\sum_{i = 1}^{N} \\alpha_{i} = 1$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> The properties of ${\\alpha}_{i}$ above means it is sampled from the Unit Probability Simplex.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5jBccATzJDu"
      },
      "source": [
        "### 1.2. Solution\n",
        "\n",
        "Assume that for N-1: $$ \\sum_{i = 1}^{N-1} {\\alpha}_{i} \\boldsymbol{x}_{i} \\in \\mathcal{C} $$\n",
        "\n",
        "Where $\\alpha_{i}$ are such that:\n",
        "\n",
        " - $\\forall i, \\; \\alpha_{i} \\geq 0$.\n",
        " - $\\sum_{i = 1}^{N-1} \\alpha_{i} = 1$.\n",
        "\n",
        " Let's now consider $ \\boldsymbol{x}_{n} \\in \\mathcal{C} $ with the combination:\n",
        "$$ \\sum_{i = 1}^{N-1} {\\beta}_{i} \\boldsymbol{x}_{i} + \\beta_{n}\\boldsymbol{x}_{n} $$\n",
        "Where $\\beta{i}$ are such that:\n",
        "\n",
        " - $\\forall i, \\; \\beta{i} \\geq 0$.\n",
        " - $\\sum_{i = 1}^{N} \\beta_{i} = 1$.\n",
        "\n",
        " We can infer that:\n",
        "$$ \\sum_{i = 1}^{N-1} {\\beta}_{i} \\boldsymbol{x}_{i} + \\beta_{n}\\boldsymbol{x}_{n} = ({\\sum_{j = 1}^{N-1} {\\beta}_{j}})\\sum_{i = 1}^{N-1} \\frac{{\\beta}_{i}}{(\\sum_{j = 1}^{N-1} {\\beta}_{j})} \\boldsymbol{x}_{i} + \\beta_{n}\\boldsymbol{x}_{n} $$\n",
        "Let's mark $ K =  \\sum_{j = 1}^{N-1} {\\beta}_{j}$.   See the combination $ \\sum_{i = 1}^{N-1} {\\frac{\\beta_{i}}{K}\\boldsymbol{x}_{i}} $:\n",
        " - $\\forall i, \\; \\frac{\\beta_{i}}{K} \\geq 0$.\n",
        " - $\\sum_{i = 1}^{N-1} \\frac{\\beta_{i}}{K} = \\frac{1}{K}\\sum_{i = 1}^{N-1} \\beta_{i} = 1$.\n",
        "\n",
        " Hence, by the induction assumption we know that $ \\sum_{i = 1}^{N-1} \\frac{{\\beta}_{i}}{K} \\boldsymbol{x}_{i} \\in \\mathcal{C} $. Let's mark $ \\sum_{i = 1}^{N-1} \\frac{{\\beta}_{i}}{K} \\boldsymbol{x}_{i} = \\boldsymbol{z} \\in \\mathcal{C} $\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$$ \\sum_{i = 1}^{N-1} {\\beta}_{i} \\boldsymbol{x}_{i} + \\beta_{n}\\boldsymbol{x}_{n} = ({\\sum_{j = 1}^{N-1} {\\beta}_{j}})\\sum_{i = 1}^{N-1} \\frac{{\\beta}_{i}}{(\\sum_{j = 1}^{N-1} {\\beta}_{j})} \\boldsymbol{x}_{i} + \\beta_{n}\\boldsymbol{x}_{n} = K\\boldsymbol{z} + \\beta_{n}\\boldsymbol{x}_{n}.$$\n",
        "\n",
        "We know that $ K + \\beta_{n} = \\sum_{j = 1}^{N-1} {\\beta}_{j} + \\beta_{n} = \\sum_{j = 1}^{N} {\\beta}_{j} = 1$, having that $K \\geq 0$ , $ \\beta_{n} \\geq 0$ and $ z, \\boldsymbol{x}_{n} \\in \\mathcal{C}$ we can conclude $ K\\boldsymbol{z} + \\beta_{n}\\boldsymbol{x}_{n} \\in \\mathcal{C}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs2F2Pe2zJDu"
      },
      "source": [
        "Let $\\mathcal{C}\\subset\\mathbb{R}^{2}$ be a convex set.  \n",
        "Consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\}_{i=1}^{10}$ such that $\\boldsymbol{x}_{i} \\neq \\boldsymbol{x}_{j}$ for all $i \\neq j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoikbfphzJDu"
      },
      "source": [
        "### 1.3. Question\n",
        "\n",
        "Prove or disprove the following assertion:\n",
        "\n",
        "Necessarily, any point $\\boldsymbol{y} \\in \\mathcal{C}$ can be represented as a convex combination of $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7EgNiukzJDv"
      },
      "source": [
        "### 1.3. Solution\n",
        "\n",
        "**Disprove by counter example**\n",
        "\n",
        "Let $\\mathcal{C}\\subset\\mathbb{R}^{2}$ be the convex square defined by the vectors ${(0,0), (0,1), (1,0), (1,1)}$\\\n",
        "It is easy to show this square is a convex set since:\n",
        "- For all vectors $(x_1,x_2) \\in \\mathcal{C} \\quad  0 \\leq x_1, x_2 \\leq 1$\n",
        "- For two vector addition $(x_1, x_2), (y_1, y_2)$ and $\\alpha \\in [0,1]$:\n",
        " - $0 \\leq \\alpha x_{1} + (1-\\alpha)y_{1} \\leq \\alpha + (1-\\alpha) = 1$\n",
        " - $0 \\leq \\alpha x_{2} + (1-\\alpha)y_{2} \\leq \\alpha + (1-\\alpha) = 1$\n",
        "\n",
        "Consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\}_{i=1}^{10} = \\left\\{(0, 0.1*i) \\right\\}_{i=1}^{10} = \\left\\{(0,0.1), (0,0.2) \\dots (0,1) \\right\\}$\\\n",
        "complying to $\\boldsymbol{x}_{i} \\neq \\boldsymbol{x}_{j}$ for all $i \\neq j$\n",
        "\n",
        "The point $(1,0) \\in \\mathcal{C}$ obviously cannot be represented as a convex combination of $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\}_{i=1}^{10}$ since the first coordinate in the convex combination of the chosen $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\}_{i=1}^{10}$ can only be 0."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4, 3))\n",
        "\n",
        "polygon = patches.Polygon([(0, 0), (0, 1), (1, 1), (1, 0)], closed=True, edgecolor='b', facecolor='none')\n",
        "ax.add_patch(polygon)\n",
        "\n",
        "for i in range(10):\n",
        "    ax.add_patch(plt.Circle((0.1 * (i + 1), 0), radius=0.05, color='g'))\n",
        "\n",
        "ax.add_patch(plt.Circle((0, 1), radius=0.05, color='r'))\n",
        "\n",
        "ax.set_xlim(-0.1, 1.1)\n",
        "ax.set_ylim(-0.1, 1.1)\n",
        "ax.set_aspect('equal', 'box')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "Ln0hGDoa0rcS",
        "outputId": "c1cbaa6a-86a6-4e02-829f-db2f4bb6cced"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_9OrJ6UzJDv"
      },
      "source": [
        "## 2. The Gradient\n",
        "\n",
        "**Remark**: Assume all functions in this section are differentiable.\n",
        "\n",
        "\n",
        "**Directional Derivative**\n",
        "\n",
        "Let $f : \\mathbb{R}^{d} \\to \\mathbb{R}$ and let $\\boldsymbol{x}_{0} \\in \\mathbb{R}^{d}$.\n",
        "\n",
        "### 2.1. Question\n",
        "\n",
        "Prove that:\n",
        "\n",
        "$$ \\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\nabla f \\left( \\boldsymbol{x}_{0} \\right) \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{g}_{0}, \\boldsymbol{h} \\right\\rangle \\implies \\boldsymbol{g}_{0} = \\nabla f \\left( \\boldsymbol{x}_{0} \\right) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmMV8RWLzJDv"
      },
      "source": [
        "### 2.1. Solution\n",
        "\n",
        "$\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] \\equiv \\lim_{t→0} \\frac{f \\left( \\boldsymbol{x} + t\\boldsymbol{h} \\right) - f \\left( \\boldsymbol{x} \\right)}{t}$\n",
        "\n",
        "By Taylor expansion around $\\boldsymbol{x}$ for small $t$:\\\n",
        "$ f \\left( \\boldsymbol{x} + t\\boldsymbol{h} \\right) = f \\left( \\boldsymbol{x} \\right) + (\\nabla f \\left( \\boldsymbol{x} \\right))^{\\top} t\\boldsymbol{h} + O(t^2)$\\\n",
        "Therefore:\\\n",
        "$\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] \\equiv \\lim_{t→0} \\frac{f \\left( \\boldsymbol{x} + t\\boldsymbol{h} \\right) - f \\left( \\boldsymbol{x} \\right)}{t} = \\lim_{t→0} \\frac{f \\left( \\boldsymbol{x} \\right) + (\\nabla f \\left( \\boldsymbol{x} \\right))^{\\top} t\\boldsymbol{h} + O(t^2) - f \\left( \\boldsymbol{x} \\right)}{t} = \\lim_{t→0} \\frac{t(\\nabla f \\left( \\boldsymbol{x} \\right))^{\\top} \\boldsymbol{h} + O(t^2)}{t} = (\\nabla f \\left( \\boldsymbol{x} \\right))^{\\top} \\boldsymbol{h} = \\left\\langle \\nabla f \\left( \\boldsymbol{x} \\right), \\boldsymbol{h} \\right\\rangle$\n",
        "$\\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\nabla f \\left( \\boldsymbol{x}_{0} \\right) \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{g}_{0}, \\boldsymbol{h} \\right\\rangle \\implies \\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\left\\langle \\nabla f \\left( \\boldsymbol{x} \\right), \\boldsymbol{h} \\right\\rangle = \\left\\langle \\boldsymbol{g}_{0}, \\boldsymbol{h} \\right\\rangle$\n",
        "$\\implies \\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\left\\langle \\nabla f \\left( \\boldsymbol{x} \\right) - \\boldsymbol{g}_{0}, \\boldsymbol{h} \\right\\rangle = 0$\\\n",
        "$\\implies \\nabla f \\left( \\boldsymbol{x} \\right) - \\boldsymbol{g}_{0} = \\boldsymbol{0} \\quad$(the 0 vector\\operator)\n",
        "\n",
        "The last step is trivial. An easy proof is to calculate $\\left\\langle \\nabla f \\left( \\boldsymbol{x} \\right) - \\boldsymbol{g}_{0}, \\boldsymbol{h} \\right\\rangle$ with $d$ different vectors $\\{\\boldsymbol{h}_1, \\boldsymbol{h}_2, ... \\boldsymbol{h}_d\\}$, where $\\boldsymbol{h}_i$ is the one-hot vector with all $0$ but $1$ in the $i^{th}$ position. Since all dot products are $0$ we will find that all components $(\\nabla f \\left( \\boldsymbol{x} \\right) - \\boldsymbol{g}_{0})_i$ are 0.\n",
        "\n",
        "$\\implies \\boldsymbol{g}_{0} = \\nabla f \\left( \\boldsymbol{x} \\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaC0SgTPzJDv"
      },
      "source": [
        "**Definition**\n",
        "\n",
        "$f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ is said to be **linear** if:\n",
        "\n",
        "$$ f \\left( \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y} \\right) = \\alpha f \\left( \\boldsymbol{x} \\right) + \\beta f \\left( \\boldsymbol{y} \\right) $$\n",
        "\n",
        "For all $\\alpha, \\beta \\in \\mathbb{R}$ and for all $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{{d}_{1}}$.\n",
        "\n",
        "\n",
        "\n",
        "Let $f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ be a linear function.\n",
        "\n",
        "### 2.2. Question\n",
        "\n",
        "Prove that:\n",
        "\n",
        "$$ \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = f \\left( \\boldsymbol{h} \\right) $$\n",
        "\n",
        "For all $\\boldsymbol{x}, \\boldsymbol{h} \\in \\mathbb{R}^{{d}_{1}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqM9Xzq5zJDv"
      },
      "source": [
        "### 2.2. Solution\n",
        "\n",
        "From linearity: $f \\left( \\boldsymbol{x} + t\\boldsymbol{h} \\right) = f \\left( \\boldsymbol{x} \\right)+ t f\\left(\\boldsymbol{h} \\right)$\\\n",
        "For all $\\boldsymbol{x}, \\boldsymbol{h} \\in \\mathbb{R}^{{d}_{1}}$, $t \\in \\mathbb{R}$\\\n",
        "$⇒ \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] \\equiv \\lim_{t→0} \\frac{f \\left( \\boldsymbol{x} + t\\boldsymbol{h} \\right) - f \\left( \\boldsymbol{x} \\right)}{t} = \\lim_{t→0} \\frac{f \\left( \\boldsymbol{x} \\right)+ t f\\left(\\boldsymbol{h} \\right) - f \\left( \\boldsymbol{x} \\right)}{t} = \\lim_{t→0} f\\left(\\boldsymbol{h} \\right) = f\\left(\\boldsymbol{h} \\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWHWAcYxzJDv"
      },
      "source": [
        "### 2.3. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUJ_AHPKzJDv"
      },
      "source": [
        "### 2.3. Solution\n",
        "\n",
        "We can write $ f $ as: $ f(x) = \\left\\langle \\boldsymbol{x}, \\boldsymbol{A}\\boldsymbol{x} \\right\\rangle $ . Using the product rule we get:\n",
        "$$  \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] =  \\nabla \\left\\langle \\boldsymbol{x}, \\boldsymbol{A}\\boldsymbol{x} \\right\\rangle \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{h}, \\boldsymbol{A}\\boldsymbol{x} \\right\\rangle + \\left\\langle \\boldsymbol{x}, \\boldsymbol{A}\\boldsymbol{h} \\right\\rangle = \\left\\langle \\boldsymbol{h}, \\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{A}^T\\boldsymbol{x}\\right\\rangle $$\n",
        "\n",
        "and therefore:\n",
        "$$  \\nabla f \\left( \\boldsymbol{x} \\right) = \\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{A}^T\\boldsymbol{x} $$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCoiWcMozJDw"
      },
      "source": [
        "### 2.4. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( X \\right) \\left[ \\boldsymbol{H} \\right]$ and the gradient $\\nabla f \\left( X \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{A} \\boldsymbol{X} \\right\\} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax8_2t4NzJDw"
      },
      "source": [
        "### 2.4. Solution\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = {\\left\\| \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x} \\right\\|}_{2}^{2} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7w5Tqj0zJDw"
      },
      "source": [
        "### 2.5. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = {\\left\\| \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x} \\right\\|}_{2}^{2} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-Pzp9NvzJDw"
      },
      "source": [
        "### 2.5. Solution\n",
        "\n",
        "$ f \\left( \\boldsymbol{x} \\right) = {\\left\\| \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x} \\right\\|}_{2}^{2} = \\left\\langle \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}, \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}\\right\\rangle$\n",
        "\n",
        "$\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]= \\nabla\\left\\langle \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}, \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}\\right\\rangle \\left[ \\boldsymbol{h} \\right]$\\\n",
        "(by product rule) = $ \\left\\langle \\nabla(\\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x})\\left[ \\boldsymbol{h} \\right], \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}\\right\\rangle + \\left\\langle \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}, \\nabla(\\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x})\\left[ \\boldsymbol{h} \\right]\\right\\rangle$\\\n",
        "(by linearity 2.2) = $ \\left\\langle -\\boldsymbol{A}\\boldsymbol{h}, \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}\\right\\rangle + \\left\\langle \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}, -\\boldsymbol{A}\\boldsymbol{h}\\right\\rangle=\n",
        "-2\\left\\langle \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}, \\boldsymbol{A}\\boldsymbol{h}\\right\\rangle = -2\\left\\langle \\boldsymbol{A}^\\top(\\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}), \\boldsymbol{h}\\right\\rangle$\n",
        "\n",
        "$\\implies  \\nabla f \\left( \\boldsymbol{x} \\right) = -2\\boldsymbol{A}^\\top(\\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLk9oODlzJDw"
      },
      "source": [
        "### 2.6. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( X \\right) \\left[ H \\right]$ and the gradient $\\nabla f \\left( X \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X} \\right\\|}_{F}^{2} $$\n",
        "\n",
        "Where:\n",
        "\n",
        " - $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{D \\times d}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$.\n",
        " - ${\\left\\| \\cdot \\right\\|}_{F}^{2}$ is the squared [Frobenius Norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm), that is, ${\\left\\| \\boldsymbol{X} \\right\\|}_{F}^{2} = \\left\\langle \\boldsymbol{X}, \\boldsymbol{X} \\right\\rangle = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{X} \\right\\}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J4MU1X_zJDw"
      },
      "source": [
        "### 2.6. Solution\n",
        "\n",
        "$ f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X} \\right\\|}_{F}^{2} = \\left\\langle \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X}, \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X}\\right\\rangle$\n",
        "\n",
        "$\\nabla f \\left( \\boldsymbol{X} \\right) \\left[ \\boldsymbol{H} \\right]= \\nabla\\left\\langle \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X}, \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X}\\right\\rangle \\left[ \\boldsymbol{H} \\right]$\\\n",
        "(by product rule) = $ \\left\\langle \\nabla(\\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X})\\left[ \\boldsymbol{H} \\right], \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X}\\right\\rangle + \\left\\langle \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X}, \\nabla(\\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X})\\left[ \\boldsymbol{H} \\right]\\right\\rangle$\\\n",
        "(by linearity 2.2) = $ \\left\\langle -\\boldsymbol{A}\\boldsymbol{H}, \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X}\\right\\rangle + \\left\\langle \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X}, -\\boldsymbol{A}\\boldsymbol{H}\\right\\rangle=\n",
        "-2\\left\\langle \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X}, \\boldsymbol{A}\\boldsymbol{H}\\right\\rangle = -2\\operatorname{Tr} \\left\\{(\\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X})^\\top\\boldsymbol{A}\\boldsymbol{H}\\right\\} = -2\\left\\langle \\boldsymbol{A}^\\top(\\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X}), \\boldsymbol{H}\\right\\rangle$\n",
        "\n",
        "$\\implies  \\nabla f \\left( \\boldsymbol{X} \\right) = -2\\boldsymbol{A}^\\top(\\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEZ1mUvxzJDw"
      },
      "source": [
        "### 2.7. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( X \\right) \\left[ H \\right]$ and the gradient $\\nabla f \\left( X \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{X}^{T} \\boldsymbol{A}, \\boldsymbol{Y}^{T} \\right\\rangle $$\n",
        "\n",
        "Where $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times D}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biOIcpRfzJDw"
      },
      "source": [
        "### 2.7. Solution\n",
        "\n",
        "First, let's present $ f $ in a different way. We consider here the inner product to be the standart matrix inner product: $ \\left\\langle \\boldsymbol{A}, \\boldsymbol{B} \\right\\rangle =  \\operatorname{Tr} \\left\\{ \\boldsymbol{A}^{T} \\boldsymbol{B} \\right\\} $ .\n",
        "We have:\n",
        "$$  f \\left( \\boldsymbol{X} \\right) =_1 \\left\\langle \\boldsymbol{X}^{T} \\boldsymbol{A}, \\boldsymbol{Y}^{T} \\right\\rangle =_2 \\left\\langle \\boldsymbol{Y}^{T} , \\boldsymbol{X}^{T} \\boldsymbol{A} \\right\\rangle =_3 \\operatorname{Tr} \\left\\{ \\boldsymbol{Y} \\boldsymbol{X}^{T} \\boldsymbol{A} \\right\\} =_4  \\operatorname{Tr} \\left\\{ \\boldsymbol{Y}^{T} \\boldsymbol{A}^{T} \\boldsymbol{X} \\right\\} =_5  \\left\\langle \\boldsymbol{A} \\boldsymbol{Y}, \\boldsymbol{X} \\right\\rangle $$\n",
        "\n",
        "where equation 1 is given, equation 2 is true because we are over the field $ \\mathbb{R} $ , equations 3 and 5 are from the inner product definition and equation 4 is true as we saw in class:\n",
        "$$ \\operatorname{Tr} \\left\\{ \\boldsymbol{C}^T \\boldsymbol{D} \\right\\} =\\operatorname{Tr} \\left\\{ \\boldsymbol{C} \\boldsymbol{D}^T \\right\\} $$ for matrices $ \\boldsymbol{C} $ and $ \\boldsymbol{D} $ of the same dimensions, where here: $ \\boldsymbol{C} = \\boldsymbol{Y}^T $ and $ \\boldsymbol{D} = \\boldsymbol{X}^T \\boldsymbol{A} $ .\n",
        "\n",
        "Before we finish we note that the derivative and every directional derivative of a constant function is zero. We use the product rule and we get:\n",
        "\n",
        "$$ \\nabla f \\left( \\boldsymbol{X} \\right) \\left[ \\boldsymbol{H} \\right] = \\nabla \\left\\langle \\boldsymbol{A} \\boldsymbol{Y}, \\boldsymbol{X} \\right\\rangle \\left[ \\boldsymbol{H} \\right] = \\left\\langle 0, \\boldsymbol{X} \\right\\rangle + \\left\\langle \\boldsymbol{A} \\boldsymbol{Y}, \\boldsymbol{H} \\right\\rangle = \\left\\langle \\boldsymbol{A} \\boldsymbol{Y}, \\boldsymbol{H} \\right\\rangle $$\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$$ \\nabla f \\left( \\boldsymbol{X} \\right) =  \\boldsymbol{A} \\boldsymbol{Y} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8deNfTzfzJDx"
      },
      "source": [
        "### 2.8. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left( \\boldsymbol{x} \\right) $$\n",
        "\n",
        "Where $g \\left( \\cdot \\right)$ is an element wise function $g \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\in \\mathbb{R}^{d}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNpp-pALzJDx"
      },
      "source": [
        "### 2.8. Solution\n",
        "\n",
        "$f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left( \\boldsymbol{x} \\right) = \\sum_{i=1}^d a_ig(x_i)$\\\n",
        "$\\implies \\frac{\\partial f}{\\partial x_i} = a_i\\acute{g}(x_i)$\\\n",
        "$\\implies \\nabla f \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix}a_1 \\acute g \\left( {x}_{1} \\right) \\\\a_2 \\acute g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\a_d \\acute g \\left( {x}_{d} \\right) \\end{bmatrix} = \\boldsymbol{a} \\circ\\nabla g\\left( \\boldsymbol{x} \\right)\\quad$, where $\\circ$ denotes the Hadamard product.\\\n",
        "$\\implies \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{a} \\circ\\nabla g\\left( \\boldsymbol{x} \\right), \\boldsymbol{h}\\right\\rangle$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVT-XyExzJDx"
      },
      "source": [
        "### 2.10. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( X \\right) \\left[ H \\right]$ and the gradient $\\nabla f \\left( X \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
        "\n",
        "Where:\n",
        "\n",
        " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
        " - The function $\\operatorname{Diag} \\left( \\cdot \\right) : \\mathbb{R}^{d \\times d} \\to \\mathbb{R}^{d} $ returns the diagonal of a matrix, that is, $\\boldsymbol{b} = \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{b} \\left[ i \\right] = \\left( \\boldsymbol{X} \\left[ i, i\\right] \\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dMrmL7JzJDx"
      },
      "source": [
        "### 2.10. Solution\n",
        "\n",
        "$f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\right\\rangle = \\sum_i \\boldsymbol{a}_i \\boldsymbol{X}_{ii}$\\\n",
        "$\\implies \\frac{\\partial f}{\\partial \\boldsymbol{X}_{ij}} = \\begin{cases} a_i \\quad \\quad\\text{if} \\quad i=j \\\\ 0 \\quad \\quad\\text{ if} \\quad i\\neq j \\end{cases} $\n",
        "\n",
        "$\\nabla f \\left( \\boldsymbol{X} \\right) =\n",
        "\\begin{bmatrix}\n",
        "a_{1} & 0 & 0 & \\dots & 0 \\\\\n",
        "0 & a_{2} & 0 & \\dots & 0 \\\\\n",
        "\\vdots &  & \\ddots &  &  \\\\\n",
        "0 &  &   &   & a_{d}\n",
        "\\end{bmatrix}$\\\n",
        "  \\\n",
        "\n",
        "$\\nabla f \\left( \\boldsymbol{X} \\right) \\left[ \\boldsymbol{H} \\right] = \\left\\langle \\nabla f \\left( \\boldsymbol{X} \\right) , \\boldsymbol{H}\\right\\rangle = \\sum_{i=1}^d a_i H_{ii} = \\left\\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( \\boldsymbol{H} \\right) \\right\\rangle$\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGTQxSzUzJDx"
      },
      "source": [
        "### 2.11. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( X \\right) \\left[ H \\right]$ and the gradient $\\nabla f \\left( X \\right)$ of:\n",
        "\n",
        "$$ f \\left( X \\right) = \\left \\langle A, \\sin \\left[ X \\right] \\right \\rangle $$\n",
        "\n",
        "Where:\n",
        "\n",
        " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
        " - The function $\\sin \\left[ \\cdot \\right]$ is the element wise $\\log$ function: $\\boldsymbol{M} = \\sin \\left[ \\boldsymbol{X} \\right] \\implies \\boldsymbol{M} \\left[ i, j \\right] = \\sin \\left( \\boldsymbol{X} \\left[ i, j\\right] \\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkIDrkFEzJDx"
      },
      "source": [
        "### 2.11. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU5vCuvXzJDx"
      },
      "source": [
        "## 3. Constraint Optimization\n",
        "\n",
        "**MinMax**  \n",
        "\n",
        "Let $G \\left( x, y \\right) = \\sin \\left( x + y \\right)$.\n",
        "\n",
        "### 3.1. Question\n",
        "\n",
        "Show that:\n",
        "\n",
        " - $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$.\n",
        " - $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSWUo4nazJDx"
      },
      "source": [
        "### 3.1. Solution\n",
        "\n",
        "Maximizing $G \\left( x, y \\right)$ on $y$ first:\\\n",
        "$\\frac{\\partial G}{\\partial y} = \\cos \\left( x + y \\right) = 0$\\\n",
        "$\\implies y = \\frac{\\pi}{2} - x$ (and $y = -\\frac{\\pi}{2} - x$ as a minimum)\\\n",
        "$\\implies \\underset{y}{\\max}G \\left( x, y \\right) = \\sin \\left(\\frac{\\pi}{2} \\right) = 1$\\\n",
        "In other words, for every $x$ we can find a $y$ to bring the function to its maximal value $1$.\\\n",
        "Since after maximization $G \\left( x, y \\right)$ is a constant, it is independent of $x$ and minimzimg over $x$ will keep the result as $1$:\n",
        "- $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$\n",
        "\n",
        "\n",
        "Similarly, minimizing $G \\left( x, y \\right)$ on $x$ first:\\\n",
        "$\\frac{\\partial G}{\\partial x} = \\cos \\left( x + y \\right) = 0$\\\n",
        "$\\implies x = -\\frac{\\pi}{2} - y$ (and $x = \\frac{\\pi}{2} - y$ as a maximum)\\\n",
        "$\\implies \\underset{x}{\\min}G \\left( x, y \\right) = \\sin \\left(-\\frac{\\pi}{2} \\right) = -1$\\\n",
        "In other words, for every $y$ we can find an $x$ to bring the function to its minimal value $-1$.\\\n",
        "Since after minimization $G \\left( x, y \\right)$ is a constant, it is independent of $y$ and maximzimg over $y$ will keep the result as $-1$:\n",
        "- $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGW5qua6zJDy"
      },
      "source": [
        "**Rayleigh Quotient**  \n",
        "\n",
        "The _Rayleigh Quotient_ is defined by:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = \\frac{ \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} $$\n",
        "\n",
        "For some symmetric matrix $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$.\n",
        "\n",
        "### 3.2. Question\n",
        "\n",
        "For a symmetric matrix $ A $ which has non negative eigen values, Follow the given steps:\n",
        "\n",
        " - Show that $ {\\min}_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) = \\begin{cases} {\\min}_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} \\\\ \\text{ s.t. } {\\left\\| \\boldsymbol{x} \\right\\|}_{2}^{2} = 1 \\end{cases} $.\n",
        " - Write the Lagrangian of the constraint objective $\\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right)$.\n",
        " - Show that ${\\nabla}_{\\boldsymbol{x}} \\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right) = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\lambda \\boldsymbol{x}$.  \n",
        "   In other words, the stationary points $\\left( \\boldsymbol{x}, \\lambda \\right)$ are the eigenvectors and eigenvalues of $\\boldsymbol{A}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqRWEyz2zJD2"
      },
      "source": [
        "### 3.2. Solution\n",
        "\n",
        "* In order to find  $ {\\min}_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) $ we will find $\\boldsymbol{x} $ which satisfies $\\nabla f \\left( \\boldsymbol{x} \\right) = 0$:\n",
        "$$\\nabla f \\left( \\boldsymbol{x} \\right) = ∇_x  \\frac{ \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} $$\n",
        "We can apply the rules of fraction derivatives and get:\n",
        "$$\\nabla_x\\frac{\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} = \\frac{ \\nabla(\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x})\\boldsymbol{x}^{T} \\boldsymbol{x} - \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} \\nabla(\\boldsymbol{x}^{T} \\boldsymbol{x})}{( \\boldsymbol{x}^{T} \\boldsymbol{x})^2}$$\n",
        "We proved that $\\nabla(\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x})=\\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{A}^T\\boldsymbol{x} = 2\\boldsymbol{A}\\boldsymbol{x}$ since $\\boldsymbol{A}$ is symmetric, and $\\nabla(\\boldsymbol{x}^{T} \\boldsymbol{x}) = 2\\boldsymbol{x}$.\n",
        " Therefore:\n",
        "\n",
        "$$\\nabla f \\left( \\boldsymbol{x} \\right) = \\frac{ 2(\\boldsymbol{A}\\boldsymbol{x}) \\boldsymbol{x}^{T} \\boldsymbol{x} - 2(\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}) \\boldsymbol{x}}{( \\boldsymbol{x}^{T} \\boldsymbol{x})^2}. $$\n",
        "We are looking for $\\boldsymbol{x}$ which satisfies $\\nabla f \\left( \\boldsymbol{x} \\right) = 0$. Therefore:\n",
        "$$\\nabla f \\left( \\boldsymbol{x} \\right) = 0 \\iff \\frac{ 2(\\boldsymbol{A}\\boldsymbol{x}) \\boldsymbol{x}^{T} \\boldsymbol{x} - 2(\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}) \\boldsymbol{x}}{( \\boldsymbol{x}^{T} \\boldsymbol{x})^2} = 0 \\iff 2(\\boldsymbol{A}\\boldsymbol{x}) \\boldsymbol{x}^{T} \\boldsymbol{x} - 2(\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}) \\boldsymbol{x} = 0 \\iff (\\boldsymbol{A}\\boldsymbol{x}) \\boldsymbol{x}^{T} \\boldsymbol{x} - (\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}) \\boldsymbol{x} = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\frac {\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}}{\\boldsymbol{x}^{T} \\boldsymbol{x}}\\boldsymbol{x} = f(\\boldsymbol{x})\\boldsymbol{x}$$\n",
        "We can infer that every eigenvector $\\boldsymbol{x}$ of the matrix $\\boldsymbol{A}$ with the eigenvalue $f(\\boldsymbol{x})$ satisfies the equation.\n",
        "Namely, the vectors $\\boldsymbol{x}$ that minimize $f \\left( \\boldsymbol{x} \\right)$ are those that satisfy this system of linear equations,i.e, the eigenvectors of $\\boldsymbol{A}$. Now, we have to choose among all the eigenvectors. In addition, each eigenvalue that corresponds to the eigenvector $\\boldsymbol{x}$ is $f(\\boldsymbol{x})$.\n",
        "Therefore, we just have to choose the eigenvector that corresponds to the minimal eigenvalue. In this case the eigenvector can be normalized, since the eigenvalues could be normalized as well. having ${\\left\\| \\boldsymbol{x} \\right\\|}_{2}^{2} = 1$ we conclude:\n",
        "${\\min}_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right)= {\\min}_{\\boldsymbol{x}}\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}$.\n",
        "\n",
        "\n",
        "* The Lagrangian:\n",
        "$L(\\lambda, \\boldsymbol{x}) = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} - \\lambda(\\boldsymbol{x}^{T}\\boldsymbol{x} - 1)$ where $\\lambda$ is the lagrangian multiplication factor, $\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}$ is the objective function and $\\boldsymbol{x}^{T}\\boldsymbol{x} - 1$ is the constraint.\n",
        "\n",
        "* $\\nabla_xL(\\lambda, \\boldsymbol{x}) = \\nabla_x\\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} - \\lambda(\\boldsymbol{x}^{T}\\boldsymbol{x} - 1) = 2\\boldsymbol{A} \\boldsymbol{x} - 2\\lambda \\boldsymbol{x}$. Therefore:\n",
        "$$\\nabla_xL(\\lambda, \\boldsymbol{x}) = 0 \\iff 2\\boldsymbol{A} \\boldsymbol{x} - 2\\lambda \\boldsymbol{x} = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\lambda \\boldsymbol{x} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws8fbiCvzJD2"
      },
      "source": [
        "<img src=\"https://i.imgur.com/qIP5xPv.png\" height=\"700\">"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
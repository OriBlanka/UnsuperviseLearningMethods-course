{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyx-YsEXW3ds"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# UnSupervised Learning Methods\n",
        "\n",
        "## Exercise 003 - Part I\n",
        "\n",
        "> Notebook by:\n",
        "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "| 1.0.000 | 27/08/2023 | Royi Avital | First version                                                      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81NfpSRaW3dv"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_08/Exercise0002Part001.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ2KTBK8W3dv"
      },
      "source": [
        "## Notations\n",
        "\n",
        "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
        "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
        "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
        "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb2bTOzvW3dw"
      },
      "source": [
        "## Guidelines\n",
        "\n",
        " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
        " - Answer all questions / tasks within the Jupyter Notebook.\n",
        " - Use MarkDown + MathJaX + Code to answer.\n",
        " - Verify the rendering on VS Code.\n",
        " - Submission in groups (Single submission per group).\n",
        " - You may and _should_ use the forums for questions.\n",
        " - Good Luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-2TC1shW3dw"
      },
      "source": [
        "## Team Members\n",
        "\n",
        " - `Ori_Blanka_208994764`.\n",
        " - `Or_Benson_308577345`.\n",
        " - `Alon_Hertz_315682773`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4SFBTzcW3dx"
      },
      "source": [
        "## 1. Principle Component Analysis (PCA)\n",
        "\n",
        "Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a diagonalizable matrix, that is $\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}$ where $\\boldsymbol{\\Lambda}$ is a diagonal matrix.\n",
        "\n",
        "### 1.1. Question\n",
        "\n",
        "Prove the following:\n",
        "\n",
        "$$ \\operatorname{Tr} \\left( A \\right) = \\sum_{i = 1}^{d} {\\lambda}_{i} \\left( A \\right) $$\n",
        "\n",
        "Where $\\operatorname{Tr} \\left( A \\right) = \\sum_{i = 1}^{d} {A}_{ii}$ and ${\\lambda}_{i} \\left( \\boldsymbol{A} \\right) = {\\Lambda}_{ii}$ is the $i$ -th eigen value of $\\boldsymbol{A}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRXsFmYaW3dx"
      },
      "source": [
        "### 1.1. Solution\n",
        "\n",
        "$\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}$ where $\\boldsymbol{\\Lambda}$ is a diagonal matrix.\n",
        "\n",
        "$$ \\operatorname{Tr} \\left( A \\right) = \\operatorname{Tr}\\left(\\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}\\right) = \\operatorname{Tr}\\left(\\boldsymbol{Q}^{-1}\\boldsymbol{Q} \\boldsymbol{\\Lambda} \\right)= \\operatorname{Tr}\\left(\\boldsymbol{I} \\boldsymbol{\\Lambda} \\right)= \\operatorname{Tr}\\left(\\boldsymbol{\\Lambda} \\right)=\\sum_{i = 1}^{d} {\\lambda}_{i} \\left( A \\right) $$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZiQkolTW3dy"
      },
      "source": [
        "A symmetric matrix $\\boldsymbol{A} = \\boldsymbol{A}^{T}$ is called a _Symmetric Positive Definite_ (SPD) matrix if either:\n",
        "\n",
        "* All eigen values are positive: $\\forall i: \\; {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) > 0$.\n",
        "* Any quadratic form is positive: $\\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$.\n",
        "\n",
        "Namely $\\boldsymbol{A} \\succ 0$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> There is also a _Symmetric Semi Positive Definite Matrix_ (SPSD) which obeys the above with weak inequality.\n",
        "\n",
        "### 1.3. Question\n",
        "\n",
        " 1. Prove the equivalency of the 2 properties, namely:\n",
        "\n",
        "$$ \\forall i: \\; {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) > 0 \\iff  \\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$$\n",
        "\n",
        " 2. Prove or disprove: $\\boldsymbol{A}^{-1}$ is a _Symmetric Positive Definite_ matrix.\n",
        " 3. Prove or disprove: The SVD of $\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T}$ is also an eigen decomposition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNH027DJW3dy"
      },
      "source": [
        "### 1.3. Solution\n",
        "\n",
        "1.\\\n",
        "if $\\boldsymbol{A}$ is symmetric, it is diagonalizable and there exists an orthonormal basis of eigenvectors  $\\{\\boldsymbol{u}_1, \\boldsymbol{u}_2.... \\boldsymbol{u}_d\\}$  with corresponding real eigenvalues $\\{\\lambda_1, \\lambda_2.... \\lambda_d\\}$, such that $\\boldsymbol{A}\\boldsymbol{u}_i = \\lambda_i\\boldsymbol{u}_i$.\\\n",
        "$\\forall \\boldsymbol{v} \\in \\mathbb{R}^{d}$ we have a decomposition:\\\n",
        "$\\boldsymbol{v} =v_1\\boldsymbol{u}_1 + v_2\\boldsymbol{u}_2 +... + v_d\\boldsymbol{u}_d = \\sum_{i=1}^d v_i\\boldsymbol{u}_i$\\\n",
        "Therefore, $\\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} = (\\sum_{i=1}^d v_i\\boldsymbol{u}_i) \\boldsymbol{A} (\\sum_{i=1}^d v_i\\boldsymbol{u}_i) = (\\sum_{i=1}^d v_i\\boldsymbol{u}_i)(\\sum_{i=1}^d v_i \\lambda_i\\boldsymbol{u}_i) = \\sum_{i=1}^d \\lambda_i v_i^2$\n",
        "\n",
        "Suppose $\\forall i \\quad \\lambda_i(\\boldsymbol{A})>0$\\\n",
        "This implies that $\\forall \\boldsymbol{v}\\neq 0 \\in \\mathbb{R}^{d} \\quad\\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} = \\sum_{i=1}^d \\lambda_i v_i^2 > 0$\n",
        "\n",
        "Conversly, assuming that $\\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$\\\n",
        "When we substitute for each of the eigenvectors $\\boldsymbol{u}_i$ to get:\\\n",
        "$\\boldsymbol{u}_i^{T} \\boldsymbol{A} \\boldsymbol{u}_i = \\boldsymbol{u}_i^{T} \\lambda_i \\boldsymbol{u}_i = \\lambda_i \\implies \\lambda_i>0 \\quad \\forall i$\n",
        "\n",
        "\n",
        "\\\n",
        "2.\\\n",
        "If $\\boldsymbol{A}$ is SPD, then $\\boldsymbol{A} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^{-1}$ with a diagonal $\\boldsymbol{\\Lambda}$.\\\n",
        "Since $\\forall i \\quad \\lambda_i >0 \\quad \\Lambda^{-1}$ exists and is defined by $(\\Lambda^{-1})_{ii} = (\\Lambda_{ii})^{-1} = \\lambda_i^{-1}$\\\n",
        "If we deine $\\boldsymbol{B} =  \\boldsymbol{Q}\\boldsymbol{\\Lambda}^{-1}\\boldsymbol{Q}^{-1}$ we find that:\\\n",
        "$\\boldsymbol{A}\\boldsymbol{B} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^{-1}\\boldsymbol{Q}\\boldsymbol{\\Lambda}^{-1}\\boldsymbol{Q}^{-1} = \\boldsymbol{I}$\\\n",
        "$\\boldsymbol{B}\\boldsymbol{A} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}^{-1}\\boldsymbol{Q}^{-1}\\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^{-1} = \\boldsymbol{I}$\\\n",
        "$\\implies  \\boldsymbol{B} = \\boldsymbol{A}^{-1}$\n",
        "\n",
        "$\\boldsymbol{A}^{-1}$ is symmetric, since the inverse of a symmetric matrix is a symmetric matrix.\n",
        "\n",
        "In summary, $\\boldsymbol{A}^{-1}$ exists, is symmetric, and is diagonalizable with all positive eigenvalues $\\lambda_i^{-1} > 0$\\\n",
        "$\\implies \\boldsymbol{A}^{-1}$ is SPD\n",
        "\n",
        "\n",
        "3.\\\n",
        "If $\\boldsymbol{A}$ is SPD, then it has an eigen decomposition $\\boldsymbol{A} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^{-1}$ with all positive eigenvalues.\n",
        "\n",
        "Consider the SVD of $\\boldsymbol{A}$:\\\n",
        "$\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T}$\\\n",
        "Vectors in $\\boldsymbol{U}$ are the eigenvectors of $\\boldsymbol{A}\\boldsymbol{A}^T$\\\n",
        "Vectors in $\\boldsymbol{V}$ are the eigenvectors of $\\boldsymbol{A}^T\\boldsymbol{A}$\\\n",
        "For SPD: $\\boldsymbol{A}^T\\boldsymbol{A} = \\boldsymbol{A}\\boldsymbol{A}^T = \\boldsymbol{A}^2 = \\boldsymbol{Q}\\boldsymbol{\\Lambda}^2\\boldsymbol{Q}^{-1}$\\\n",
        "Suppose that all eigenvalues are unique. We know there are no $0$ eigenvalues. Then  $\\boldsymbol{U}$ and $\\boldsymbol{V}$ are either equal to vectors in  $\\boldsymbol{Q}$ or different by a sign.\\\n",
        "The singular values in $\\Sigma$ are the absolute values of the eigenvalues. Since  $\\boldsymbol{A}$ is SPD and all eigenvalues are positive, they are equal:\n",
        "$\\Sigma = \\Lambda$\\\n",
        "Therefore, we must choose the same sign for $\\boldsymbol{U}$ and $\\boldsymbol{V}$ and in fact $\\boldsymbol{U}= \\boldsymbol{V}$\\\n",
        "Both $\\boldsymbol{U}$ and $\\boldsymbol{V}$ are unitary and so  $\\boldsymbol{U}^{-1} = \\boldsymbol{U}^T = \\boldsymbol{V}^{-1} = \\boldsymbol{V}^T$\n",
        "\n",
        "To conclude, if eigenvalues are unique:\\\n",
        "$\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T} = \\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{-1}$\\\n",
        "And so the SVD of a SPD matrix is also an eigen decomposition.\n",
        "\n",
        "If eigenvalues are not unique, we can still choose the same orthogonal basis for $\\boldsymbol{U}$ and $\\boldsymbol{V}$ and get an EVD.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f62oTntiW3dz"
      },
      "source": [
        " * Consider the data $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ with mean $\\boldsymbol{\\mu}_{x} \\in \\mathbb{R}^{D}$ and covariance $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{D \\times D}$.\n",
        " * Let $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{T}$ be the eigen decomposition of $\\boldsymbol{\\Sigma}_{x}$.\n",
        " * Let $\\boldsymbol{z}_{i} = \\boldsymbol{U}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$.\n",
        "\n",
        "\n",
        "### 1.4. Question\n",
        "\n",
        "Prove the following:\n",
        "\n",
        " 1. The mean of the set $\\mathcal{Z} = \\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{D} \\right\\}$ is zero, that is, $\\boldsymbol{\\mu}_{z} = \\frac{1}{N} \\sum_{i = 1}^{N} \\boldsymbol{z}_{i} = 0$.\n",
        " 2. The covariance of $\\mathcal{Z}$ is diagonal, that is, $\\boldsymbol{\\Sigma}_{z}$ is diagonal.\n",
        " 3. The pair wise distance is equal, that is, $\\forall i, j: \\; {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ows8hzJW3d0"
      },
      "source": [
        "### 1.4. Solution\n",
        "\n",
        "1. The mean of z is $\\boldsymbol{\\mu}_{z} = \\frac{1}{N} \\sum_{i = 1}^{N} \\boldsymbol{z}_{i}=  \\frac{1}{N} \\sum_{i = 1}^{N} (\\boldsymbol{{U}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)})=  \\frac{1}{N}\\boldsymbol{{U}^{T}} \\sum_{i = 1}^{N} {\\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)}$\n",
        "\n",
        "By definition, ${\\mu}_{x}=  \\frac{\\sum{x}_{i}}{N} $\n",
        "\n",
        " $\\sum_{i = 1}^{N}({x}_{i} - \\frac{\\sum{x}_{i}}{N})= ({x}_{1}- \\frac{\\sum{x}_{i}}{N})+ ({x}_{2}- \\frac{\\sum{x}_{i}}{N})...+({x}_{N}- \\frac{\\sum{x}_{i}}{N})$\n",
        "\n",
        " $=({x}_{1}+{x}_{2}...+{x}_{N})- N*\\frac{\\sum{x}_{i}}{N}= \\frac{\\sum{x}_{i}}{N} - \\frac{\\sum{x}_{i}}{N}= 0$\n",
        "\n",
        "Thus:\n",
        "$\\frac{1}{N}\\boldsymbol{{U}^{T}} \\sum_{i = 1}^{N} {\\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)} = \\frac{1}{N}\\boldsymbol{{U}^{T}} *0= 0$\n",
        "\n",
        "2. The covariance of Z is diagonal:\n",
        "\n",
        "The covariance of random vector X defined by: $E[(X-E(X))(X-E(X))^{T}] $\n",
        "\n",
        "Cov(Z)= $E[({Z}-{\\mu}_{z})({Z}-{\\mu}_{z})^{T}] $, we prove that ${\\mu}_{z}=0$ , thus:\n",
        "\n",
        "$E[{Z}{Z}^{T}]= E[(U^{T}(X-{\\mu}_{x}))(U^{T}(X-{\\mu}_{x}))^{T}]=E[U^{T}(X-{\\mu}_{x})(X-{\\mu}_{x})^{T}U]= U^{T} E[(X-{\\mu}_{x})(X-{\\mu}_{x})^{T}]U$\n",
        "\n",
        "*${\\Sigma}_{x}= E[(X-{\\mu}_{x})(X-{\\mu}_{x})^{T}]=E(XX^{T})$\n",
        "\n",
        "$= U^{T} E[(X-{\\mu}_{x})(X-{\\mu}_{x})^{T}]U= U^{T} {\\Sigma}_{x}U =U^{T}\\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{T}U =  \\boldsymbol{I}\\boldsymbol{\\Lambda}\\boldsymbol{I} = \\boldsymbol{\\Lambda}$\n",
        "\n",
        "\n",
        "${\\Lambda}$ is a diagonal matrix that contains eigenvalues of ${\\Sigma}_{x}$, thus ${\\Sigma}_{z}$ is diagonal\n",
        "\n",
        "\n",
        "3. ${\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}= {\\left\\| \\boldsymbol{U^{T}(x_{i}-{\\mu}_{x}}) - \\boldsymbol{U^{T}(x_{j}-{\\mu}_{x})} \\right\\|}_{2} = $\n",
        "${\\left\\| \\boldsymbol{U^{T}x_{i}-U^{T}{\\mu}_{x}} - \\boldsymbol{U^{T}x_{j}+U^{T}{\\mu}_{x})} \\right\\|}_{2}=$\n",
        "\n",
        "${\\left\\| \\boldsymbol{U^{T}x_{i}} - \\boldsymbol{U^{T}x_{j}} \\right\\|}_{2}= {\\left\\| \\boldsymbol{U^{T}(x_{i}} - \\boldsymbol{x_{j})} \\right\\|}_{2}$\n",
        "\n",
        "For any orthogonal matrix U, it holds that: $ {\\left\\| \\boldsymbol{Ux} \\right\\|}_{2}=  {\\left\\| \\boldsymbol{x} \\right\\|}_{2}$ - that is, the length of\n",
        "any vector is invariant under multiplication by $\\boldsymbol{U}$.\n",
        "\n",
        "Thus, ${\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} = {\\left\\| \\boldsymbol{U^{T}(x_{i}} - \\boldsymbol{x_{j})} \\right\\|}_{2}= {\\left\\| \\boldsymbol{x_{i}} - \\boldsymbol{x_{j}} \\right\\|}_{2}$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EBpQzKwW3d0"
      },
      "source": [
        " * Let $\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}$ be a full rank matrix with $d \\leq D$.\n",
        "\n",
        " * <font color='brown'>(**#**)</font> If a matrix has rank $d$ it means its SVD  has $d$ non zero singular values.\n",
        "\n",
        "\n",
        "### 1.5. Question\n",
        "\n",
        "Show that exists an invertible matrix $\\boldsymbol{M} \\in \\mathbb{R}^{d \\times d}$ such that $\\boldsymbol{O} = \\boldsymbol{U}_{d} \\boldsymbol{M} \\in \\mathbb{R}^{D \\times d}$ is semi orthogonal, that is $\\boldsymbol{O}^{T} \\boldsymbol{O} = \\boldsymbol{I}_{d}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u36HkkLHW3d0"
      },
      "source": [
        "### 1.5. Solution\n",
        "To show that there exists an invertible matrix $\\boldsymbol{M} \\in \\mathbb{R}^{d \\times d}$ such that $\\boldsymbol{O} = \\boldsymbol{U}_d \\boldsymbol{M}$ is semi-orthogonal, we need to demonstrate that $\\boldsymbol{O}^T \\boldsymbol{O} = \\boldsymbol{I}_d$, where $\\boldsymbol{O}^T$ denotes the transpose of $\\boldsymbol{O}$ and $\\boldsymbol{I}_d$ is the $d \\times d$ identity matrix.\n",
        "\n",
        "Let's write the expression for $\\boldsymbol{O}^T \\boldsymbol{O}$ in terms of $\\boldsymbol{U}_d$ and $\\boldsymbol{M}$ and expanding:\n",
        "\n",
        "$\\boldsymbol{O}^T \\boldsymbol{O} = {(\\boldsymbol{U}_{d} \\boldsymbol{M})}^T (\\boldsymbol{U}_{d} \\boldsymbol{M}) = \\boldsymbol{M}^T \\boldsymbol{U}_{d}^T \\boldsymbol{U}_{d} \\boldsymbol{M}$\n",
        "\n",
        "Since $\\boldsymbol{U}_d$ is full rank, its transpose $\\boldsymbol{U}_d^T$ and product $\\boldsymbol{U}_d^T \\boldsymbol{U}_d$ are invertible.\n",
        "\n",
        "Denote $\\boldsymbol{U}_d^T \\boldsymbol{U}_d$ as $\\boldsymbol{A}$. So we will get:\n",
        "\n",
        "$\\boldsymbol{O}^T \\boldsymbol{O} = \\boldsymbol{M}^T \\boldsymbol{A} \\boldsymbol{M}$\n",
        "\n",
        "<br>\n",
        "\n",
        "To satisfy $\\boldsymbol{O}^T \\boldsymbol{O} = \\boldsymbol{I}_d$, it must be the case that $\\boldsymbol{M}^T \\boldsymbol{A} \\boldsymbol{M} = \\boldsymbol{I}_d$.\n",
        "Since $\\boldsymbol{A}$ is invertible, we can take its inverse, denoted as $\\boldsymbol{A}^{-1}$ and multiplying both sides of the equation by $\\boldsymbol{A}^{-1}$ from the left:\n",
        "\n",
        "$\\boldsymbol{M}^T \\boldsymbol{A}^{-1} \\boldsymbol{A} \\boldsymbol{M} = \\boldsymbol{M}^T \\boldsymbol{M} = \\boldsymbol{I}_d$\n",
        "\n",
        "This implies that $\\boldsymbol{M}^T$ is the inverse of $\\boldsymbol{M}$, and therefore $\\boldsymbol{M}$ is invertible.\n",
        "\n",
        "<br>\n",
        "\n",
        "Hence, we have shown that there exists an invertible matrix $\\boldsymbol{M} \\in \\mathbb{R}^{d \\times d}$ such that $\\boldsymbol{O} = \\boldsymbol{U}_d \\boldsymbol{M}$ is semi-orthogonal, satisfying $\\boldsymbol{O}^T \\boldsymbol{O} = \\boldsymbol{I}_d$.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkqXCVA9W3d0"
      },
      "source": [
        " * Consider the data $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$ with mean $\\boldsymbol{\\mu}_{x} \\in \\mathbb{R}^{D}$ and covariance $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{D \\times D}$.\n",
        " * Let $\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}$ be a semi orthogonal matrix, that is, $\\boldsymbol{U}_{d}^{T} \\boldsymbol{U}_{d} = \\boldsymbol{I}$.\n",
        " * Where $\\boldsymbol{U}_{d}$ be the $d$ eigen vectors corressponding to the $d$ largest eigen values of $\\boldsymbol{\\Sigma}_{x}$.\n",
        " * Let $\\boldsymbol{z}_{i} = \\boldsymbol{U}_{d}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$.\n",
        " * Let $\\hat{\\boldsymbol{x}}_{i} = \\boldsymbol{U}_{d} \\boldsymbol{z}_{i} + \\boldsymbol{\\mu}_{x}$.\n",
        " * Let $\\boldsymbol{\\epsilon}_{i} = \\hat{\\boldsymbol{x}}_{i} - \\boldsymbol{x}_{i}$.\n",
        "\n",
        "\n",
        "### 1.7. Question\n",
        "\n",
        "Prove the following:\n",
        "\n",
        " 1. $\\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{x} \\right) = \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{z} \\right) + \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{\\epsilon} \\right)$.\n",
        " 2. $\\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{\\epsilon} \\right) = \\sum_{i = d + 1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)$.\n",
        "\n",
        "Where\n",
        "\n",
        " * $\\boldsymbol{\\Sigma}_{z} \\in \\mathbb{R}^{d \\times d}$ is the covariance of $\\left\\{ \\boldsymbol{z}_{i} \\right\\}_{i = 1}^{N}$.\n",
        " * $\\boldsymbol{\\Sigma}_{\\epsilon} \\in \\mathbb{R}^{D \\times D}$ is the covariance of $\\left\\{ \\boldsymbol{\\epsilon}_{i} \\right\\}_{i = 1}^{N}$.\n",
        "\n",
        "</br>\n",
        "\n",
        " * <font color='brown'>(**#**)</font> The idea is to prove the total variance / energy is kept. Part of it in the low dimensionality data and the other is in the error (Lost information).\n",
        " * <font color='brown'>(**#**)</font> To make calculations easier, think how $\\boldsymbol{\\mu}_{x}$ effects the variance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxrxdKqW3d1"
      },
      "source": [
        "### 1.7. Solution\n",
        "\n",
        "1. We will denote $\\boldsymbol{\\epsilon} = \\boldsymbol{X} - \\hat{\\boldsymbol{X}}$, where $\\boldsymbol{\\epsilon}_{i} = \\hat{\\boldsymbol{x}}_{i} - \\boldsymbol{x}_{i}$.\n",
        "\n",
        "We've seen in class that $\\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{z} \\right) = \\operatorname{Tr} \\left(\\boldsymbol{U}_{d}^{T}\\boldsymbol{\\Sigma}_{x}\\boldsymbol{U}_{d} \\right) $\n",
        "\n",
        "We proved that $\\frac{1}{N} {\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2} = \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{x}\\right) -\\operatorname{Tr} \\left(\\boldsymbol{U}_{d}^{T}\\boldsymbol{\\Sigma}_{x}\\boldsymbol{U}_{d} \\right) = \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{x}\\right) - \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{z} \\right)$\n",
        "\n",
        "Let's approve that $\\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{\\epsilon} \\right) =  \\frac{1}{N} {\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2}$\n",
        "\n",
        "$\\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{\\epsilon} \\right) = \\operatorname{Tr} \\left(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{T}] \\right) = \\operatorname{Tr} \\left(E[(\\boldsymbol{X} - \\hat{\\boldsymbol{X}})(\\boldsymbol{X} - \\hat{\\boldsymbol{X}})^{T}] \\right) = \\operatorname{Tr} \\left(E[(\\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})(\\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})^{T}] \\right) = \\operatorname{Tr} \\left(\\frac{1}{N}[(\\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})(\\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})^{T}] \\right) = \\frac{1}{N}\\operatorname{Tr} \\left([(\\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})(\\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})^{T}] \\right) = \\frac{1}{N} {\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2}$\n",
        "\n",
        "Finally, we got that $\\frac{1}{N} {\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2} = \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{\\epsilon} \\right) = \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{x}\\right) - \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{z} \\right)$.\n",
        "\n",
        "As a result, $\\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{x}\\right) =  \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{\\epsilon} \\right) + \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{z} \\right)$.\n",
        "\n",
        "\n",
        "2.in class we've shown that: $\\Sigma_{x}U_{d}= U_{d}\\Lambda_{d}$\n",
        "\n",
        "$Tr(\\Sigma_{z})= Tr(U_{d}^{T}\\Sigma_{x}U_{d})= Tr(U_{d}^{T}U_{d}\\Lambda_{d})=Tr(\\Lambda_{d}) = \\sum_{i = 1}^{d} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)\n",
        "$\n",
        "\n",
        "In addition, $\\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{x}\\right) = \\sum_{i = 1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)$.\n",
        "\n",
        "Let's replace $\\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{x}\\right)$ and $\\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{z}\\right)$ in 1:\n",
        "\n",
        "$\\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{x}\\right) =  \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{\\epsilon} \\right) + \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{z} \\right) \\implies \\sum_{i = 1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right) = \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{\\epsilon} \\right) + \\sum_{i = 1}^{d} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right) \\implies \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{\\epsilon} \\right) = \\sum_{i = 1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right) - \\sum_{i = 1}^{d} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right) \\implies \\operatorname{Tr} \\left(\\boldsymbol{\\Sigma}_{\\epsilon} \\right) = \\sum_{i = d + 1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVeaEbsSW3d1"
      },
      "source": [
        " * Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a square matrix.\n",
        " * Let $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\in \\mathbb{R}^{d}$ be eigen vectors with the same eigen value $\\lambda$ such that $\\boldsymbol{A} \\boldsymbol{u}_{1} = \\lambda \\boldsymbol{u}_{1}$ and $\\boldsymbol{A} \\boldsymbol{u}_{2} = \\lambda \\boldsymbol{u}_{2}$.\n",
        " * The vectors $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2}$ are perpendicular, that is, $\\left \\langle \\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\right \\rangle = 0$.\n",
        " * Let $\\boldsymbol{v} = \\alpha \\boldsymbol{u}_{1}$ where $\\alpha \\neq 0$.\n",
        "\n",
        "\n",
        "### 1.10. Question\n",
        "\n",
        " 1. Show $\\boldsymbol{v}$ is an eigen vector of $\\boldsymbol{A}$ and find its corresponding eigen value.\n",
        " 2. Prove or disprove that $\\boldsymbol{u} = \\boldsymbol{u}_{1} + \\boldsymbol{u}_{2}$ is an eigen vector of $A$.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOqePgb-W3d1"
      },
      "source": [
        "### 1.10. Solution\n",
        "\n",
        "1. To show that $\\boldsymbol{v}$ is an eigenvector of $\\boldsymbol{A}$, we need to demonstrate that $\\boldsymbol{Av} = \\lambda\\boldsymbol{v}$, where $\\lambda$ is the corresponding eigenvalue.\n",
        "\n",
        "<blockquote>\n",
        "\n",
        "Given that $\\boldsymbol{A}\\boldsymbol{u}_1 = \\lambda\\boldsymbol{u}_1$, we can substitute $\\boldsymbol{v}$ in terms of $\\boldsymbol{u}_1$ as $\\boldsymbol{v} = \\alpha\\boldsymbol{u}_1$.\n",
        "\n",
        "So we can compute $\\boldsymbol{Av}$ as follow:\n",
        "\n",
        "$\n",
        "\\boldsymbol{Av} =\n",
        "\\boldsymbol{A}(\\alpha\\boldsymbol{u}_1)\n",
        "$\n",
        "\n",
        "$\n",
        "= \\alpha\\boldsymbol{A}\\boldsymbol{u}_1 \\quad \\text{(due to linearity of matrix multiplication)}\n",
        "$\n",
        "\n",
        "$\n",
        "= \\alpha(\\lambda\\boldsymbol{u}_1) \\quad (\\text{since} \\boldsymbol{A}\\boldsymbol{u}_1 = \\lambda\\boldsymbol{u}_1)\n",
        "$\n",
        "\n",
        "$\n",
        "= \\lambda(\\alpha\\boldsymbol{u}_1) \\quad \\text{(rearranging terms)}\n",
        "$\n",
        "\n",
        "$\n",
        "= \\lambda\\boldsymbol{v}\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "Therefore, we have shown that $\\boldsymbol{v}$ is an eigenvector of $\\boldsymbol{A}$ with eigenvalue $\\lambda$.\n",
        "\n",
        "</blockquote>\n",
        "\n",
        "2. To determine whether $\\boldsymbol{u} = \\boldsymbol{u}_1 + \\boldsymbol{u}_2$ is an eigenvector of $\\boldsymbol{A}$, we need to check if there exists an eigenvalue $\\lambda$ such that $\\boldsymbol{A}\\boldsymbol{u} = \\lambda\\boldsymbol{u}$.\n",
        "\n",
        "<blockquote>\n",
        "\n",
        "Let's compute $\\boldsymbol{A}\\boldsymbol{u}$:\n",
        "\n",
        "$\n",
        "\\boldsymbol{A}\\boldsymbol{u}\n",
        "= \\boldsymbol{A}(\\boldsymbol{u}_1 + \\boldsymbol{u}_2)\n",
        "$\n",
        "\n",
        "$\n",
        "= \\boldsymbol{A}\\boldsymbol{u}_1 + \\boldsymbol{A}\\boldsymbol{u}_2 \\quad \\text{(by distributive property)}\n",
        "$\n",
        "\n",
        "$\n",
        "= \\lambda \\boldsymbol{u}_1 + \\lambda\\boldsymbol{u}_2 \\quad (\\text{since} \\boldsymbol{A}\\boldsymbol{u}_1 = \\lambda\\boldsymbol{u}_1$ and $\\boldsymbol{A}\\boldsymbol{u}_2 = \\lambda\\boldsymbol{u}_2)\n",
        "$\n",
        "\n",
        "$\n",
        "= \\lambda(\\boldsymbol{u}_1 + \\boldsymbol{u}_2) \\quad (\\text{factor out} \\lambda)\n",
        "$\n",
        "\n",
        "$\n",
        "= \\lambda\\boldsymbol{u}\n",
        "$\n",
        "\n",
        "From the above calculation, we see that $\\boldsymbol{A}\\boldsymbol{u} = \\lambda\\boldsymbol{u}$, which implies that $\\boldsymbol{u}$ is an eigenvector of $\\boldsymbol{A}$ with eigenvalue $\\lambda$.\n",
        "\n",
        "Therefore, we have proven that $\\boldsymbol{u} = \\boldsymbol{u}_1 + \\boldsymbol{u}_2$ is indeed an eigenvector of $\\boldsymbol{A}$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTK1_VpYW3d1"
      },
      "source": [
        " * Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a diagonalizable matrix.\n",
        " * Let $\\left\\{ \\left( \\boldsymbol{u}_{i}, {\\lambda}_{i} \\right) \\right\\}_{i = 1}^{d}$ be the set of eigen pairs, that is, $\\boldsymbol{A} \\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$.\n",
        " * Let $\\boldsymbol{B} = \\boldsymbol{R} \\boldsymbol{A} \\boldsymbol{R}^{T}$ where $\\boldsymbol{R} \\in \\mathbb{R}^{d \\times d}$ is an orthogonal matrix, namely, $\\boldsymbol{R}^{T} \\boldsymbol{R} = \\boldsymbol{R} \\boldsymbol{R}^{T} = \\boldsymbol{I}$.\n",
        "\n",
        "\n",
        "### 1.11. Question\n",
        "\n",
        "Find the set $\\left\\{ \\left( \\boldsymbol{v}_{i}, {\\alpha}_{i} \\right) \\right\\}_{i = 1}^{d}$ of eigen pairs of $\\boldsymbol{B}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj3UKWk-W3d1"
      },
      "source": [
        "### 1.11. Solution\n",
        "\n",
        "given $Au= \\lambda u$ and $B=RAR^{T}$, in particular $R^{T}BR=R^{T}(RAR^{T})R=(R^{T}R)A(R^{T}R) =A$\n",
        "\n",
        "$Au=R^{T}BRu= \\lambda u$\n",
        "\n",
        "$(RR^{T})BRu= R\\lambda u$\n",
        "\n",
        "$B(Ru)= \\lambda (Ru)$\n",
        "\n",
        "Thus, $\\left\\{ \\left( \\boldsymbol{Ru}_{i}, \\lambda_{i} \\right) \\right\\}_{i = 1}^{d}$ is the set of eigen pairs of $\\boldsymbol{B}$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSNAZkpvW3d1"
      },
      "source": [
        " * Consider the data $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{10} \\right\\}_{i = 1}^{N}$ and its matrix form $\\boldsymbol{X} \\in \\mathbb{R}^{10 \\times N}$.\n",
        " * Let $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{10 \\times 10}$ be the covariance matrix where all of its eigen values are unique.\n",
        " * Let $\\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{3} \\right\\}_{i = 1}^{N}$ be the low dimensional representation obtained by applying PCA from $\\mathbb{R}^{10}$ to $\\mathbb{R}^{3}$\n",
        " * Let $\\left\\{ \\boldsymbol{w}_{i} \\in \\mathbb{R}^{2} \\right\\}_{i = 1}^{N}$ be the low dimensional representation obtained by applying PCA from $\\mathbb{R}^{10}$ to $\\mathbb{R}^{2}$\n",
        "\n",
        "\n",
        "### 1.12. Question\n",
        "\n",
        "Prove or disprove:\n",
        "\n",
        "$$ \\boldsymbol{w}_{i} = \\begin{bmatrix}\n",
        "{\\alpha}_{1} & 0 & 0 \\\\\n",
        "0 & {\\alpha}_{2} & 0\n",
        "\\end{bmatrix} \\boldsymbol{z}_{i}, \\; {\\alpha}_{1}, {\\alpha}_{2} \\in \\left\\{ -1, 1 \\right\\} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JSsuyMdW3d2"
      },
      "source": [
        "### 1.12. Solution\n",
        "\n",
        "$\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{U}\\boldsymbol{\\Lambda}\\boldsymbol{U}^{T}$\\\n",
        "$\\boldsymbol{z}_{i} = \\boldsymbol{U}_3^{T}\\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$\\\n",
        "$\\boldsymbol{w}_{i} = \\tilde{\\boldsymbol{U}}_2^{T}\\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$\\\n",
        "$\\boldsymbol{U}_3 = \\left[\n",
        "  \\begin{array}{cccc}\n",
        "    | & | &  | \\\\\n",
        "    u_{1}  & u_{2} & u_{3}    \\\\\n",
        "    | & | &   |\n",
        "  \\end{array}\n",
        "\\right]$\\\n",
        "$\\tilde{\\boldsymbol{U}}_2 = \\left[\n",
        "  \\begin{array}{cccc}\n",
        "    | & |  \\\\\n",
        "    \\tilde{u}_{1}  & \\tilde{u}_{2}    \\\\\n",
        "    | & |\n",
        "  \\end{array}\n",
        "\\right]$\\\n",
        "$u_{1},u_{2}, u_{3}$ are all eigenvectors of $\\boldsymbol{\\Sigma}_{x}$ and since all eigenvalues are unique, the normalized eigenvectors are also unique up to a sign.\\\n",
        "Since vectors are ordered according to decreasing eigenvalue, this means that:\\\n",
        "$\\boldsymbol{u}_{1} = \\pm \\tilde{\\boldsymbol{u}}_1$\\\n",
        "$\\boldsymbol{u}_{2} = \\pm \\tilde{\\boldsymbol{u}}_2$\n",
        "\n",
        "\n",
        "To construct a transition matrix $\\boldsymbol{A}$ between the bases of $\\boldsymbol{U}_3$ and $\\tilde{\\boldsymbol{U}}_2$ we can write:\\\n",
        "$\\boldsymbol{A}\\boldsymbol{u}_{1} = \\tilde{\\boldsymbol{u}}_1 = \\pm\\boldsymbol{u}_{1}$\\\n",
        "$\\boldsymbol{A}\\boldsymbol{u}_{2} = \\tilde{\\boldsymbol{u}}_2 = \\pm\\boldsymbol{u}_{2}$\\\n",
        "$\\boldsymbol{A}\\boldsymbol{u}_{3} = 0$\n",
        "\n",
        "$\\implies \\boldsymbol{w}_{i} = \\tilde{\\boldsymbol{U}}_2^{T}\\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)=\\boldsymbol{A}\\boldsymbol{U}_3^{T}\\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)=\\boldsymbol{A}\\boldsymbol{z}_{i}\\quad$, with:\\\n",
        "$$ \\boldsymbol{A} = \\begin{bmatrix}\n",
        "{\\alpha}_{1} & 0 & 0 \\\\\n",
        "0 & {\\alpha}_{2} & 0\n",
        "\\end{bmatrix}, \\; {\\alpha}_{1}, {\\alpha}_{2} \\in \\left\\{ -1, 1 \\right\\} $$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH9a0Z_1W3d2"
      },
      "source": [
        "## 2. Kernel Principle Component Analysis (K-PCA)\n",
        "\n",
        "Let $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T} \\in \\mathbb{R}^{N \\times N}$ be the centering matrix.\n",
        "\n",
        "### 2.1. Question\n",
        "\n",
        "Prove that $\\boldsymbol{J}$ is an idempotent matrix, that is, $\\boldsymbol{J}^{k} = \\boldsymbol{J}$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> This implies that if we center the data multiple times it will have no effect beyond doing it once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCC5oWZRW3d2"
      },
      "source": [
        "### 2.1. Solution\n",
        "\n",
        "We will show that $J=J^{2}$:\n",
        "\n",
        "$$\n",
        "J^{2}= (\\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T})(\\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T})= I^{2}- \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}- \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}+ \\frac{1}{N^{2}} \\boldsymbol{1} \\boldsymbol{1}^{T}\\boldsymbol{1} \\boldsymbol{1}^{T}\n",
        "$$\n",
        "\n",
        "Lets notice that $\\boldsymbol{1} \\boldsymbol{1}^{T}$ is $NxN$ matrix of 1's, so the result of $\\boldsymbol{1} \\boldsymbol{1}^{T}\\boldsymbol{1} \\boldsymbol{1}^{T}$ is a $NxN$ matrix with $N$.\n",
        "\n",
        "So basically, $\\boldsymbol{1} \\boldsymbol{1}^{T}\\boldsymbol{1} \\boldsymbol{1}^{T}= N \\boldsymbol{1} \\boldsymbol{1}^{T}$. Therefore:\n",
        "$$\n",
        "I^{2}- \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}- \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}+ \\frac{1}{N^{2}} \\boldsymbol{1} \\boldsymbol{1}^{T}\\boldsymbol{1} \\boldsymbol{1}^{T} = I- \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}- \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T} + \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}= I- \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}= J\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\implies J=J^{2}\n",
        "$$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZS4DP1cW3d2"
      },
      "source": [
        " * Let $\\boldsymbol{X} \\in \\mathbb{R}^{D \\times N}$.\n",
        " * The covariance matrix $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{X} \\boldsymbol{X}^{T} \\in \\mathbb{R}^{D \\times D}$.\n",
        " * The matrix $\\boldsymbol{K}_{x} = \\boldsymbol{X}^{T} \\boldsymbol{X} \\in \\mathbb{R}^{N \\times N}$.\n",
        " * Let $\\left( \\boldsymbol{u}_{i}, {\\lambda}_{i} > 0 \\right)$ be an eigen pair of $\\boldsymbol{\\Sigma}_{x}$, such that, $\\boldsymbol{\\Sigma}_{x} \\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$.\n",
        "\n",
        "\n",
        "### 2.2. Question\n",
        "\n",
        " 1. Show that ${\\lambda}_{i}$ is an eigen value of $\\boldsymbol{K}_{x}$.\n",
        " 2. Find the corresponding eigen vector $\\boldsymbol{v}_{i}$ such that $\\boldsymbol{K}_{x} \\boldsymbol{v}_{i} = {\\lambda}_{i} \\boldsymbol{v}_{i}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P8udrSUW3d2"
      },
      "source": [
        "### 2.2. Solution\n",
        "\n",
        " 1. ${\\lambda}_{i}$ is an eigenvalue of $\\boldsymbol{K}_{x}$:\n",
        "\n",
        "<blockquote>\n",
        "\n",
        "Given that $\\boldsymbol{\\Sigma}_{x} \\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$, we can left-multiply both sides of the equation by $\\boldsymbol{X^T}$:\n",
        "\n",
        "$\\boldsymbol{X^T} \\boldsymbol{\\Sigma}_{x} \\boldsymbol{u}_{i} = \\boldsymbol{X^T} {\\lambda}_{i} \\boldsymbol{u}_{i}$\n",
        "\n",
        "Let's substitute $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{X} \\boldsymbol{X}^{T}$:\n",
        "\n",
        "$\\boldsymbol{X^T} (\\boldsymbol{X} \\boldsymbol{X}^{T}) \\boldsymbol{u}_{i} = {\\lambda}_{i} (\\boldsymbol{X^T} \\boldsymbol{u}_{i})$\n",
        "\n",
        "Using the associative property of matrix multiplication:\n",
        "\n",
        "$(\\boldsymbol{X^T} \\boldsymbol{X}) \\boldsymbol{X}^{T} \\boldsymbol{u}_{i} = {\\lambda}_{i} (\\boldsymbol{X^T} \\boldsymbol{u}_{i})$\n",
        "\n",
        "Let $\\boldsymbol{v_i}$ be a new vector, such that $\\boldsymbol{v_i} = \\boldsymbol{X^T} \\boldsymbol{u}_{i}$. Therefore:\n",
        "\n",
        "$(\\boldsymbol{X^T} \\boldsymbol{X}) \\boldsymbol{v_i} = {\\lambda}_{i} \\boldsymbol{v_i}$\n",
        "\n",
        "Therefore, ${\\lambda}_{i}$ is an eigenvalue of $\\boldsymbol{K}_{x}$.\n",
        "\n",
        "</blockquote>\n",
        "\n",
        "<br>\n",
        "\n",
        " 2. Find the corresponding eigen vector $\\boldsymbol{v}_{i}$ such that $\\boldsymbol{K}_{x} \\boldsymbol{v}_{i} = {\\lambda}_{i} \\boldsymbol{v}_{i}$.\n",
        "\n",
        "<blockquote>\n",
        "\n",
        "To find the corresponding eigenvector $\\boldsymbol{v}_{i}$, we can simply use the eigenvector $\\boldsymbol{u}_{i}$ that we found earlier. Since $\\boldsymbol{X}^T \\boldsymbol{u}_{i} = \\boldsymbol{v}_{i}$, we can conclude that $\\boldsymbol{v}_{i}$ is the eigenvector corresponding to the eigenvalue ${\\lambda}_{i}$ of $\\boldsymbol{K}_{x}$.\n",
        "\n",
        "Therefore, $\\boldsymbol{K}_{x} \\boldsymbol{v}_{i} = {\\lambda}_{i} \\boldsymbol{v}_{i}$, where $\\boldsymbol{v}_{i}$ is the eigenvector corresponding to the eigenvalue ${\\lambda}_{i}$ of $\\boldsymbol{\\Sigma}_{x}$.\n",
        "\n",
        "</blockquote>\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDfWIn7DW3d2"
      },
      "source": [
        " **Kernel Functions**\n",
        "\n",
        " * Let $k: \\mathbb{R}^{d} \\times \\mathbb{R}^{d} \\to \\mathbb{R}$.\n",
        " * Consider $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{N}$.\n",
        "\n",
        "### 2.3. Question\n",
        "\n",
        "Show that if $k \\left( \\cdot, \\cdot \\right)$ can be written as inner product:\n",
        "\n",
        "$$ k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = \\left \\langle \\phi \\left( \\boldsymbol{x}_{i} \\right), \\phi \\left( \\boldsymbol{x}_{j} \\right) \\right \\rangle $$\n",
        "\n",
        "for some $\\phi : \\mathbb{R}^{d} \\to \\mathbb{R}^{M} $ then the matrix defined by $\\boldsymbol{K} \\left[ i, j \\right] = k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) $ is _Symmetric Positive Semi Definite_ (SPSD), that is, $\\boldsymbol{K} \\succeq 0$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYq5kDnBW3d2"
      },
      "source": [
        "### 2.3. Solution\n",
        "\n",
        "$\\boldsymbol{K}$ is a symmetric matrix, since the inner product is symmetric:\\\n",
        "$\\boldsymbol{K} \\left[ i, j \\right] = \\left \\langle \\phi \\left( \\boldsymbol{x}_{i} \\right), \\phi \\left( \\boldsymbol{x}_{j} \\right) \\right \\rangle= \\left \\langle \\phi \\left( \\boldsymbol{x}_{j} \\right), \\phi \\left( \\boldsymbol{x}_{i} \\right) \\right \\rangle= \\boldsymbol{K} \\left[ j, i \\right]$\n",
        "\n",
        "Next, consider the set $\\Phi = \\left\\{ \\phi(\\boldsymbol{x}_{i}) \\right\\}_{i = 1}^{N}$.\n",
        "\n",
        "\n",
        "We can write: $\\boldsymbol{K} \\left[ i, j \\right] = \\phi \\left( \\boldsymbol{x}_{i} \\right)^T \\phi \\left( \\boldsymbol{x}_{j} \\right)$\\\n",
        "In matrix form: $\\boldsymbol{K} = \\Phi^T\\Phi$\n",
        "\n",
        "\n",
        "For every $\\boldsymbol{v} \\in \\mathbb{R}^{d} \\quad \\boldsymbol{v}^T\\boldsymbol{K}\\boldsymbol{v} = \\boldsymbol{v}^T\\Phi^T\\Phi\\boldsymbol{v} = \\left \\langle \\Phi\\boldsymbol{v}, \\Phi\\boldsymbol{v} \\right \\rangle \\geq 0 $\n",
        "$\\implies \\boldsymbol{K} \\succeq 0$ (SPSD)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt47MREAW3d2"
      },
      "source": [
        " * Let $\\boldsymbol{A} \\succ 0 $ be an SPD matrix.\n",
        " * Let $k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = \\boldsymbol{x}_{i}^{T} \\boldsymbol{A} \\boldsymbol{x}_{j} $.\n",
        "\n",
        "### 2.4. Question\n",
        "\n",
        "Prove or disprove that $k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ is a kernel function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzKdrk7mW3d3"
      },
      "source": [
        "### 2.4. Solution\n",
        "\n",
        "Since $\\boldsymbol{A} \\succ 0 $ is an SPD matrix, $\\boldsymbol{A}$ is congruent with a diagonal matrix with all positive eigenvalues:\\\n",
        " $\\boldsymbol{A} = \\boldsymbol{P}^T\\boldsymbol{\\Lambda} \\boldsymbol{P}$\n",
        "\n",
        "Since all eigenvalues are positive, we can easily take the square root of the matrix $\\boldsymbol{\\Lambda}$: $\\quad\\boldsymbol{\\Lambda} = \\boldsymbol{\\Lambda}^{\\frac 1 2}\\boldsymbol{\\Lambda}^{\\frac 1 2} $, with $\\quad {\\boldsymbol{\\Lambda}^{\\frac 1 2}}_{ii} = \\sqrt{\\boldsymbol{\\Lambda}_{ii}}$\n",
        "\n",
        "\n",
        "\n",
        " $k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = \\boldsymbol{x}_{i}^{T} \\boldsymbol{A} \\boldsymbol{x}_{j}  = \\boldsymbol{x}_{i}^{T}  \\boldsymbol{P}^T\\boldsymbol{\\Lambda} \\boldsymbol{P} \\boldsymbol{x}_{j} = \\boldsymbol{x}_{i}^{T}  \\boldsymbol{P}^T\\boldsymbol{\\Lambda}^{\\frac 1 2}\\boldsymbol{\\Lambda}^{\\frac 1 2}  \\boldsymbol{P} \\boldsymbol{x}_{j} = \\boldsymbol{x}_{i}^{T}  \\boldsymbol{P}^T{\\boldsymbol{\\Lambda}^{\\frac 1 2}}^T\\boldsymbol{\\Lambda}^{\\frac 1 2}  \\boldsymbol{P} \\boldsymbol{x}_{j}\\quad $ ($\\boldsymbol{\\Lambda}^{\\frac 1 2}$ is symmetric)\n",
        "\n",
        "\n",
        "Now we can define the mapping: $\\phi: \\boldsymbol{x}_{i} \\rightarrow  \\boldsymbol{\\Lambda}^{\\frac 1 2}  \\boldsymbol{P} \\boldsymbol{x}_{i}$\n",
        "\n",
        "With this:$\\quad k\\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) =  \\phi \\left( \\boldsymbol{x}_{i}\\right)^T \\phi \\left( \\boldsymbol{x}_{j}\\right) = \\left \\langle \\phi \\left( \\boldsymbol{x}_{i} \\right), \\phi \\left( \\boldsymbol{x}_{j} \\right) \\right \\rangle$\n",
        "\n",
        "$k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ can now be written as an explicit inner product and is therefore a kernel function.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyf-mXwfW3d3"
      },
      "source": [
        "\n",
        "Let $k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left( 1 + \\boldsymbol{x}^{T} \\boldsymbol{y} \\right)}^{2}$.\n",
        "\n",
        "### 2.5. Question\n",
        "\n",
        "Prove or disprove that $k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right)$ is a kernel function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg6ln0OFW3d3"
      },
      "source": [
        "### 2.5. Solution\n",
        "\n",
        "To prove that $k(\\boldsymbol{x}, \\boldsymbol{y}) = (1 + \\boldsymbol{x}^T \\boldsymbol{y})^2$ is a kernel function, we need to show that it satisfies the properties of a valid kernel. In particular, we need to demonstrate that $k(\\boldsymbol{x}, \\boldsymbol{y})$ corresponds to a valid positive semi-definite kernel matrix (Symmetry and Positive Semi-definiteness).\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's consider a set of data points $\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_n$, and let $\\boldsymbol{K}$ be the corresponding kernel matrix, where $\\boldsymbol{K}_{ij} = k(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$:\n",
        "\n",
        "<br>\n",
        "\n",
        "$\n",
        "\\boldsymbol{K} =\n",
        "\\begin{bmatrix}\n",
        "k(\\boldsymbol{x_1}, \\boldsymbol{x_1}) & k(\\boldsymbol{x_1}, \\boldsymbol{x_2})\n",
        "&  \\ldots & k(\\boldsymbol{x_1}, \\boldsymbol{x_n}) \\\\\n",
        "k(\\boldsymbol{x_2}, \\boldsymbol{x_1}) & k(\\boldsymbol{x_2}, \\boldsymbol{x_2})\n",
        "& \\ldots & k(\\boldsymbol{x_2}, \\boldsymbol{x_n}) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "k(\\boldsymbol{x_n}, \\boldsymbol{x_1}) & k(\\boldsymbol{x_n}, \\boldsymbol{x_2})\n",
        "& \\ldots & k(\\boldsymbol{x_n}, \\boldsymbol{x_n})\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "1. Symmetry:\n",
        "\n",
        "<blockquote>\n",
        "\n",
        "The kernel matrix should be symmetric. In this case, we have:\n",
        "\n",
        "$\n",
        "\\boldsymbol{K}_{ij} =\n",
        "k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) =\n",
        "\\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right)^2 =\n",
        "\\left(1 + \\boldsymbol{x}_j^T \\boldsymbol{x}_i\\right)^2 =\n",
        "k(\\boldsymbol{x}_j, \\boldsymbol{x}_i) =\n",
        "\\boldsymbol{K}_{ji}\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "Therefore, $k(\\boldsymbol{x}, \\boldsymbol{y})$ satisfies the symmetry property.\n",
        "\n",
        "</blockquote>\n",
        "\n",
        "2. Positive Semi-definiteness:\n",
        "\n",
        "<blockquote>\n",
        "\n",
        "For any vector $\\boldsymbol{c}$, we need to show that $\\boldsymbol{c}^T \\boldsymbol{K} \\boldsymbol{c} \\geq 0$:\n",
        "\n",
        "$\n",
        "\\boldsymbol{c}^T \\boldsymbol{K} \\boldsymbol{c} =\n",
        "\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\boldsymbol{c_i} \\boldsymbol{c_j} k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) =\n",
        "\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\boldsymbol{c_i} \\boldsymbol{c_j} \\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right)^2\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "We can rewrite the squared term as:\n",
        "\n",
        "$\n",
        "\\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right)^2 =\n",
        "1 + 2 \\boldsymbol{x}_i^T \\boldsymbol{x}_j + \\left(\\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right)^2 =\n",
        "1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j + \\boldsymbol{x}_i^T \\boldsymbol{x}_j + \\left(\\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right)^2 =\n",
        "\\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right) + \\boldsymbol{x}_i^T \\boldsymbol{x}_j \\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right)\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "Substituting this back into the expression, we have:\n",
        "\n",
        "$\n",
        "\\boldsymbol{c}^T \\boldsymbol{K} \\boldsymbol{c} =\n",
        "\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\boldsymbol{c_i} \\boldsymbol{c_j} \\left[\\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right) + \\boldsymbol{x}_i^T \\boldsymbol{x}_j \\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right)\\right] =\n",
        "\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\boldsymbol{c_i} \\boldsymbol{c_j} \\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right) + \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\boldsymbol{c_i} \\boldsymbol{c_j} \\boldsymbol{x}_i^T \\boldsymbol{x}_j \\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right)\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "Note that the first term $\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\boldsymbol{c_i} \\boldsymbol{c_j} \\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right)$ is a scalar term, so we can rewrite it as $\\boldsymbol{c}^T \\boldsymbol{M} \\boldsymbol{c}$, where $\\boldsymbol{M}$ is a positive semi-definite matrix (since it corresponds to a sum of valid kernel evaluations).\n",
        "\n",
        "<br>\n",
        "\n",
        "The second term can be expanded as:\n",
        "\n",
        "$\n",
        "\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\boldsymbol{c_i} \\boldsymbol{c_j} \\boldsymbol{x}_i^T \\boldsymbol{x}_j \\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right) =\n",
        "\\left(\\sum_{i=1}^{n} \\boldsymbol{c_i} \\boldsymbol{x}_i\\right)^T \\left(\\sum_{j=1}^{n} \\boldsymbol{c_j} \\boldsymbol{x}_j\\right) \\left(\\boldsymbol{I} + \\boldsymbol{X} \\boldsymbol{X}^T\\right)\n",
        "$\n",
        "\n",
        "where $\\boldsymbol{X}$ is the matrix of data points with columns $\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_n$, and $\\boldsymbol{I}$ is the identity matrix.\n",
        "\n",
        "<br>\n",
        "\n",
        "Since $\\boldsymbol{I} + \\boldsymbol{X} \\boldsymbol{X}^T$ is positive semi-definite (the sum of a positive semi-definite matrix and the identity matrix is positive semi-definite):\n",
        "\n",
        "$\n",
        "\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\boldsymbol{c_i} \\boldsymbol{c_j} \\boldsymbol{x}_i^T \\boldsymbol{x}_j \\left(1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\right) =\n",
        "\\left(\\sum_{i=1}^{n} \\boldsymbol{c_i} \\boldsymbol{x}_i\\right)^T \\left(\\sum_{j=1}^{n} \\boldsymbol{c_j} \\boldsymbol{x}_j\\right) \\left(\\boldsymbol{I} + \\boldsymbol{X} \\boldsymbol{X}^T\\right) =\n",
        "\\boldsymbol{v}^T \\left(\\boldsymbol{I} + \\boldsymbol{X} \\boldsymbol{X}^T\\right) \\boldsymbol{v}\n",
        "$\n",
        "\n",
        "where $\\boldsymbol{v} = \\sum_{i=1}^{n} \\boldsymbol{c_i} \\boldsymbol{x}_i$.\n",
        "\n",
        "<br>\n",
        "\n",
        "Combining the terms will produce that:\n",
        "\n",
        "$\n",
        "\\boldsymbol{c}^T \\boldsymbol{K} \\boldsymbol{c} =\n",
        "\\boldsymbol{c}^T \\boldsymbol{M} \\boldsymbol{c} + \\boldsymbol{v}^T \\left(\\boldsymbol{I} + \\boldsymbol{X} \\boldsymbol{X}^T\\right) \\boldsymbol{v}\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "Since both terms on the right-hand side are non-negative (the first term corresponds to a valid kernel matrix, and the second term is positive semi-definite), we conclude that $\\boldsymbol{c}^T \\boldsymbol{K} \\boldsymbol{c} \\geq 0$ for any vector $\\boldsymbol{c}$.\n",
        "\n",
        "</blockquote>\n",
        "\n",
        "<br>\n",
        "\n",
        "Therefore, $k(\\boldsymbol{x}, \\boldsymbol{y}) = (1 + \\boldsymbol{x}^T \\boldsymbol{y})^2$ is a valid kernel function.\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsRkp_foW3d3"
      },
      "source": [
        "\n",
        " * Let $\\boldsymbol{K}_{x} \\left[ i, j \\right] = k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ be an **SPD** kernel matrix.\n",
        " * Let $\\tilde{\\boldsymbol{K}}_{x} = \\boldsymbol{J} \\boldsymbol{K}_{x} \\boldsymbol{J}$ be a centered kernel matrix where $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$.\n",
        "\n",
        "### 2.7. Question\n",
        "\n",
        "Prove or disprove: $\\tilde{\\boldsymbol{K}}_{x}$ is an SPD matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLHM8z0BW3d3"
      },
      "source": [
        "### 2.7. Solution\n",
        "\n",
        "Incorrect.\n",
        "\n",
        "Counter example: $\\boldsymbol{K}_{x}= \\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "2 & 4\n",
        "\\end{bmatrix}$\n",
        "\n",
        "$\\boldsymbol{K}_{x}$ is SPD since it's symmertic and satisfies: $\\boldsymbol{x}^{T} \\boldsymbol{K}\\boldsymbol{x}>0$ for every non zero $\\boldsymbol{x}$:\n",
        "\n",
        "given $\\boldsymbol{x}=[x_{1},x_{2}]: \\boldsymbol{x}^{T}\\boldsymbol{K}\\boldsymbol{x} = x_{1}^{2} + 4x_{1}x_{2} + 4x_{2}^{2} >0$\n",
        "\n",
        "$\\boldsymbol{J}$ with $N=2$: \\begin{bmatrix}\n",
        "0.5 & -0.5 \\\\\n",
        "-0.5 & 0.5\n",
        "\\end{bmatrix}\n",
        "\n",
        "So,\n",
        "$\\tilde{\\boldsymbol{K}}_{x}= \\boldsymbol{J} \\boldsymbol{K}_{x} \\boldsymbol{J}$= \\begin{bmatrix}\n",
        "0.25 & -0.25 \\\\\n",
        "-0.25 & 0.25\n",
        "\\end{bmatrix}\n",
        "\n",
        "given $\\boldsymbol{x}=[1,1]$\n",
        "\n",
        "$\\boldsymbol{x}^{T}(\\boldsymbol{J} \\boldsymbol{K}_{x} \\boldsymbol{J})\\boldsymbol{x} = 0$, means that there is a non zero vector $\\boldsymbol{x}$ where $\\boldsymbol{x}^{T}(\\boldsymbol{J} \\boldsymbol{K}_{x} \\boldsymbol{J})\\boldsymbol{x}$ is not $ > 0$ and thus $\\tilde{\\boldsymbol{K}}_{x}$ is not SPD matrix.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJXrwY6kW3d3"
      },
      "source": [
        "**Out of Sample Extension**\n",
        "\n",
        " * Consider the training set $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$.\n",
        " * Let $\\boldsymbol{K}_{x}$ be the kernel matrix obtained by applying the kernel on the training set.\n",
        " * Let $\\boldsymbol{J} \\boldsymbol{K} \\boldsymbol{K} = \\boldsymbol{V} \\boldsymbol{\\Sigma}^{2} \\boldsymbol{V}^{T}$ be the eigen decomposition of the centered kernel matrix.\n",
        " * Let $\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}$ be the low dimensional representation obtained by applying K-PCA, that is, $\\boldsymbol{Z} = \\boldsymbol{\\Sigma}_{d} \\boldsymbol{V}_{d}^{T}$.\n",
        "\n",
        "</br>\n",
        "\n",
        " * <font color='brown'>(**#**)</font> Kernel matrix is also called _Gram Matrix_.\n",
        "\n",
        "### 2.8. Question\n",
        "\n",
        "Let $\\boldsymbol{X}^{\\star} \\in \\mathbb{R}^{D \\times M}$ be a set of a new unseen data points. Write an expression, in a matrix form, for $\\boldsymbol{Z}^{\\star} \\in \\mathbb{R}^{d \\times M}$, the K-PCA out of sample extension applied to $\\boldsymbol{X}^{\\star}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cxSTrpFW3d3"
      },
      "source": [
        "### 2.8. Solution\n",
        "\n",
        "If we had the mapping $\\phi: \\boldsymbol{x}_i \\rightarrow\\boldsymbol{\\Phi}_i \\in \\mathbb{R}^{\\bar{D}}$ for the kernel matrix, then we could write:\\\n",
        "$\\boldsymbol{J} \\boldsymbol{K} \\boldsymbol{J} = \\boldsymbol{J} \\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi} \\boldsymbol{J} = \\tilde{\\boldsymbol{\\Phi}}^T \\tilde{\\boldsymbol{\\Phi}}$\n",
        "\n",
        "For out-of-sample encoding we would like to find:\\\n",
        "$\\boldsymbol{Z}^{\\star} = \\boldsymbol{\\Sigma}_d^{-1}\\boldsymbol{V}^T\\tilde{\\boldsymbol{\\Phi}}^T \\tilde{\\boldsymbol{\\phi}^{\\star}}$  with $\\tilde{\\boldsymbol{\\phi}^{\\star}} \\in \\mathbb{R}^{\\bar{D}\\times M}$\n",
        "\n",
        "$\\tilde{\\boldsymbol{\\Phi}}^T \\tilde{\\boldsymbol{\\phi}^{\\star}} = \\boldsymbol{J} \\boldsymbol{\\Phi}^T\\boldsymbol{\\phi}^{\\star}\\boldsymbol{J}$\n",
        "\n",
        "$\\boldsymbol{\\phi}^{\\star}$ needs to be centered by the in-sample data:\\\n",
        "$\\boldsymbol{\\phi}^{\\star}\\boldsymbol{J} = \\boldsymbol{\\phi}^{\\star} - \\frac 1 N \\boldsymbol{\\Phi}\\boldsymbol{1}_N \\boldsymbol{1}_M^T$\\\n",
        "$\\implies \\boldsymbol{J} \\boldsymbol{\\Phi}^T\\boldsymbol{\\phi}^{\\star}\\boldsymbol{J} = \\boldsymbol{J} \\boldsymbol{\\Phi}^T (\\boldsymbol{\\phi}^{\\star} - \\frac 1 N \\boldsymbol{\\Phi}\\boldsymbol{1}_N \\boldsymbol{1}_M^T) = \\boldsymbol{J} (\\boldsymbol{\\Phi}^T \\boldsymbol{\\phi}^{\\star} - \\frac 1 N \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi}\\boldsymbol{1}_N \\boldsymbol{1}_M^T) = \\boldsymbol{J} (\\boldsymbol{k}^{\\star} - \\frac 1 N \\boldsymbol{K}\\boldsymbol{1}_N \\boldsymbol{1}_M^T)$\\\n",
        "Where, \\\n",
        "$\\boldsymbol{k}^{\\star} \\in \\mathbb{R}^{N\\times M} $ is a matrix such that $\\boldsymbol{k}^{\\star}[i,j] = k(\\boldsymbol{x}_i, \\boldsymbol{x}^{\\star}_j), \\quad \\boldsymbol{x}^{\\star}_j \\in \\{ \\boldsymbol{X}^{\\star}\\}_{i = 1}^M$\n",
        "\n",
        "Now we can write the expression using only the kernel function (no mapping):\\\n",
        "$\\boldsymbol{Z}^{\\star} = \\boldsymbol{\\Sigma}_d^{-1}\\boldsymbol{V}^T\\boldsymbol{J} (\\boldsymbol{k}^{\\star} - \\frac 1 N \\boldsymbol{K}\\boldsymbol{1}_N \\boldsymbol{1}_M^T)$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORHeQy8gW3d4"
      },
      "source": [
        " * Consider the set $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$ and the corresponding matrix $\\boldsymbol{X} \\in \\mathbb{R}^{D \\times N}$.\n",
        " * Consider the kernel function $k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = \\left \\langle \\boldsymbol{x}, \\boldsymbol{y} \\right \\rangle$.\n",
        "\n",
        "\n",
        "### 2.10. Question\n",
        "\n",
        "Prove or disprove: The Kernel PCA using the above kernel is equivalent of the PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYkRxPLrW3d4"
      },
      "source": [
        "### 2.10. Solution\n",
        "\n",
        "let's break this down:\n",
        "\n",
        "We are given a set of data points $\\mathcal{X} = {\\boldsymbol{x}i}{i=1}^N$ where each $\\boldsymbol{x}_i \\in \\mathbb{R}^D$, and the corresponding data matrix $\\boldsymbol{X} \\in \\mathbb{R}^{D\\times N}$.\n",
        "\n",
        "The kernel function is defined as the inner product $k(\\boldsymbol{x}, \\boldsymbol{y}) = \\langle \\boldsymbol{x}, \\boldsymbol{y}\\rangle$.\n",
        "\n",
        "The question is asking whether Kernel PCA using this kernel is equivalent to regular PCA.\n",
        "\n",
        "To show equivalence, we need to show that Kernel PCA gives the same principal components as PCA.\n",
        "\n",
        "In PCA, the principal components are the eigenvectors of the covariance matrix $\\boldsymbol{\\Sigma} = \\frac{1}{N}\\boldsymbol{X}\\boldsymbol{X}^T$.\n",
        "\n",
        "In Kernel PCA, we center the kernel matrix $\\boldsymbol{K}$ by subtracting the mean of each column from the column. Then we find the eigenvectors of the centered kernel matrix.\n",
        "\n",
        "For this linear kernel, $\\boldsymbol{K}_{ij} = k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\langle \\boldsymbol{x}_i, \\boldsymbol{x}_j\\rangle$.\n",
        "\n",
        "Centering $\\boldsymbol{K}$ gives $\\boldsymbol{K}_c = \\boldsymbol{X}\\boldsymbol{X}^T - \\boldsymbol{1}\\mu^T - \\mu\\boldsymbol{1}^T$ where $\\mu = \\frac{1}{N}\\boldsymbol{X}\\boldsymbol{1}$.\n",
        "\n",
        "Therefore, the centered kernel matrix $\\boldsymbol{K}_c = \\frac{1}{N}\\boldsymbol{X}\\boldsymbol{X}^T = \\boldsymbol{\\Sigma}$.\n",
        "\n",
        "Since Kernel PCA and PCA find eigenvectors of the same covariance/centered kernel matrix, they give the same principal components.\n",
        "\n",
        "Therefore, for this linear kernel, Kernel PCA is equivalent to PCA. The statement is true.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3MlPfqaW3d4"
      },
      "source": [
        " * Consider the set $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$ and the corresponding matrix $\\boldsymbol{X} \\in \\mathbb{R}^{D \\times N}$.\n",
        " * Consider the non linear transformation $\\phi : \\mathbb{R}^{D} \\to \\mathbb{R}^{M}$ with the corresponding kernel matrix $\\boldsymbol{K}$.\n",
        "\n",
        "\n",
        "### 2.11. Question\n",
        "\n",
        "Let $\\boldsymbol{u} = \\sum_{i = 1}^{D} {\\alpha}_{i} \\phi \\left( \\boldsymbol{x}_{i} \\right)$ and $\\boldsymbol{v} = \\sum_{i = 1}^{D} {\\beta}_{i} \\phi \\left( \\boldsymbol{x}_{i} \\right)$ be 2 eigen vectors of of the Kernel PCA.  \n",
        "\n",
        "Show that $\\sum_{i = 1} \\sum_{j = 1} {\\alpha}_{i} {\\beta}_{j} \\boldsymbol{K}_{i, j} = 0 $.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33y0bHTDW3d4"
      },
      "source": [
        "### 2.11. Solution\n",
        "\n",
        "\n",
        "\n",
        "* We have a dataset $\\mathcal{X} = \\{\\boldsymbol{x}_i\\}_{i=1}^N$ where each $\\boldsymbol{x}_i \\in \\mathbb{R}^D$, and the data matrix $\\boldsymbol{X} \\in \\mathbb{R}^{D\\times N}$.\n",
        "* We have a nonlinear transformation $\\phi: \\mathbb{R}^D \\rightarrow \\mathbb{R}^M$ that maps each data point to a higher dimensional feature space.\n",
        "* The kernel matrix is defined as $\\boldsymbol{K}_{ij} = k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\langle \\phi(\\boldsymbol{x}_i), \\phi(\\boldsymbol{x}_j) \\rangle$.\n",
        "\n",
        "* We have two eigenvectors $\\boldsymbol{u}$ and $\\boldsymbol{v}$ of the centered kernel matrix $\\boldsymbol{K}_c$ from Kernel PCA:\n",
        "\n",
        "$$\\boldsymbol{u} = \\sum_{i=1}^N \\alpha_i \\phi(\\boldsymbol{x}_i)$$\n",
        "\n",
        "$$\\boldsymbol{v} = \\sum_{i=1}^N \\beta_i \\phi(\\boldsymbol{x}_i)$$\n",
        "\n",
        "* We want to show that:\n",
        "\n",
        "$$\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\beta_j \\boldsymbol{K}_{ij} = 0$$\n",
        "\n",
        "Proof:\n",
        "* Since $\\boldsymbol{u}$ and $\\boldsymbol{v}$ are eigenvectors of $\\boldsymbol{K}_c$, we have:\n",
        "\n",
        "$$\\boldsymbol{K}_c\\boldsymbol{u} = \\lambda_u \\boldsymbol{u}$$\n",
        "\n",
        "$$\\boldsymbol{K}_c\\boldsymbol{v} = \\lambda_v \\boldsymbol{v}$$  \n",
        "\n",
        "* Taking the dot product:\n",
        "\n",
        "$$\\langle \\boldsymbol{K}_c\\boldsymbol{u}, \\boldsymbol{v} \\rangle = \\lambda_u \\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle$$\n",
        "\n",
        "$$\\langle \\boldsymbol{u}, \\boldsymbol{K}_c\\boldsymbol{v} \\rangle = \\lambda_v \\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle$$\n",
        "\n",
        "* Since $\\boldsymbol{K}_c$ is symmetric, $\\langle \\boldsymbol{K}_c\\boldsymbol{u}, \\boldsymbol{v} \\rangle = \\langle \\boldsymbol{u}, \\boldsymbol{K}_c\\boldsymbol{v} \\rangle$.\n",
        "\n",
        "* Therefore, $\\lambda_u \\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle = \\lambda_v \\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle$\n",
        "\n",
        "* As $\\lambda_u \\neq \\lambda_v$, we must have $\\langle \\boldsymbol{u}, \\boldsymbol{v} \\rangle = 0$\n",
        "\n",
        "* Substituting the definitions of $\\boldsymbol{u}$ and $\\boldsymbol{v}$, this implies:\n",
        "\n",
        "$$\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\beta_j \\langle \\phi(\\boldsymbol{x}_i), \\phi(\\boldsymbol{x}_j) \\rangle = \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\beta_j \\boldsymbol{K}_{ij} = 0$$\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

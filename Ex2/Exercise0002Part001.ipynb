{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c-CA5LA9Os9"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# UnSupervised Learning Methods\n",
        "\n",
        "## Exercise 002 - Part I\n",
        "\n",
        "> Notebook by:\n",
        "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "| 1.0.000 | 16/08/2023 | Royi Avital | First version                                                      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loYxPpr-9OtA"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_08/Exercise0002Part001.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xueodn-Y9OtB"
      },
      "source": [
        "## Notations\n",
        "\n",
        "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
        "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
        "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
        "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKKkdF3u9OtB"
      },
      "source": [
        "## Guidelines\n",
        "\n",
        " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
        " - Answer all questions / tasks within the Jupyter Notebook.\n",
        " - Use MarkDown + MathJaX + Code to answer.\n",
        " - Verify the rendering on VS Code.\n",
        " - Submission in groups (Single submission per group).\n",
        " - You may and _should_ use the forums for questions.\n",
        " - Good Luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL38AyAW9OtC"
      },
      "source": [
        "## Team Members\n",
        "\n",
        "- `Ori_Blanka_208994764`.\n",
        " - `Or_Benson_308577345`.\n",
        " - `Alon_Hertz_315682773`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We37FtWs9OtD"
      },
      "source": [
        "## 1. Linear Algebra\n",
        "\n",
        "The Frobenius Norm ${\\left\\| \\cdot \\right\\|}_{F} : \\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{+}$ is defined as ${\\left\\| A \\right\\|}_{F} = \\sqrt{\\sum_{i} \\sum_{j} {A}_{ij}^{2}}$.\n",
        "\n",
        "### 1.1. Question\n",
        "\n",
        "Let $A \\in \\mathbb{R}^{m \\times n}$ and ${\\lambda}_{i} \\left( M \\right)$ is the $i$ -th eigen value of the matrix $M$ (Assuming $M$ has a valid eigen decomposition).  \n",
        "Prove that ${\\left\\| A \\right\\|}_{F}^{2} = \\sum_{i = 1}^{n} {\\lambda}_{i} \\left( {A}^{T} A \\right)$.  \n",
        "\n",
        "* <font color='brown'>(**#**)</font> Make sure to show why ${\\lambda}_{i} \\left( {A}^{T} A \\right)$ exists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wReWT89Q9OtD"
      },
      "source": [
        "### 1.1. Solution\n",
        "\n",
        "\n",
        "\n",
        "We can initiate the process by employing the spectral theorem to express $A^T A$ in relation to its eigen decomposition. Consider the orthogonal matrix $Q$, composed of the eigenvectors of $A^T A$, and the diagonal matrix $\\Lambda$, comprising the corresponding eigenvalues. This allows us to express $A^T A$ as $Q \\Lambda Q^T$. It's worth noting that due to the orthogonality of matrix $Q$, we can assert that $Q^T Q$ equals the identity matrix $I$..\n",
        "\n",
        "and now:\n",
        "$$\\begin{aligned} {\\left\\| A \\right\\|}_{F}^{2} &= \\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{i,j}^2 \\\\\n",
        "&= \\text{tr}(A^T A) \\quad \\text{by definition of Frobenius norm} \\\\\n",
        "&= \\text{tr}(Q \\Lambda Q^T) \\quad \\text{by eigen decomposition} \\\\\n",
        "&= \\text{tr}(\\Lambda Q^T Q) \\quad \\text{rearranging} \\\\\n",
        "&= \\text{tr}(\\Lambda) \\quad \\text{since $Q^T Q = I$} \\\\\n",
        "&= \\sum_{i=1}^{n} \\lambda_i(A^T A)  \\quad  \\text{trace as the sum of eigenvalues}   \\end{aligned}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vfeg0rh9OtE"
      },
      "source": [
        "A matrix $Q \\in \\mathbb{R}^{n \\times n}$ is called a positive definite matrix if and only if $\\forall \\boldsymbol{x} \\in \\mathbb{R}^{n} \\setminus \\left\\{ \\boldsymbol{0} \\right\\}, \\; \\boldsymbol{x}^{T} Q \\boldsymbol{x} > \\boldsymbol{0}$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> The following are the common notations:\n",
        "    * $\\boldsymbol{S}^{N}      = \\left\\{ X \\in \\mathbb{R}^{n \\times n} \\mid X = {X}^{T} \\right\\}$ (Symmetric matrices).\n",
        "    * $\\boldsymbol{S}^{N}_{+}  = \\left\\{ X \\in \\mathbb{R}^{n \\times n} \\mid X \\succeq 0 \\right\\}$ (Positive semi definite matrices which are symmetric).\n",
        "    * $\\boldsymbol{S}^{N}_{++} = \\left\\{ X \\in \\mathbb{R}^{n \\times n} \\mid X \\succ 0 \\right\\}$ (Positive definite matrices which are symmetric).\n",
        "\n",
        "### 1.2. Question\n",
        "\n",
        "Let $Q \\in \\mathbb{R}^{n \\times n}$ be a positive definite matrix. Show that ${\\left\\| \\boldsymbol{x} \\right\\|}_{Q} = \\sqrt{\\boldsymbol{x}^{T} Q \\boldsymbol{x}}$ is a norm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js-tJtO49OtF"
      },
      "source": [
        "### 1.2. Solution\n",
        "\n",
        "first we check if it satisfies the three properties of a norm which are:\n",
        "\n",
        "1. Non-negativity: ${\\left\\| \\boldsymbol{x} \\right\\|}_{Q} \\geq 0$ for all $\\boldsymbol{x} \\in \\mathbb{R}^n$, and ${\\left\\| \\boldsymbol{x} \\right\\|}_{Q} = 0$ if and only if $\\boldsymbol{x} = \\boldsymbol{0}$.\n",
        "2. Homogeneity: ${\\left\\| \\alpha \\boldsymbol{x} \\right\\|}_{Q} = |\\alpha| {\\left\\| \\boldsymbol{x} \\right\\|}_{Q}$ for all $\\alpha \\in \\mathbb{R}$ and $\\boldsymbol{x} \\in \\mathbb{R}^n$.\n",
        "3. Triangle inequality: ${\\left\\| \\boldsymbol{x} + \\boldsymbol{y} \\right\\|}_{Q} \\leq {\\left\\| \\boldsymbol{x} \\right\\|}_{Q} + {\\left\\| \\boldsymbol{y} \\right\\|}_{Q}$ for all $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^n$.\n",
        "\n",
        "now we check each of these properties:\n",
        "\n",
        "1. Non-negativity: we know that $Q$ is positive definite, so we know that $\\boldsymbol{x}^T Q \\boldsymbol{x} > 0$ for all $\\boldsymbol{x} \\neq \\boldsymbol{0}$ (quadratic form of a matrix). Therefore, ${\\left\\| \\boldsymbol{x} \\right\\|}_{Q} = \\sqrt{\\boldsymbol{x}^{T} Q \\boldsymbol{x}} \\geq 0$ for all $\\boldsymbol{x} \\in \\mathbb{R}^n$, and ${\\left\\| \\boldsymbol{x} \\right\\|}_{Q} = 0$ if and only if $\\boldsymbol{x} = \\boldsymbol{0}$.\n",
        "2. Homogeneity:  ${\\left\\| \\alpha \\boldsymbol{x} \\right\\|}_{Q} = \\sqrt{(\\alpha \\boldsymbol{x})^T Q (\\alpha \\boldsymbol{x})} = \\sqrt{\\alpha^2 \\boldsymbol{x}^T Q \\boldsymbol{x}} = |\\alpha| \\sqrt{\\boldsymbol{x}^T Q \\boldsymbol{x}} = |\\alpha| {\\left\\| \\boldsymbol{x} \\right\\|}_{Q}$.\n",
        "3. Triangle inequality:\n",
        "\n",
        "\\begin{align*}\n",
        "{\\left\\| \\boldsymbol{x} + \\boldsymbol{y} \\right\\|}_{Q}^2 &= (\\boldsymbol{x} + \\boldsymbol{y})^T Q (\\boldsymbol{x} + \\boldsymbol{y}) \\\\\n",
        "&= \\boldsymbol{x}^T Q \\boldsymbol{x} +2\\boldsymbol{x}^TQy + \\boldsymbol{y}^T Q \\boldsymbol{y}\\\\\n",
        "&\\leq \\boldsymbol{x}^T Q \\boldsymbol{x} +2\\boldsymbol{x}^T Q \\boldsymbol{x} \\boldsymbol{y}^T Q \\boldsymbol{y} + \\boldsymbol{y}^T Q \\boldsymbol{y} &&\\text{Cauchy-Schwarz inequality} \\\\\n",
        "&= {\\left\\| \\boldsymbol{x} \\right\\|}_{Q}^2 + 2{\\left\\| \\boldsymbol{x} \\right\\|}_{Q} {\\left\\| \\boldsymbol{y} \\right\\|}_{Q} + {\\left\\| \\boldsymbol{y} \\right\\|}_{Q}^2 \\\\\n",
        "&=({\\left\\| \\boldsymbol{x} \\right\\|}_{Q} + {\\left\\| \\boldsymbol{y} \\right\\|}_{Q})^2\n",
        "\\end{align*}\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgcWrnmA9OtF"
      },
      "source": [
        "A matrix $U \\in \\mathbb{R}^{n \\times n}$ is called an _orthogonal matrix_ if and only if ${U}^{T} U = U {U}^{T} = {U}^{-1} U = U {U}^{-1} = I$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> For matrices over $\\mathbb{C}$ we call such matrices a unitary matrices.\n",
        "\n",
        "### 1.3. Question\n",
        "\n",
        "Show that for any orthogonal matrix $U$ is an isometry with respect to the euclidean norm, that is ${\\left\\| U \\boldsymbol{x} \\right\\|}_{2} = {\\left\\| x \\right\\|}_{2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-TfvJes9OtG"
      },
      "source": [
        "### 1.3. Solution\n",
        "\n",
        "\n",
        "we keep in mind that since $U$ is an orthogonal matrix, it follows by definition that $U^TU=I.\" Therefore, we can express this as:\n",
        "\n",
        "$$\\left|\\left|Ux\\right|\\right|_{2}=\\left(Ux\\right)^{T}\\left(Ux\\right)=x^{T}U^{T}Ux=x^{T}Ix=x^{T}x=\\left|\\left|x\\right|\\right|_{2}$$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9VeFtQj9OtG"
      },
      "source": [
        "The matrix $R = \\begin{bmatrix*}[r] \\cos \\left( \\theta \\right) & - \\sin \\left( \\theta \\right) \\\\ \\sin \\left( \\theta \\right) & \\cos \\left( \\theta \\right) \\end{bmatrix*}$ is called a rotation matrix.\n",
        "\n",
        "### 1.4. Question\n",
        "\n",
        "For the set of matrices of size $2 \\times 2$, Prove or disprove:\n",
        "\n",
        " - The matrix $R$ is a orthogonal matrix.\n",
        " - Any orthogonal matrix is a rotation matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1aB9aQp9OtG"
      },
      "source": [
        "### 1.4. Solution\n",
        "\n",
        "*   True: To establish that $\\boldsymbol{R}$ is an orthogonal matrix, we can apply the equivalence mentioned earlier:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$ \\boldsymbol{R}^{T} \\boldsymbol{R} = \\begin{bmatrix} \\cos (\\theta) & \\sin (\\theta) \\\\ -\\sin (\\theta) & \\cos (\\theta) \\end{bmatrix} \\cdot \\begin{bmatrix} \\cos (\\theta) & -\\sin (\\theta) \\\\ \\sin (\\theta) & \\cos (\\theta) \\end{bmatrix} = \\begin{bmatrix} \\cos^2 (\\theta) + \\sin^2 (\\theta)  & - \\cos (\\theta) \\sin (\\theta) + \\sin (\\theta)\\cos (\\theta) \\\\ - \\sin (\\theta) \\cos (\\theta)  + \\cos (\\theta) \\sin (\\theta) & \\sin^2 (\\theta) + \\cos^2 (\\theta) \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "$$ \\boldsymbol{R} \\boldsymbol{R}^{T} =  \\begin{bmatrix} \\cos (\\theta) & -\\sin (\\theta) \\\\ \\sin (\\theta) & \\cos (\\theta) \\end{bmatrix} \\cdot \\begin{bmatrix} \\cos (\\theta) & \\sin (\\theta) \\\\ -\\sin (\\theta) & \\cos (\\theta) \\end{bmatrix} = \\begin{bmatrix} \\cos^2 (\\theta) + \\sin^2 (\\theta)  & \\cos (\\theta) \\sin (\\theta) - \\sin (\\theta)\\cos (\\theta) \\\\  \\sin (\\theta) \\cos (\\theta)  - \\cos (\\theta) \\sin (\\theta) & \\sin^2 (\\theta) + \\cos^2 (\\theta) \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "Hence, we can conclude that $\\boldsymbol{R}$ is an orthogonal matrix.\n",
        "\n",
        "\n",
        "*   False: Another category of orthogonal matrices consists of reflection matrices. As an illustration, consider the following matrix:\n",
        "\n",
        "$$ A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}  $$\n",
        "\n",
        "To begin, let's demonstrate that matrix $A$ is indeed an orthogonal matrix:\n",
        "\n",
        "$$ {A}^{T}A = A{A}^{T} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 + 1 & 0 + 0 \\\\ 0 + 0 & 1 + 0 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "Now, let's proceed by assuming, for the sake of contradiction, that there exists a value $\\theta$ such that:\n",
        "\n",
        "$$ \\begin{bmatrix} \\cos (\\theta) & - \\sin (\\theta) \\\\ \\sin (\\theta) & \\cos (\\theta) \\end{bmatrix} = A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$$\n",
        "\n",
        "This leads us to the result that $\\sin(\\theta) = 1$, which implies $-\\sin(\\theta) = -1$. However, we require $-\\sin(\\theta)$ to be equal to $1\", which leads to a contradiction.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVj5TChK9OtG"
      },
      "source": [
        "## 2. Optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSQz46_-9OtH"
      },
      "source": [
        "### 2.2. Question\n",
        "\n",
        "Find the global minimum and maximum points of the linear function $f \\left( x, y \\right) = 7 x + 12 y$ over the set $\\mathcal{S} = \\left\\{ \\left( x, y \\right) \\mid 2 {x}^{2} + 6 x y + 9 {y}^{2} - 2 x - 6 y \\leq 24 \\right\\}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G64H1lji9OtH"
      },
      "source": [
        "### 2.2. Solution\n",
        "\n",
        "Let's go ahead and formulate the Lagrangian function:\n",
        "\n",
        "$\\mathcal{L}(x,y,\\lambda) = 7x + 12y + \\lambda(2  xy + 9{y}^{2} -2x -6y -24) $\n",
        "\n",
        "1.  $\\frac{\\partial \\mathcal{L}}{\\partial x} = 7 + 4 \\lambda x + 6 \\lambda y -2 \\lambda \\implies \\lambda = \\frac{-7}{4x+6y-2}$\n",
        "\n",
        "2. $\\frac{\\partial \\mathcal{L}}{\\partial y} = 12 + 6 \\lambda x + 18 \\lambda y -6 \\lambda \\implies \\lambda = \\frac{-12}{6x+18y-6}$\n",
        "\n",
        "3. $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = 2 x^2 + 6xy + 9y^2 - 2x -6y -24 \\lambda $\n",
        "\n",
        "Upon comparing equations 1 and 2, and subsequently substituting equation 3, we obtain:\n",
        "\n",
        "$ y=\\frac{x+3}{9} \\implies x = \\pm 3 $\n",
        "\n",
        "so we get,\n",
        "\n",
        "$ min = (-3,0), \\quad f(-3,0)=-21 ,\\quad \\lambda = 0.5$\n",
        "\n",
        "$ max = (3,\\frac{2}{3}), \\quad f(3,\\frac{2}{3})=29, \\quad  \\lambda = -0.5 $\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0MCTyWo9OtH"
      },
      "source": [
        "## 3. K-Means\n",
        "\n",
        "The K-Means objective is given by:\n",
        "\n",
        "$$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "### 3.1. Question\n",
        "\n",
        "Show that the following 2 objectives are equivalent to the K-Means objectives:\n",
        "\n",
        "1. As a function of the clusters:\n",
        "\n",
        "$$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\} } \\sum_{k = 1}^{K} \\frac{1}{\\left| \\mathcal{D}_{k} \\right|} \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "2. As a function of the centroids:\n",
        "\n",
        "$$ \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\min_{k} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21rNhviJ9OtI"
      },
      "source": [
        "### 3.1. Solution\n",
        "\n",
        "1.   To demonstrate that minimizing the objective function with respect to the clusters yields an equivalent objective function, we start by observing that, in our context, minimizing any function is tantamount to minimizing any positive multiple of that function. In other words, although the minimum value may vary, the optimal solutions remain the same whether we work on function $f$ or $c \\cdot f$ for any $c > 0\".\n",
        "\n",
        "  we will now write the full expression of $\\boldsymbol{\\mu}_k$ :\n",
        "\n",
        "  $$ \\boldsymbol{\\mu}_k =\\frac{1}{|\\mathcal{D}_k|} \\sum_{\\boldsymbol{x}_i \\in \\mathcal{D}_k} \\boldsymbol{x}_i$$\n",
        "\n",
        "  Initially, we have:\n",
        "\n",
        "$$ \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} = \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } \\left( {\\left\\| \\boldsymbol{x}_{i} \\right\\|}_{2}^{2} + {\\left\\| \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} - 2⟨\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} ⟩ \\right) = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } \\sum_{ \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } \\left( {\\left\\| \\boldsymbol{x}_{i} \\right\\|}_{2}^{2} + {\\left\\| \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} - 2⟨\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} ⟩ \\right) = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} }  \\left( | \\mathcal{D}_k| {\\left\\| \\boldsymbol{x}_{i} \\right\\|}_{2}^{2} + \\sum_{ \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} }{\\left\\| \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} - 2 | \\mathcal{D}_k|⟨\\boldsymbol{x}_{i}, \\boldsymbol{\\mu}_{k} ⟩ \\right) $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "We describe the most recent change. The summation should be inserted within parentheses. The first element only receives a multiplication because the value of $j$ has no effect on it. The second one is straightforward, and in the last one we employ the above-written definition of the mean as well as the linearity of the inner-product. Using the same reasoning as the first summary we receive:\n",
        "\n",
        "\\\\\n",
        "\n",
        "$$ \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} }  \\left( | \\mathcal{D}_k| {\\left\\| \\boldsymbol{x}_{i} \\right\\|}_{2}^{2} + \\sum_{ \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} }{\\left\\| \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} - 2 | \\mathcal{D}_k|⟨\\boldsymbol{x}_{i}, \\boldsymbol{\\mu}_{k} ⟩ \\right) = 2 | \\mathcal{D}_k| \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} }  {\\left\\| \\boldsymbol{x}_{i} \\right\\|}_{2}^{2} - 2 | \\mathcal{D}_k|^2 {\\left\\| \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2}$$\n",
        "\n",
        "\\\\\n",
        "\n",
        "  Then we have:\n",
        "\n",
        "\\\\\n",
        "\n",
        "$$ \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} = \\sum_{ \\boldsymbol{x}_{i}\\in \\mathcal{D}_{k} } \\left( {\\left\\| \\boldsymbol{x}_{i} \\right\\|}_{2}^{2} + {\\left\\| \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} - 2⟨\\boldsymbol{x}_{i}, \\boldsymbol{\\mu}_{k} ⟩ \\right) = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} }  {\\left\\| \\boldsymbol{x}_{i} \\right\\|}_{2}^{2} + | \\mathcal{D}_k| {\\left\\| \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} - 2| \\mathcal{D}_k| {\\left\\| \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} }  {\\left\\| \\boldsymbol{x}_{i} \\right\\|}_{2}^{2} - | \\mathcal{D}_k| {\\left\\| \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "where the previous transitions were made using the same rationale. Therefore:\n",
        "\n",
        "\\\\\n",
        "\n",
        "$$ \\frac{1}{2} \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} = | \\mathcal{D}_k| \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "and we get the conclusion that identifying the arguments that reduce the clusters' total is\n",
        "\n",
        "$$  \\frac{1}{| \\mathcal{D}_k|} \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "is similar to\n",
        "\n",
        "$$ \\frac{1}{2} ⋅ \\frac{1}{| \\mathcal{D}_k|} \\sum_{ \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} = \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "and we achieved our goal.\n",
        "\n",
        "\\\\\n",
        "\n",
        "2.   With the objective function written as a function of the centroids, we aim to achieve the same result. First, we note that the $ min $ operator may be represented as the sum of the $ k $ Kronecker deltas of the centroid that gives the sample's minimal square distance:\n",
        "\n",
        "$$  \\min_{k} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} = \\sum_{k=1}^{K} δ_{ik}  {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "By selecting these delta functions, we can now maximize the objective function, which is similar to associating each sample with a certain cluster. Finally, we say:\n",
        "\n",
        "\\\\\n",
        "\n",
        "$$ \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\min_{k} {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} = \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{i = 1}^{N} \\sum_{k=1}^{K} δ_{ik}  {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} = \\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k=1}^{K} \\sum_{i = 1}^{N}  δ_{ik}  {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} =\\arg \\min_{ \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k=1}^{K} \\sum_{i = 1}^{N} \\boldsymbol{1}_{\\mathcal{D}_k}  {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} = \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "and now we achieved our goal.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td8g4Q-n9OtI"
      },
      "source": [
        "### 3.2. Question\n",
        "\n",
        "Prove or disprove: The K-Means algorithm **always** converge to a global minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOl15uWV9OtJ"
      },
      "source": [
        "### 3.2. Solution\n",
        "\n",
        "We shall contradict it with a counterexample, beginning with incorrect centroid initialization:\n",
        "\n",
        "Let's allocate six points along a real line to clusters with k=2:\n",
        "\n",
        "$x_{1,2,3,4,5,6}=1,2,5,8,9,10$\n",
        "\n",
        "The centroids will be located at:\n",
        "\n",
        "$\\mu_1 = 1, \\mu_2 = 8 $\n",
        "\n",
        "Following the initial iteration, the following will be the scene:\n",
        "\n",
        "$C_1 = \\{1,2\\} , C_1 = \\{5,8,9,10\\} $\n",
        "\n",
        "The centroids' values will be $mu_1 = 1.5, $mu_2 = 8$, and the cost function will be $J=14.5$.\n",
        "\n",
        "This will be the setting following the second iteration:\n",
        "\n",
        "$C_1 = \\{1,2\\} , C_1 = \\{5,8,9,10\\} $\n",
        "\n",
        "The centroids' values will be $mu_1 = 1.5, $mu_2 = 8, and $J=14.5, which will cause the process to stall because the clusters and the centroids haven't been updated..\n",
        "\n",
        "The ideal outcome in this situation will be: $C_1 = \\{1,2,5\\} , C_1 = \\{8,9,10\\} $ , $\\mu_1 = 2.66, \\mu_2 = 9 $ and the cost function $J=13.66$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXzrTLBn9OtJ"
      },
      "source": [
        "The K-Medians is a variation of the K-Means where the $ {L}_{1} $ norm is used instead of the squared $ {L}_{2} $:\n",
        "\n",
        "$$ \\arg \\min_{ \\left\\{ \\mathcal{D}_{k} \\right\\}, \\left\\{ {\\boldsymbol{\\mu}}_{k} \\right\\} } \\sum_{k = 1}^{K} \\sum_{ \\boldsymbol{x}_{i} \\in \\mathcal{D}_{k} } {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{k} \\right\\|}_{1} $$\n",
        "\n",
        "### 3.3. Question\n",
        "\n",
        "1. Derive _Step I_ of the K-Medians.  \n",
        "   Give an example where the assignment will be different than the assignment in K-Means.\n",
        "2. Derive _Step II_ of the K-Medians.  \n",
        "   Is the derived solution unique? Explain.  \n",
        "   Give a motivation for using the $ {L}_{1} $.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iUhf1T59OtK"
      },
      "source": [
        "### 3.3. Solution\n",
        "\n",
        "Here are the steps to derive K-Medians and an example showing the difference from K-Means:\n",
        "\n",
        "**step I Derive Step I of the K-Medians** - is to assign each data point $\\boldsymbol{x}_i$ to the closest median $\\boldsymbol{\\mu}_k$: $$ \\mathcal{D}_k = \\left\\{ \\boldsymbol{x}_i : \\left\\|\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k\\right\\|_1 < \\left\\|\\boldsymbol{x}_i - \\boldsymbol{\\mu}_j\\right\\|_1 \\;\\; \\forall j \\neq k\\right\\} $$ For example, consider points {[0, 10], [5,8], [3,4]} and medians {[0,0], [10,0]}. In K-Medians, [0,10] would be assigned to [0,0]. But in K-Means, it would be assigned to [10,0] due to squared distances.\n",
        "\n",
        "**Derive Step II of the K-Medians** -  is to update each median to the component-wise median of points assigned to it: $$ \\boldsymbol{\\mu}k = \\text{median}i\\{x{i1}, x{i2}, \\ldots\\} \\;\\;\\text{for } \\boldsymbol{x}_i \\in \\mathcal{D}_k $$ This solution is not unique since multiple data points may have the same component value, leading to multiple possible medians.\n",
        "The $L_1$ norm is more robust to outliers compared to the squared $L_2$ norm in K-Means. It reduces the influence of outliers by not squaring large differences.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv9lO1Xu9OtK"
      },
      "source": [
        "## 4. Gaussian Mixture Model\n",
        "\n",
        " * Let $\\underline{X} \\sim \\mathcal{N}_{d} \\left( \\boldsymbol{\\mu}_{x}, {\\Sigma}_{x} \\right)$ be a Gaussian Radom Vector.\n",
        " * Let $Y = {\\boldsymbol{a}}^{T} \\underline{X} + b$ be a random variable.\n",
        "\n",
        "\n",
        "### 4.1. Question\n",
        "\n",
        "Find ${f}_{Y} \\left( y \\right)$, the Probability Density Function (PDF) of $Y$ as a function of $\\boldsymbol{\\mu}_{x}, {\\Sigma}_{x}, \\boldsymbol{a}, b$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNteqlx69OtL"
      },
      "source": [
        "### 4.1. Solution\n",
        "\n",
        "Knowing that Y is likewise a Gaussian Radom Vector, we'll first compute its mean and variance before demonstrating the PDF function.\n",
        "\n",
        "1. MEAN :\n",
        "$ E[Y] = E[{\\boldsymbol{a}}^{T} \\underline{X} + b] = {\\boldsymbol{a}}^{T} E[\\underline{X}] + b = {\\boldsymbol{a}}^{T} \\mu_{x} + b $\n",
        "\n",
        "2. VAR :\n",
        "$ E[(Y - \\mu_{y})(Y - \\mu_{y})^{T}] = E[({\\boldsymbol{a}}^{T} \\underline{X} + b - ({\\boldsymbol{a}}^{T} \\mu_{x} + b))({\\boldsymbol{a}}^{T} \\underline{X} + b - ({\\boldsymbol{a}}^{T} \\mu_{x} + b))^{T}] = E[({\\boldsymbol{a}}^{T} (\\underline{X} - \\mu_{x}))({\\boldsymbol{a}}^{T} (\\underline{X} - \\mu_{x}))^{T}] = {\\boldsymbol{a}}^{T}E[ (\\underline{X} - \\mu_{x})(\\underline{X} - \\mu_{x})^{T}]{\\boldsymbol{a}} = {\\boldsymbol{a}}^{T}{\\Sigma}_{x}{\\boldsymbol{a}}$\n",
        "\n",
        "Now, PDF of y is :\n",
        "$f_{Y}(y) = - \\frac{1}{{\\sigma}_{y} \\sqrt{2π}} \\exp(-\\frac{1}{2}(\\frac{y-\\mu_{y}}{\\sigma_{y}})^{2}) = - \\frac{1}{{\\boldsymbol{a}}^{T}{\\Sigma}_{x}{\\boldsymbol{a}}\\sqrt{2π}} \\exp(-\\frac{1}{2}\\frac{(y-({\\boldsymbol{a}}^{T} \\mu_{x} + b))^{2}}{{\\boldsymbol{a}}^{T}{\\Sigma}_{x}{\\boldsymbol{a}}})$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvFQmCq89OtL"
      },
      "source": [
        "A matrix $A \\in \\mathbb{R}^{d \\times d}$ is called Symmetric Positive Semi Definite (SPSD) if ${A}^{T} = A$ and for any $\\boldsymbol{v} \\in \\mathbb{R}^{d}$:\n",
        "\n",
        "$$ \\boldsymbol{v}^{T} A \\boldsymbol{v} \\geq 0 $$\n",
        "\n",
        "### 4.2. Question\n",
        "\n",
        "Let $\\underline{X}$ be a random vector with covariance matrix ${\\Sigma}_{x}$. Show that ${\\Sigma}_{x}$ is an SPSD matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYpq1S7i9OtL"
      },
      "source": [
        "$$\n",
        "\\begin{array}{rcl}\n",
        "\\mathbf{\\boldsymbol{v}}^T\\Sigma_{x}\\mathbf{\\boldsymbol{v}} & = & \\mathbf{\\boldsymbol{v}}^TE\\{(\\mathbf{x}-\\bar{\\mathbf{x}})(\\mathbf{x}-\\bar{\\mathbf{x}})^T\\}\\mathbf{\\boldsymbol{v}} \\\\\n",
        "& = & E\\{\\mathbf{\\boldsymbol{v}}^T(\\mathbf{x}-\\bar{\\mathbf{x}})(\\mathbf{x}-\\bar{\\mathbf{x}})^T\\mathbf{\\boldsymbol{v}}\\} \\\\\n",
        "& = & E\\{s^2\\} \\\\\n",
        "& = & \\sigma_s^2. \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "Where $s$ is the zero-mean scalar random variable and $sigma_s$ is its variance,\n",
        "$$\n",
        "s = \\mathbf{\\boldsymbol{v}}^T(\\mathbf{x}-\\bar{\\mathbf{x}}) = (\\mathbf{x}-\\bar{\\mathbf{x}})^T\\mathbf{\\boldsymbol{v}}.\n",
        "$$\n",
        "Any real number has a square that is bigger than or equal to one.\n",
        "$$\n",
        "\\sigma_s^2 \\ge 0\n",
        "$$\n",
        "then,\n",
        "$$\n",
        "\\mathbf{\\boldsymbol{v}}^T\\Sigma_{x}\\mathbf{\\boldsymbol{v}} = \\sigma_s^2 \\ge 0.\n",
        "$$\n",
        "Inferring that any real random vector's covariance matrix is always positive and semi-definite.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH49zeDz9OtM"
      },
      "source": [
        "The quadratic form $ \\left \\langle \\boldsymbol{x}, \\boldsymbol{A} \\boldsymbol{x} \\right \\rangle = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} $ is a common operation in algebra and appears in the PDF of the Gaussian distribution.\n",
        "\n",
        "### 4.3. Question\n",
        "\n",
        "For $ \\boldsymbol{x} \\in \\mathbb{R}^{2} $ define $ Q \\left( \\boldsymbol{x} \\right) = 3 {x}_{1}^{2} + 2 {x}_{1} {x}_{2} - 5 {x}_{2}^{2} $.\n",
        "\n",
        "1. Find a symmetric matrix $ \\boldsymbol{A} $ which defines the quadratic form.\n",
        "2. Prove / Disprove: The matrix is unique.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> You may find the decomposition of each matrix into symmetric and anti symmetric useful: $ \\boldsymbol{B} = \\boldsymbol{B}_{s} + \\boldsymbol{B}_{a} $ where $ \\boldsymbol{B}_{s} = \\frac{1}{2} \\left( \\boldsymbol{B} + \\boldsymbol{B}^{T} \\right) $ and $ \\boldsymbol{B}_{a} = \\frac{1}{2} \\left( \\boldsymbol{B} - \\boldsymbol{B}^{T} \\right) $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02L7ngCj9OtM"
      },
      "source": [
        "### 4.3. Solution\n",
        "\n",
        "1. Let $\\boldsymbol{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$. Then the quadratic form can be written as:\n",
        "\n",
        "$Q(\\boldsymbol{x}) = \\boldsymbol{x}^T \\begin{bmatrix} 3 & 1 \\\\ 1 & -5 \\end{bmatrix} \\boldsymbol{x}$\n",
        "\n",
        "Therefore, the symmetric matrix $\\boldsymbol{A}$ that defines this quadratic form is:\n",
        "\n",
        "$\\boldsymbol{A} = \\begin{bmatrix} 3 & 1 \\\\ 1 & -5 \\end{bmatrix}$\n",
        "\n",
        "2. The matrix $\\boldsymbol{A}$ is unique. Proof:\n",
        "\n",
        "Let $\\boldsymbol{B}$ be another symmetric matrix such that $Q(\\boldsymbol{x}) = \\boldsymbol{x}^T\\boldsymbol{B}\\boldsymbol{x}$.\n",
        "\n",
        "Decompose $\\boldsymbol{B}$ into its symmetric and anti-symmetric parts:\n",
        "\n",
        "$\\boldsymbol{B} = \\boldsymbol{B}_s + \\boldsymbol{B}_a$\n",
        "\n",
        "Where $\\boldsymbol{B}_s = \\frac{1}{2}(\\boldsymbol{B} + \\boldsymbol{B}^T)$ and $\\boldsymbol{B}_a = \\frac{1}{2}(\\boldsymbol{B} - \\boldsymbol{B}^T)$\n",
        "\n",
        "Since $\\boldsymbol{B}$ is symmetric, $\\boldsymbol{B}_a = \\boldsymbol{0}$.\n",
        "\n",
        "Therefore, $\\boldsymbol{B} = \\boldsymbol{B}_s = \\frac{1}{2}(\\boldsymbol{B} + \\boldsymbol{B}^T) = \\boldsymbol{A}$\n",
        "\n",
        "Thus, the symmetric matrix $\\boldsymbol{A}$ is unique.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHXtD2lJ9OtN"
      },
      "source": [
        "## 5. Hierarchical Clustering\n",
        "\n",
        "The _complete linkage distance_ between the 2 clusters $\\mathcal{C}_{1} = {\\left\\{ \\boldsymbol{x}_{i} \\right\\}}_{i = 1}^{{N}_{1}}$ and $\\mathcal{C}_{2} = {\\left\\{ \\boldsymbol{x}_{j} \\right\\}}_{j = 1}^{{N}_{2}}$:\n",
        "\n",
        "$$ {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) = \\begin{cases}\n",
        "0 & \\text{ if } \\mathcal{C}_{1} = \\mathcal{C}_{2} \\\\\n",
        "\\max_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| & \\text{ if } \\mathcal{C}_{1} \\neq \\mathcal{C}_{2}\n",
        "\\end{cases} $$\n",
        "\n",
        "### 5.1. Question\n",
        "\n",
        "Prove that the complete linkage is indeed a metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FRKStwN9OtN"
      },
      "source": [
        "### 5.1. Solution\n",
        "\n",
        "The following characteristics must be demonstrated in order to prove that the aforementioned function is in fact a metric:\n",
        "\n",
        "1. Non-negativity:\n",
        "$$ \\text{ if } \\mathcal{C}_{1} = \\mathcal{C}_{2} ⇒ {d}^{2}_{\\text{Complete Link}}  = 0$$\n",
        "$$ \\text{ if } \\mathcal{C}_{1} \\neq \\mathcal{C}_{2} ⇒ {d}^{2}_{\\text{Complete Link}}  = \\max_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| > 0 )(*)$$\n",
        "\n",
        "(*)The second instance results directly from the norm's definition..\n",
        "\n",
        "Therefore ${d}^{2}_{\\text{Complete Link}}$ is Non-negativity function.\n",
        "\n",
        "2. Identity of indiscernibles ($ {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) = 0 \\text{ iff } \\mathcal{C}_{1} = \\mathcal{C}_{2}$):\n",
        "* $\\text{ if } \\mathcal{C}_{1} = \\mathcal{C}_{2} ⇒ {d}^{2}_{\\text{Complete Link}}  = 0$ as nedded.\n",
        "* $\\text{ if } \\mathcal{C}_{1} \\neq \\mathcal{C}_{2} ⇒ {d}^{2}_{\\text{Complete Link}}  = \\max_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| \\text{, we now that } ∀\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2} ⇒\\boldsymbol{x}_{i} \\notin \\mathcal{C}_{2} \\& \\boldsymbol{x}_{j} \\notin \\mathcal{C}_{1} ⇒ \\boldsymbol{x}_{j} \\neq \\boldsymbol{x}_{i} ⇒ \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| > 0 ⇒ \\max_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| > 0$\n",
        "\n",
        "Therfore we got that $ {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) = 0 \\text{ iff } \\mathcal{C}_{1} = \\mathcal{C}_{2}$.\n",
        "\n",
        "3. Symmetry : This property derives directly from the properties of the norm.\n",
        "\n",
        "4. Triangle inequality :     \n",
        "Let's consider three clusters $\\mathcal{C}_{1}, \\mathcal{C}_{2}, \\mathcal{C}_{3}$ and look on the next options :\n",
        "* $\\mathcal{C}_{1} = \\mathcal{C}_{2} = \\mathcal{C}_{3} ⇒ 0 = {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{3} \\right) \\leq {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) + {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right) = 0 + 0 = 0$\n",
        "\n",
        "* $\\mathcal{C}_{1} = \\mathcal{C}_{3} ⇒ 0 = {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{3} \\right) \\leq {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) + {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right) \\text{because the Non-negativity we showed above}$\n",
        "\n",
        "* $\\mathcal{C}_{1} = \\mathcal{C}_{2} ⇒ {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{3} \\right) \\leq {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) + {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right) = 0 + {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{3} \\right) ⇒ {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{3} \\right) \\leq {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{3} \\right) \\text{(same for } \\mathcal{C}_{2} = \\mathcal{C}_{3} \\text{)}$\n",
        "* $\\mathcal{C}_{1} \\neq \\mathcal{C}_{2} \\neq \\mathcal{C}_{3} ⇒ {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{3} \\right) = max_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| \\leq_{Triangle- inequality-for-norm} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{k} \\right\\| + \\left\\| \\boldsymbol{x}_{k} - \\boldsymbol{x}_{j} \\right\\| \\text{where } \\boldsymbol{x}_{k} \\in \\mathcal{C}_{3} \\leq max_{\\boldsymbol{x}_{r} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{t} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{r} - \\boldsymbol{x}_{t} \\right\\| + max_{\\boldsymbol{x}_{y} \\in \\mathcal{C}_{2}, \\boldsymbol{x}_{u} \\in \\mathcal{C}_{3}} \\left\\| \\boldsymbol{x}_{y} - \\boldsymbol{x}_{u} \\right\\| = {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) + {d}^{2}_{\\text{Complete Link}} \\left( \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right)$\n",
        "\n",
        "The Triangle Inequality is valid as a result.\n",
        "Additionally, the total linkage is a measure.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX3hixUl9OtO"
      },
      "source": [
        " * The _single linkage dissimilarity_ between the 2 clusters $\\mathcal{C}_{1} = {\\left\\{ \\boldsymbol{x}_{i} \\right\\}}_{i = 1}^{{N}_{1}}$ and $\\mathcal{C}_{2} = {\\left\\{ \\boldsymbol{x}_{j} \\right\\}}_{j = 1}^{{N}_{2}}$:\n",
        "\n",
        "$$ {d}^{2}_{\\text{Single Link}} \\left( \\mathcal{C}_{1}, \\mathcal{C}_{2} \\right) = \\min_{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}} \\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\| $$\n",
        "\n",
        " * The _Lance Williams_ update rule is given by: ${D}_{\\overline{ij}, k} = {\\alpha}_{i} {D}_{i, k} + {\\alpha}_{j} {D}_{j, k} + \\beta {D}_{i, j} + \\gamma \\left| {D}_{i, k} - {D}_{j, k} \\right|$.\n",
        "\n",
        "### 5.2. Question\n",
        "\n",
        "Consider 3 clusters $\\mathcal{C}_{1}, \\mathcal{C}_{2}, \\mathcal{C}_{3}$ with ${D}_{i, j} = {d}_{\\text{Single Link}} \\left( \\mathcal{C}_{i}, \\mathcal{C}_{j} \\right)$.  \n",
        "Prove that:\n",
        "\n",
        "$$ {D}_{\\overline{12}, 3} = {d}_{\\text{Single Link}} \\left( \\mathcal{C}_{1} \\cup \\mathcal{C}_{2}, \\mathcal{C}_{3} \\right) $$\n",
        "\n",
        "In other words, show that the _Lance Williams_ algorithm is correct for the _single linkage dissimilarity_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1pzEG0m9OtO"
      },
      "source": [
        "### 5.2. Solution\n",
        "\n",
        " single-linkage params are $\\alpha_i=\\frac{1}{2}$, $\\alpha_j=\\frac{1}{2}$, $\\gamma=-\\frac{1}{2}$\n",
        "\n",
        "then we can get the update rule which is $D_{\\bar{ij}, k} = \\frac{1}{2} D_{i,k} + \\frac{1}{2} D_{j,k} - \\frac{1}{2} |D_{i,k} - D_{j,k}|$. now, WLOG, let $D_{1,3}\\leq D_{2,3}$, so the update rule just simflify it to $\\frac{1}{2}D_{1,3} + \\frac{1}{2} D_{2,3} - \\frac{1}{2} |D_{1,3} - D_{2,3}| = D_{1,3}$\n",
        "\n",
        "The proof: In the merged cluster $\\bar{12}$ we have $D_{\\bar{12}, 3} = ||x_{\\bar{12}}, x_3||$ for some $x_{\\bar{12}} \\in C_{\\bar{12}}, x_3 \\in C_3$ so that $||x_{\\bar{12}}, x_3||$ is the minimal. since that $x_{\\bar{12}}$ is in either $C_1$ or $C_2$ so we then get that $D_{\\bar{12}, 3} = D_{1,3}$.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
